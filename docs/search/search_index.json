{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Dimensions BigQuery Lab!","text":"<p>This site and its related Github repository contain a collection of tutorials and queries showing how to carry out common research data analytics tasks using the Dimensions database on Google BigQuery (GBQ).</p> <p>Note</p> <p>Dimensions is a comprehensive database for the research &amp; innovation ecosystem, used by governments, universities, businesses, funders and investors around the world. It is a structured, linked database containing trillions of data points about the research landscape, organized in tables e.g. research organizations, publications, patents, grants, clinical trials and more.</p> <p>You can use the structured code examples on this site to put together analyses for specific use cases e.g.</p> <ul> <li>Competitive intelligence</li> <li>Horizon-scanning &amp; emerging trends</li> <li>Academic &amp; industry partnerships and collaboration networks</li> <li>Key Opinion Leader (KOL) identification</li> <li>Recruitment &amp; talent</li> <li>Performance &amp; benchmarking</li> <li>Tracking funding dollar flows and citation patterns</li> <li>Social and economic impact of research</li> </ul>"},{"location":"#get-started-now-fast-and-for-free","title":"Get started now, fast and for free","text":"<p>If you haven\u2019t used Dimensions on BigQuery before, you can get started now for free. This video will show you how to set up Google cloud, run a query and do a visualization with our trial dataset - all in 5 minutes.</p> <p>Following the steps in the video above gives you access to our sandbox environment. This contains all Dimensions data related to COVID-19. It has the same data structure as the full Dimensions database and is updated on a daily basis.</p> <p>Note</p> <p>Access to the full Dimensions dataset on Google BigQuery is subscription-only: you or your organization needs to subscribe for you to gain access to the full dataset. Get in touch here.</p>"},{"location":"#how-to-use-this-lab","title":"How to use this Lab","text":"<p>The Lab is designed for learning and inspiration. It demonstrates how to query BigQuery according to best practices, how to work with the Dimensions data structure, and how to do core analytical tasks. As you become more experienced with the data, you will want to combine these building blocks into more complex analyses that answer your business or research questions.</p> <p>The materials in this Lab can be navigated sequentially, or you can use the search box above to enter a keyword and see matching documents.</p> <p>There are three main sections:</p> <ul> <li>The Tutorials section contains guides that focus on specific topics or use cases e.g. how to deal with a specific data type, or how to use GBQ in combination with other technologies.</li> <li>The Collections section contains thematic groupings of queries based on application scenarios (e.g. citation analysis).</li> <li>The Queries section is an archive of reusable SQL queries together with an explanation of what they do.</li> </ul>"},{"location":"#finding-out-more","title":"Finding out more","text":"<p>Find out more about Dimensions on BigQuery with the following resources: * The Dimensions BigQuery homepage is the place to start from if you\u2019ve never heard about Dimensions on GBQ. * The Dimensions on BigQuery official documentation contains detailed data model information, tutorials on how to sign up and how to access it using tools like Data Studio and Tableau. * The BigQuery Lab Github repository contains the source code for all the materials on this website, and more.</p>"},{"location":"#video-tutorials","title":"Video tutorials","text":"<p>The Dimensions YouTube channel contains various how-to videos like this one: Dimensions on Google BigQuery - what it is and how to get started with it:</p>"},{"location":"#changelog","title":"Changelog","text":"<p>See the CHANGELOG.md file on Github.</p>"},{"location":"collections/","title":"About","text":"<p>The Collection section contains thematic groupings of queries based on common application scenarios.</p> <p>The source code of the queries and further explanations are available in the Queries section.</p>"},{"location":"collections/#collections","title":"Collections","text":"<ul> <li>Publications</li> <li>Citations</li> <li>Authors</li> <li>Funding</li> </ul> <p>Note</p> <p>New queries and collections are added frequently, so watch this space or keep an eye on the CHANGELOG file on Github!</p>"},{"location":"collections/01-publications/","title":"Publications","text":"<ul> <li>Number of publications added to Dimensions each month</li> <li>Number of Publications by Type</li> <li>Generate a list of publication categories by flattening/concatenating nested data</li> <li>Number of publications per SDG category</li> <li>Publications count per FoR category, total and percentage against total</li> <li>Finding articles matching a specific affiliation string</li> <li>Top publications by Altmetric score and research organization </li> <li>Select publications matching selected concepts</li> <li>Extracting complex publications records</li> <li>Finding Journals using string matching</li> <li>International collaboration of an organisation in a field</li> <li>International collaboration of a researcher with org and country context</li> <li>Researcher collaboration counts</li> <li>Institution collaboration counts</li> <li>Publication growth rate with sliding window</li> <li>Country-level publication activity over time</li> </ul>"},{"location":"collections/02-citations/","title":"Citations","text":"<ul> <li>Top N publications by citations percentile</li> <li>One-degree citation network for a single publication</li> <li>Incoming citations by year</li> <li>Incoming citations by journal</li> <li>Citations by journal, for a specific publisher</li> <li>Outgoing citations from a journal</li> <li>Citing authors by country</li> </ul>"},{"location":"collections/03-authors/","title":"Authors","text":"<ul> <li>Generate a list of publication authors by flattening/concatenating nested data</li> <li>Count of corresponding authors by publisher</li> <li>Counting new vs recurring authors, for a specific journal</li> <li>International collaboration rate of individuals, with context</li> </ul>"},{"location":"collections/04-funding/","title":"Funding &amp; Grants","text":"<ul> <li>Grants of an organization</li> <li>Funding by journal</li> </ul>"},{"location":"queries/","title":"About Queries","text":"<p>The Queries section is an archive of reusable SQL queries together with an explanation of how they work. </p> <p>You can navigate these contents sequentially, or simply use the search box at the top to find items of interest. Alternatively, you can find thematic groups of queries in the Collections section.</p> <p>Note</p> <p>The Google BigQuery official documentation website provides in-depth tutorials about SQL syntax.</p>"},{"location":"queries/01/","title":"1. Number of publications added to Dimensions each month","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/01/#description","title":"Description","text":"<p>Counts the number of publications added to Dimensions each month, using the <code>date_inserted</code> field.</p>"},{"location":"queries/01/#query","title":"Query","text":"<pre><code>SELECT\n  DATE_TRUNC(date_inserted, MONTH) as date,\n  COUNT(id) as countDim\nFROM\n  `dimensions-ai.data_analytics.publications`\nGROUP BY date\nORDER BY date DESC\nLIMIT 5\n</code></pre>"},{"location":"queries/01/#breaking-it-down","title":"Breaking it down","text":"<p>The DATETIME_TRUNC function is used here to \"round down\" the timestamps in the <code>date_inserted</code> field to the month level.</p>"},{"location":"queries/01/#results","title":"Results","text":"<pre><code>[\n  {\n    \"date\": \"2021-04-01 00:00:00 UTC\",\n    \"countDim\": \"458175\"\n  },\n  {\n    \"date\": \"2021-03-01 00:00:00 UTC\",\n    \"countDim\": \"746884\"\n  },\n  {\n    \"date\": \"2021-02-01 00:00:00 UTC\",\n    \"countDim\": \"661512\"\n  },\n  {\n    \"date\": \"2021-01-01 00:00:00 UTC\",\n    \"countDim\": \"687725\"\n  },\n  {\n    \"date\": \"2020-12-01 00:00:00 UTC\",\n    \"countDim\": \"828301\"\n  }\n]\n</code></pre>"},{"location":"queries/02/","title":"2. Number of Publications by Type","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/02/#description","title":"Description","text":"<p>Each publication is classified into one of several \"types.\" This query counts how many have been classified as each type.</p>"},{"location":"queries/02/#query","title":"Query","text":"<pre><code>SELECT\n  COUNT(id) AS tot_articles, type\nFROM\n  `dimensions-ai.data_analytics.publications`\nGROUP BY\n  type\nORDER BY\n  tot_articles DESC\n</code></pre>"},{"location":"queries/02/#results","title":"Results","text":"<pre><code>[\n  {\n    \"tot_articles\": \"96627450\",\n    \"type\": \"article\"\n  },\n  {\n    \"tot_articles\": \"10781485\",\n    \"type\": \"chapter\"\n  },\n  {\n    \"tot_articles\": \"6527269\",\n    \"type\": \"proceeding\"\n  },\n  {\n    \"tot_articles\": \"2648537\",\n    \"type\": \"preprint\"\n  },\n  {\n    \"tot_articles\": \"795713\",\n    \"type\": \"monograph\"\n  },\n  {\n    \"tot_articles\": \"525722\",\n    \"type\": \"book\"\n  }\n]\n</code></pre>"},{"location":"queries/03/","title":"3. Generate a list of publication authors by flattening/concatenating nested data","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/03/#description","title":"Description","text":"<p>This query returns a table with a single row. One field contains the Dimensions publication ID, and the other contains a string of all author names associated with the paper, separated by semicolons. </p> <p>For more details about working with nested fields, see the tutorial page on the topic. Example 3 in the tutorial deals with this query specifically.</p>"},{"location":"queries/03/#query","title":"Query","text":"<pre><code>WITH author_array AS (\n  SELECT\n    id,\n    ARRAY (\n      SELECT CONCAT(first_name, \" \", last_name)\n      FROM UNNEST(authors)\n    ) AS author_names\n  FROM\n    `dimensions-ai.data_analytics.publications`\n  WHERE\n    id = 'pub.1132070778'\n)\n\nSELECT\n  id,\n  ARRAY_TO_STRING(author_names, '; ') AS authors_list\nFROM author_array\n</code></pre>"},{"location":"queries/03/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1132070778\",\n    \"authors_list\": \"O Gr\u00e5n\u00e4s; A Mocellin; E S Cardoso; F Burmeister; C Caleman; O Bj\u00f6rneholm; A Naves de Brito\"\n  }\n]\n</code></pre>"},{"location":"queries/04/","title":"4. Generate a list of publication categories by flattening/concatenating nested data","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/04/#description","title":"Description","text":"<p>This query returns a table with a single row. One field contains the Dimensions publication ID, and the other contains a string of all categories associated with the publication, separated by semicolons. </p> <p>It's very similar to the publication authors query. You may also be interested in the tutorial about working with nested fields.</p>"},{"location":"queries/04/#query","title":"Query","text":"<pre><code>WITH categories AS (\n  SELECT\n    id,\n    ARRAY(\n        SELECT name\n        FROM UNNEST(category_for.first_level.FULL)\n    ) AS category_names\n  FROM\n    `dimensions-ai.data_analytics.publications`\n  WHERE\n    id = 'pub.1132070778'\n)\n\nSELECT\n  id,\n  ARRAY_TO_STRING(category_names, '; ') AS categories_list\nFROM categories\n</code></pre>"},{"location":"queries/04/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1132070778\",\n    \"categories_list\": \"Physical Sciences; Chemical Sciences\"\n  }\n]\n</code></pre>"},{"location":"queries/05/","title":"5. Number of publications per SDG category","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/05/#description","title":"Description","text":"<p>Many publications in the Dimensions database are classified under Sustainable Development Goals (SDGs). </p> <p>This query returns the top five most commonly applied SDG classifications and the total number of publications in each one.</p>"},{"location":"queries/05/#query","title":"Query","text":"<pre><code>SELECT\n  COUNT(p.id) AS tot,\n  sdg.name\nFROM `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(category_sdg.full) sdg\nGROUP BY\n  sdg.name\nORDER BY\n  tot DESC\nLIMIT 5\n</code></pre>"},{"location":"queries/05/#breaking-it-down","title":"Breaking it down","text":"<p>This query is short but wades through a complex data structure. While the author names query has to deal with an array of structs, this one has to parse a struct of arrays: Refer to the schema documentation for the details, but if we want to access the names of the SDG categories relevant to a single publication, we have to drill through multiple layers:</p> <ol> <li>The <code>publications</code> table uses a single row to represent a single publication.</li> <li>Publications with an SDG categorization have a struct in the <code>category_sdg</code> field with two keys: <code>codes</code> and <code>full</code>.</li> <li>We can access this by name, so <code>category_sdg.full</code> will give us the relevant entry for a publication. However, <code>category_sdg.full</code> is an array.</li> <li>Each entry in the <code>category_sdg.full</code> array is another struct, with three fields: <code>code</code>, <code>id</code> and <code>name</code>. We want the names.</li> </ol> <p>We can get through the first struct by getting the <code>full</code> field by name. From there, we need to do something more complex: <pre><code>CROSS JOIN UNNEST(category_sdg.full) sdg\n</code></pre></p> <p>This line performs a cross join. The less-technical explanation for what happens here is that, since <code>category_sdg.full</code> is an array, using a cross join with the publications table creates a new field called <code>sdg</code>, and each row of the <code>publications</code> table is repeated, once for each value for <code>sdg</code>. A quick demonstration:</p> <p>WITHOUT UNNEST():</p> id category_sdg.full pub.123 sdg1, sdg2, sdg3 pub.987 sdg4, sdg1 <p>WITH UNNEST():</p> id sdg pub.123 sdg1 pub.123 sdg2 pub.123 sdg3 pub.987 sdg4 pub.987 sdg1 <p>So after this clause: <pre><code>CROSS JOIN UNNEST(category_sdg.full) sdg\n</code></pre> we have a new field, <code>sdg</code>, that has the nested fields associated with an individual SDG: <code>code</code>, <code>id</code> and <code>name</code>. Since we want <code>name</code>, that's what we refer to in the final query:</p> <pre><code>SELECT\n  COUNT(p.id) AS tot,\n  sdg.name\nFROM `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(category_sdg.full) sdg\nGROUP BY\n  sdg.name\nORDER BY\n  tot DESC\nLIMIT 5\n</code></pre> <p>We then count the number of IDs associated with each SDG <code>name</code> by using a <code>GROUP BY</code> clause.</p>"},{"location":"queries/05/#results","title":"Results","text":"<pre><code>[\n  {\n    \"tot\": \"1577950\",\n    \"name\": \"Affordable and Clean Energy\"\n  },\n  {\n    \"tot\": \"1455575\",\n    \"name\": \"Good Health and Well Being\"\n  },\n  {\n    \"tot\": \"769875\",\n    \"name\": \"Peace, Justice and Strong Institutions\"\n  },\n  {\n    \"tot\": \"633369\",\n    \"name\": \"Quality Education\"\n  },\n  {\n    \"tot\": \"507003\",\n    \"name\": \"Climate Action\"\n  }\n]\n</code></pre>"},{"location":"queries/06/","title":"6. Publications count per FoR category, total and percentage against total","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/06/#description","title":"Description","text":"<p>Many publications in the Dimensions database are classified under ANZSRC Field of Research codes. </p> <p>This query returns the total number of publications classified under each, plus the percentage of all publications in Dimensions with that classification.</p>"},{"location":"queries/06/#query","title":"Query","text":"<pre><code>SELECT\n  cat.name,\n  COUNT(p.id) AS pubs_global,\n  ROUND(\n    (\n      COUNT(p.id) * 100 /(\n        SELECT COUNT(*)\n        FROM `dimensions-ai.data_analytics.publications`\n      )\n    ), 2\n  ) AS pubs_global_pc\nFROM\n  `dimensions-ai.data_analytics.publications` p,\n  UNNEST(category_for.first_level.full) cat\nGROUP BY\n  cat.name\nORDER BY\n  pubs_global_pc DESC,\n  cat.name\n</code></pre>"},{"location":"queries/06/#breaking-it-down","title":"Breaking it down","text":"<p>This query looks more complicated than it is. The main component could be summarized using this query:</p> <pre><code>SELECT\n  cat.name,\n  COUNT(p.id) AS pubs_global\nFROM\n  `dimensions-ai.data_analytics.publications` p,\n  UNNEST(category_for.first_level.full) cat\nGROUP BY\n  cat.name\n</code></pre> <p>This query works exactly like the \"publications per SDG\" query, and there's a full breakdown there explaining how we unnest structs full of arrays full of structs. The main take-away is that we extract the names of all the first-level FOR codes, then count the number of publications listed under each one. That leaves only one more SELECT statement:</p> <pre><code>ROUND(\n    (\n      COUNT(p.id) * 100 /(\n        SELECT COUNT(*)\n        FROM `dimensions-ai.data_analytics.publications`\n      )\n    ), 2\n  ) AS pubs_global_pc\n</code></pre> <p>This piece of the query just counts how many total records are in the <code>publications</code> table: <pre><code>SELECT COUNT(*)\nFROM `dimensions-ai.data_analytics.publications`\n</code></pre></p> <p>And this next piece of the query takes the total number of publications in a single classification and divides it by that total. The <code>* 100</code> piece converts the decimal into a percentage: <pre><code>COUNT(p.id) * 100 /(\n  SELECT COUNT(*)\n  FROM `dimensions-ai.data_analytics.publications`\n)\n</code></pre></p> <p>The outer-most call, to the <code>ROUND()</code> function, states that the function should return the percentage rounded to two decimal places.</p>"},{"location":"queries/06/#results","title":"Results","text":"<pre><code>[\n  {\n    \"name\": \"Medical and Health Sciences\",\n    \"pubs_global\": \"30145537\",\n    \"pubs_global_pc\": \"25.49\"\n  },\n  {\n    \"name\": \"Engineering\",\n    \"pubs_global\": \"12388815\",\n    \"pubs_global_pc\": \"10.48\"\n  },\n  {\n    \"name\": \"Biological Sciences\",\n    \"pubs_global\": \"9009961\",\n    \"pubs_global_pc\": \"7.62\"\n  },\n  {\n    \"name\": \"Chemical Sciences\",\n    \"pubs_global\": \"7876669\",\n    \"pubs_global_pc\": \"6.66\"\n  },\n  {\n    \"name\": \"Physical Sciences\",\n    \"pubs_global\": \"6149858\",\n    \"pubs_global_pc\": \"5.2\"\n  },\n  {\n    \"name\": \"Information and Computing Sciences\",\n    \"pubs_global\": \"5236596\",\n    \"pubs_global_pc\": \"4.43\"\n  },\n  {\n    \"name\": \"Mathematical Sciences\",\n    \"pubs_global\": \"5040655\",\n    \"pubs_global_pc\": \"4.26\"\n  },\n  {\n    \"name\": \"Psychology and Cognitive Sciences\",\n    \"pubs_global\": \"3871992\",\n    \"pubs_global_pc\": \"3.27\"\n  },\n  {\n    \"name\": \"Studies in Human Society\",\n    \"pubs_global\": \"3414299\",\n    \"pubs_global_pc\": \"2.89\"\n  },\n  {\n    \"name\": \"Language, Communication and Culture\",\n    \"pubs_global\": \"2531296\",\n    \"pubs_global_pc\": \"2.14\"\n  },\n  {\n    \"name\": \"History and Archaeology\",\n    \"pubs_global\": \"2357976\",\n    \"pubs_global_pc\": \"1.99\"\n  },\n  {\n    \"name\": \"Agricultural and Veterinary Sciences\",\n    \"pubs_global\": \"2108659\",\n    \"pubs_global_pc\": \"1.78\"\n  },\n  {\n    \"name\": \"Earth Sciences\",\n    \"pubs_global\": \"2059783\",\n    \"pubs_global_pc\": \"1.74\"\n  },\n  {\n    \"name\": \"Technology\",\n    \"pubs_global\": \"1956256\",\n    \"pubs_global_pc\": \"1.65\"\n  },\n  {\n    \"name\": \"Commerce, Management, Tourism and Services\",\n    \"pubs_global\": \"1830245\",\n    \"pubs_global_pc\": \"1.55\"\n  },\n  {\n    \"name\": \"Education\",\n    \"pubs_global\": \"1838328\",\n    \"pubs_global_pc\": \"1.55\"\n  },\n  {\n    \"name\": \"Economics\",\n    \"pubs_global\": \"1751713\",\n    \"pubs_global_pc\": \"1.48\"\n  },\n  {\n    \"name\": \"Philosophy and Religious Studies\",\n    \"pubs_global\": \"1680088\",\n    \"pubs_global_pc\": \"1.42\"\n  },\n  {\n    \"name\": \"Environmental Sciences\",\n    \"pubs_global\": \"1375226\",\n    \"pubs_global_pc\": \"1.16\"\n  },\n  {\n    \"name\": \"Law and Legal Studies\",\n    \"pubs_global\": \"902366\",\n    \"pubs_global_pc\": \"0.76\"\n  },\n  {\n    \"name\": \"Studies in Creative Arts and Writing\",\n    \"pubs_global\": \"644962\",\n    \"pubs_global_pc\": \"0.55\"\n  },\n  {\n    \"name\": \"Built Environment and Design\",\n    \"pubs_global\": \"491404\",\n    \"pubs_global_pc\": \"0.42\"\n  }\n]\n</code></pre>"},{"location":"queries/07/","title":"7. Finding Journals using string matching","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/07/#description","title":"Description","text":"<p>Returns data on how many publications have been published by journals that include a set of characters in the journal's title. Each row contains basic metadata about a single journal, plus a count of total publications indexed by Dimensions.</p>"},{"location":"queries/07/#query","title":"Query","text":"<pre><code>SELECT COUNT(id) AS pubs,\n  journal.id,\n  journal.title,\n  journal.issn,\n  journal.eissn,\n  publisher.name\nFROM\n  `dimensions-ai.data_analytics.publications`\nWHERE\n  LOWER(journal.title) LIKE '%medicine%'\nGROUP BY 2, 3, 4, 5, 6\nORDER BY pubs DESC\nLIMIT 20\n</code></pre>"},{"location":"queries/07/#breaking-it-down","title":"Breaking it down","text":"<p><code>WHERE LOWER(journal.title) LIKE '%medicine%'</code> is a trick to do a case-insensitive substring search:</p> <ul> <li><code>LIKE '%medicine%'</code> searches for any string that contains the characters <code>medicine</code>, in that order; the <code>%</code> characters indicate any other characters can show up on either side.</li> <li>Using <code>LOWER(journal.title)</code> here converts the entire journal title to lowercase before doing the string comparison, which means \"Medicine\" will be a match, as will \"MEDICINE,\" \"medicine,\" and so on.</li> </ul>"},{"location":"queries/07/#results","title":"Results","text":"<pre><code>[\n  {\n    \"pubs\": \"168838\",\n    \"id\": \"jour.1014075\",\n    \"title\": \"New England Journal of Medicine\",\n    \"issn\": \"0028-4793\",\n    \"eissn\": \"1533-4406\",\n    \"name\": \"Massachusetts Medical Society\"\n  },\n  {\n    \"pubs\": \"84137\",\n    \"id\": \"jour.1011551\",\n    \"title\": \"Medicine &amp; Science in Sports &amp; Exercise\",\n    \"issn\": \"0195-9131\",\n    \"eissn\": \"1530-0315\",\n    \"name\": \"Wolters Kluwer\"\n  },\n  {\n    \"pubs\": \"58776\",\n    \"id\": \"jour.1017222\",\n    \"title\": \"Annals of Internal Medicine\",\n    \"issn\": \"0003-4819\",\n    \"eissn\": \"1539-3704\",\n    \"name\": \"American College of Physicians\"\n  },\n  {\n    \"pubs\": \"52827\",\n    \"id\": \"jour.1312267\",\n    \"title\": \"Journal of the Royal Society of Medicine\",\n    \"issn\": \"0141-0768\",\n    \"eissn\": \"1758-1095\",\n    \"name\": \"SAGE Publications\"\n  },\n  {\n    \"pubs\": \"52384\",\n    \"id\": \"jour.1017256\",\n    \"title\": \"JAMA Internal Medicine\",\n    \"issn\": \"2168-6106\",\n    \"eissn\": \"2168-6114\",\n    \"name\": \"American Medical Association (AMA)\"\n  },\n  {\n    \"pubs\": \"47157\",\n    \"id\": \"jour.1027092\",\n    \"title\": \"Experimental Biology and Medicine\",\n    \"issn\": \"1535-3702\",\n    \"eissn\": \"1535-3699\",\n    \"name\": \"SAGE Publications\"\n  },\n  {\n    \"pubs\": \"46459\",\n    \"id\": \"jour.1016342\",\n    \"title\": \"Critical Care Medicine\",\n    \"issn\": \"0090-3493\",\n    \"eissn\": \"1530-0293\",\n    \"name\": \"Wolters Kluwer\"\n  },\n  {\n    \"pubs\": \"37666\",\n    \"id\": \"jour.1057918\",\n    \"title\": \"Journal of Molecular Medicine\",\n    \"issn\": \"0946-2716\",\n    \"eissn\": \"1432-1440\",\n    \"name\": \"Springer Nature\"\n  },\n  {\n    \"pubs\": \"34891\",\n    \"id\": \"jour.1017275\",\n    \"title\": \"Arizona Medicine\",\n    \"issn\": \"0093-0415\",\n    \"eissn\": \"1476-2978\",\n    \"name\": null\n  },\n  {\n    \"pubs\": \"31166\",\n    \"id\": \"jour.1014535\",\n    \"title\": \"The American Journal of Medicine\",\n    \"issn\": \"0002-9343\",\n    \"eissn\": \"1555-7162\",\n    \"name\": \"Elsevier\"\n  },\n  {\n    \"pubs\": \"29793\",\n    \"id\": \"jour.1017863\",\n    \"title\": \"Oral Surgery Oral Medicine Oral Pathology and Oral Radiology\",\n    \"issn\": \"2212-4403\",\n    \"eissn\": \"2212-4411\",\n    \"name\": \"Elsevier\"\n  },\n  {\n    \"pubs\": \"28529\",\n    \"id\": \"jour.1090935\",\n    \"title\": \"Annals of Emergency Medicine\",\n    \"issn\": \"0196-0644\",\n    \"eissn\": \"1097-6760\",\n    \"name\": \"Elsevier\"\n  },\n  {\n    \"pubs\": \"27453\",\n    \"id\": \"jour.1077253\",\n    \"title\": \"Medicine\",\n    \"issn\": \"0025-7974\",\n    \"eissn\": \"1536-5964\",\n    \"name\": \"Wolters Kluwer\"\n  },\n  {\n    \"pubs\": \"25713\",\n    \"id\": \"jour.1017316\",\n    \"title\": \"Bulletin of Experimental Biology and Medicine\",\n    \"issn\": \"0007-4888\",\n    \"eissn\": \"1573-8221\",\n    \"name\": \"Springer Nature\"\n  },\n  {\n    \"pubs\": \"24861\",\n    \"id\": \"jour.1077126\",\n    \"title\": \"Journal of Experimental Medicine\",\n    \"issn\": \"0022-1007\",\n    \"eissn\": \"1540-9538\",\n    \"name\": \"Rockefeller University Press\"\n  },\n  {\n    \"pubs\": \"24370\",\n    \"id\": \"jour.1319882\",\n    \"title\": \"Journal of Internal Medicine\",\n    \"issn\": \"0954-6820\",\n    \"eissn\": \"1365-2796\",\n    \"name\": \"Wiley\"\n  },\n  {\n    \"pubs\": \"23679\",\n    \"id\": \"jour.1017748\",\n    \"title\": \"Academic Medicine\",\n    \"issn\": \"1040-2446\",\n    \"eissn\": \"1938-808X\",\n    \"name\": \"Wolters Kluwer\"\n  },\n  {\n    \"pubs\": \"22723\",\n    \"id\": \"jour.1017031\",\n    \"title\": \"American Journal of Respiratory and Critical Care Medicine\",\n    \"issn\": \"1073-449X\",\n    \"eissn\": \"1535-4970\",\n    \"name\": \"American Thoracic Society\"\n  },\n  {\n    \"pubs\": \"22340\",\n    \"id\": \"jour.1036793\",\n    \"title\": \"Military Medicine\",\n    \"issn\": \"0026-4075\",\n    \"eissn\": \"1930-613X\",\n    \"name\": \"Oxford University Press (OUP)\"\n  },\n  {\n    \"pubs\": \"21837\",\n    \"id\": \"jour.1017021\",\n    \"title\": \"American Journal of Tropical Medicine and Hygiene\",\n    \"issn\": \"0002-9637\",\n    \"eissn\": \"1476-1645\",\n    \"name\": \"American Society of Tropical Medicine and Hygiene\"\n  }\n]\n</code></pre>"},{"location":"queries/08/","title":"8. Finding articles matching a specific affiliation string","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/08/#description","title":"Description","text":"<p>This query returns a list of publications and affiliations associated with a single institution. Each line represents a single affiliation string, and includes the publication ID, the institution ID, and the affiliation string as reported by the journal. </p> <p>Results are filtered to include only a single institution, specified by GRID ID (grid.69566.3a), and affiliation strings that include the phrase <code>\"school of medicine\"</code>.  For more details about working with nested fields, see the tutorial page on the topic.</p>"},{"location":"queries/08/#query","title":"Query","text":"<pre><code>SELECT\n  p.id,\n  aff.grid_id,\n  aff.raw_affiliation\nFROM\n  `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(authors) auth\nCROSS JOIN UNNEST(auth.affiliations_address) aff\nWHERE\n  year = 2020\n  AND aff.grid_id = \"grid.69566.3a\" -- Sendai, Japan\n  AND LOWER(aff.raw_affiliation) LIKE \"%school of medicine%\"\n</code></pre>"},{"location":"queries/08/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1120198400\",\n    \"grid_id\": \"grid.69566.3a\",\n    \"raw_affiliation\": \"Department of Neurosurgery, Tohoku University Graduate School of Medicine, Sendai, Miyagi, Japan\"\n  },\n  {\n    \"id\": \"pub.1117164397\",\n    \"grid_id\": \"grid.69566.3a\",\n    \"raw_affiliation\": \"Division of Cardiovascular Surgery, Tohoku University Graduate School of Medicine, 1-1 Seiryo-machi, Aoba-ku, Sendai, Miyagi, Japan.\"\n  },\n  {\n    \"id\": \"pub.1120113207\",\n    \"grid_id\": \"grid.69566.3a\",\n    \"raw_affiliation\": \"Division of Internal Medicine and Hypertension Unit Division of Cardiology, Department of Medical Sciences, University of Torino, Torino Division of Internal Medicine, Department of Medicine, University of Udine, Udine, Italy Division of Clinical Hypertension, Endocrinology and Metabolism, Tohoku University Graduate School of Medicine, Sendai, Japan.\"\n  },\n  {\n    \"id\": \"pub.1119863526\",\n    \"grid_id\": \"grid.69566.3a\",\n    \"raw_affiliation\": \"Division of Emergency and Critical Care Medicine, Tohoku University Graduate School of Medicine, Japan.\"\n  },\n  // many thousands more records...\n]\n</code></pre>"},{"location":"queries/08/#81-variant-get-unique-publication-records-with-affiliation-count","title":"8.1 Variant: get unique publication records with affiliation count","text":"<pre><code>SELECT\n  COUNT(aff) AS matching_affiliations,\n  id,\n  title.preferred AS title\nFROM\n  `dimensions-ai.data_analytics.publications`,\n  UNNEST(authors) auth,\n  UNNEST(auth.affiliations_address) AS aff\nWHERE\n  year = 2020\n  AND aff.grid_id = \"grid.69566.3a\"\n  AND LOWER(aff.raw_affiliation) LIKE \"%school of medicine%\"\nGROUP BY\n  id,\n  title\n</code></pre>"},{"location":"queries/08/#results-from-variant","title":"Results from variant","text":"<pre><code>[\n  {\n    \"matching_affiliations\": \"3\",\n    \"id\": \"pub.1123153684\",\n    \"title\": \"Management following endoscopic resection in elderly patients with early\u2010stage upper gastrointestinal neoplasia\"\n  },\n  {\n    \"matching_affiliations\": \"1\",\n    \"id\": \"pub.1124283456\",\n    \"title\": \"Unique Sex Steroid Profiles in Estrogen-Producing Adrenocortical Adenoma Associated with Bilateral Hyperaldosteronism\"\n  },\n  {\n    \"matching_affiliations\": \"5\",\n    \"id\": \"pub.1124295695\",\n    \"title\": \"Clinical implication of myocardial FDG uptake pattern in oncologic PET: retrospective comparison study with stress myocardial perfusion imaging as the reference standard\"\n  },\n  {\n    \"matching_affiliations\": \"7\",\n    \"id\": \"pub.1124238412\",\n    \"title\": \"Keap1 deletion accelerates mutant K-ras/p53-driven cholangiocarcinoma\"\n  },\n  // many more results...\n]\n</code></pre>"},{"location":"queries/09/","title":"9. Top publications by Altmetric score and research organization","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/09/#description","title":"Description","text":"<p>Retrieve publications for a selected research organization (using its GRID identifier) and sort them using their Altmetric Attention Score.</p>"},{"location":"queries/09/#query","title":"Query","text":"<pre><code>SELECT\n  id,\n  title.preferred as title,\n  ARRAY_LENGTH(authors) as authors,  -- include number of authors\n  altmetrics.score as altmetrics_score\nFROM\n  `dimensions-ai.data_analytics.publications`\nWHERE\n  year = 2020\n  AND 'grid.4991.5' in UNNEST(research_orgs)  -- a sample grid ID\nORDER BY\n  altmetrics.score DESC\nLIMIT 5  -- Get top 5 only\n</code></pre>"},{"location":"queries/09/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1130340155\",\n    \"title\": \"Two metres or one: what is the evidence for physical distancing in covid-19?\",\n    \"authors\": \"6\",\n    \"altmetrics_score\": \"15978\"\n  },\n  {\n    \"id\": \"pub.1129493369\",\n    \"title\": \"Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial\",\n    \"authors\": \"366\",\n    \"altmetrics_score\": \"15612\"\n  },\n  {\n    \"id\": \"pub.1133359801\",\n    \"title\": \"Safety and efficacy of the ChAdOx1 nCoV-19 vaccine (AZD1222) against SARS-CoV-2: an interim analysis of four randomised controlled trials in Brazil, South Africa, and the UK\",\n    \"authors\": \"766\",\n    \"altmetrics_score\": \"12292\"\n  },\n  {\n    \"id\": \"pub.1127239818\",\n    \"title\": \"Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial\",\n    \"authors\": \"46\",\n    \"altmetrics_score\": \"12036\"\n  },\n  {\n    \"id\": \"pub.1131721397\",\n    \"title\": \"Scientific consensus on the COVID-19 pandemic: we need to act now\",\n    \"authors\": \"31\",\n    \"altmetrics_score\": \"10534\"\n  }\n]\n</code></pre>"},{"location":"queries/10/","title":"10. Select publications matching selected concepts","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/10/#description","title":"Description","text":"<p>This query counts the number of publications about a set of tropical diseases. It filters publications by selecting only those that either refer to those diseases in the title, or that have been tagged with those diseases as concepts within the Dimensions database.</p> <p>Once publications have been found, the counts are broken down both by year and by publisher; the final list shows the top 10 publisher-years in which the most papers were published about those diseases.</p>"},{"location":"queries/10/#query","title":"Query","text":"<pre><code>SELECT\n  publisher.NAME AS publisher,\n  year,\n  COUNT(*) AS num_pub\nFROM\n  `dimensions-ai.data_analytics.publications`,\n  UNNEST(concepts) c\nWHERE\n  (LOWER(c.concept) IN UNNEST([\"buruli ulcer\", \"mycobacterium\", \"mycolactone\", \"bairnsdale ulcer\"])\n    OR REGEXP_CONTAINS(title.preferred, r\"(?i)/buruli ulcer|mycobacterium|mycolactone|bairnsdale ulcer/\"))\n  AND year &gt;= 2010\n  AND publisher IS NOT NULL\nGROUP BY\n  publisher,\n  year\nORDER BY\n  num_pub DESC,\n  year,\n  publisher\nLIMIT 10\n</code></pre>"},{"location":"queries/10/#results","title":"Results","text":"<pre><code>[\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2020\",\n    \"num_pub\": \"31602\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2018\",\n    \"num_pub\": \"29639\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2019\",\n    \"num_pub\": \"28941\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2017\",\n    \"num_pub\": \"28415\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2015\",\n    \"num_pub\": \"27299\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2011\",\n    \"num_pub\": \"25757\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2016\",\n    \"num_pub\": \"25149\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2013\",\n    \"num_pub\": \"23205\"\n  },\n  {\n    \"publisher\": \"Elsevier\",\n    \"year\": \"2014\",\n    \"num_pub\": \"22952\"\n  },\n  {\n    \"publisher\": \"Springer Nature\",\n    \"year\": \"2019\",\n    \"num_pub\": \"22072\"\n  }\n]\n</code></pre>"},{"location":"queries/11/","title":"11. Count of corresponding authors by publisher","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/11/#description","title":"Description","text":"<p>This query counts the number of unique corresponding authors that appear on publications from each publisher. The results list publishers with the count of unique researcher IDs.</p>"},{"location":"queries/11/#query","title":"Query","text":"<pre><code>SELECT\n  COUNT(DISTINCT researcher_id) AS tot,\n  publisher.name\nFROM\n  `dimensions-ai.data_analytics.publications`,\n  UNNEST(authors) aff\nWHERE\n  aff.corresponding IS TRUE\n  AND publisher.name IS NOT NULL\n  AND year &gt;= 2010\nGROUP BY\n  publisher.name\nORDER BY\n  tot DESC\nLIMIT 10\n</code></pre>"},{"location":"queries/11/#results","title":"Results","text":"<pre><code>[\n  {\n    \"tot\": \"1717859\",\n    \"name\": \"Springer Nature\"\n  },\n  {\n    \"tot\": \"1716636\",\n    \"name\": \"Elsevier\"\n  },\n  {\n    \"tot\": \"303497\",\n    \"name\": \"Institute of Electrical and Electronics Engineers (IEEE)\"\n  },\n  {\n    \"tot\": \"287259\",\n    \"name\": \"SAGE Publications\"\n  },\n  {\n    \"tot\": \"262973\",\n    \"name\": \"MDPI\"\n  },\n  {\n    \"tot\": \"141491\",\n    \"name\": \"Hindawi\"\n  },\n  {\n    \"tot\": \"122421\",\n    \"name\": \"Public Library of Science (PLoS)\"\n  },\n  {\n    \"tot\": \"84153\",\n    \"name\": \"Cold Spring Harbor Laboratory\"\n  },\n  {\n    \"tot\": \"75136\",\n    \"name\": \"Frontiers\"\n  },\n  {\n    \"tot\": \"68176\",\n    \"name\": \"Pleiades Publishing\"\n  }\n]\n</code></pre>"},{"location":"queries/12/","title":"12. Counting new vs recurring authors, for a specific journal","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/12/#description","title":"Description","text":"<p>This query evaluates a single journal (specified by journal ID - e.g. jour.1115214) and counts the number of authors per year that it has published. Authors are split into two categories: new authors who have never appeared in the journal before, and \"recurring\" authors who have already written at least one previous article in the journal.</p> <p>Note</p> <p>Because the query starts in 2011, some \"recurring\" authors are never counted as new authors: If an author publishes one paper each in, say, 2007 and 2014, they will appear as a recurring author in 2014, but the year in which they would be a \"new\" author is not displayed.</p>"},{"location":"queries/12/#query","title":"Query","text":"<pre><code>WITH authoryear AS (\n  -- how many papers has each individual researcher published in\n  -- the specified journal? Broken down by year.\n  SELECT\n    pubs.year,\n    author.researcher_id,\n    COUNT(pubs.id) AS numpubs\n  FROM\n    `dimensions-ai.data_analytics.publications` AS pubs\n  CROSS JOIN UNNEST(pubs.authors) AS author\n  WHERE\n    author.researcher_id IS NOT NULL\n    AND journal.id=\"jour.1115214\" -- Nature BioTechnology\n  GROUP BY\n    author.researcher_id, pubs.year\n),\nauthorfirst AS (\n  -- For each author, what year is their FIRST publication in\n  -- the specified journal?\n  SELECT researcher_id, MIN(year) AS minyear\n  FROM authoryear\n  GROUP BY researcher_id\n),\nauthorsummary AS (\n  -- Modify the author-level list of publications per\n  -- year by adding a new field, \"firstyear\", that indicates\n  -- whether this is the year in which they are \"new.\"\n  SELECT\n    ay.*,\n    IF(ay.year=af.minyear, TRUE, FALSE) AS firstyear\n  FROM authoryear ay\n  INNER JOIN authorfirst af\n    ON af.researcher_id=ay.researcher_id\n),\nnumauthors AS (\n  -- For each year, total up the new and recurring authors\n  SELECT year, firstyear,\n    COUNT(DISTINCT researcher_id) AS numresearchers\n  FROM authorsummary\n  WHERE year&gt;2010\n  GROUP BY year, firstyear\n)\n\n-- Finally, we rearrange the \"numauthors\" subquery so\n-- each year in the specified range only has a SINGLE ROW,\n-- indicating both the new and recurring authors.\nSELECT\n  year,\n  SUM(\n    CASE\n      WHEN firstyear\n        THEN numresearchers\n      ELSE 0\n    END\n  ) AS num_first,\n  SUM(\n    CASE\n      WHEN NOT firstyear\n        THEN numresearchers\n      ELSE 0\n    END\n  ) AS num_recurring\nFROM numauthors\nGROUP BY year\nORDER BY year\n</code></pre>"},{"location":"queries/12/#results","title":"Results","text":"<pre><code>[\n  {\n    \"year\": \"2011\",\n    \"num_first\": \"1041\",\n    \"num_recurring\": \"352\"\n  },\n  {\n    \"year\": \"2012\",\n    \"num_first\": \"859\",\n    \"num_recurring\": \"374\"\n  },\n  {\n    \"year\": \"2013\",\n    \"num_first\": \"927\",\n    \"num_recurring\": \"347\"\n  },\n  {\n    \"year\": \"2014\",\n    \"num_first\": \"1088\",\n    \"num_recurring\": \"338\"\n  },\n  {\n    \"year\": \"2015\",\n    \"num_first\": \"1044\",\n    \"num_recurring\": \"392\"\n  },\n  {\n    \"year\": \"2016\",\n    \"num_first\": \"1319\",\n    \"num_recurring\": \"350\"\n  },\n  {\n    \"year\": \"2017\",\n    \"num_first\": \"1074\",\n    \"num_recurring\": \"404\"\n  },\n  {\n    \"year\": \"2018\",\n    \"num_first\": \"1111\",\n    \"num_recurring\": \"419\"\n  },\n  {\n    \"year\": \"2019\",\n    \"num_first\": \"1219\",\n    \"num_recurring\": \"447\"\n  },\n  {\n    \"year\": \"2020\",\n    \"num_first\": \"1611\",\n    \"num_recurring\": \"570\"\n  },\n  {\n    \"year\": \"2021\",\n    \"num_first\": \"411\",\n    \"num_recurring\": \"189\"\n  }\n]\n</code></pre>"},{"location":"queries/13/","title":"13. Funding by journal","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/13/#description","title":"Description","text":"<p>This query looks at all publications from a single specified journal and counts the total number of grants associated with those publications. </p> <p>The results list each funding agency with a count of how many papers it has been linked to, combined with the number of grants from that agency that were referenced by those papers.</p>"},{"location":"queries/13/#query","title":"Query","text":"<pre><code>WITH funding AS (\n  SELECT\n    funding.grid_id AS funders,\n    COUNT(id) AS pubs,\n    COUNT(funding.grant_id) AS grants\n  FROM\n    `dimensions-ai.data_analytics.publications`,\n    UNNEST(funding_details) AS funding\n  WHERE\n    journal.id = \"jour.1113716\" -- nature medicine\n  GROUP BY\n    funders\n)\n\nSELECT\n  funding.*,\n  grid.name\nFROM funding\nINNER JOIN `dimensions-ai.data_analytics.grid` grid\n  ON funding.funders = grid.id\nORDER BY\n  pubs DESC,\n  grants DESC\nLIMIT 10\n</code></pre>"},{"location":"queries/13/#results","title":"Results","text":"<pre><code>[\n  {\n    \"funders\": \"grid.48336.3a\",\n    \"pubs\": \"2727\",\n    \"grants\": \"2510\",\n    \"name\": \"National Cancer Institute\"\n  },\n  {\n    \"funders\": \"grid.419681.3\",\n    \"pubs\": \"2043\",\n    \"grants\": \"1910\",\n    \"name\": \"National Institute of Allergy and Infectious Diseases\"\n  },\n  {\n    \"funders\": \"grid.419635.c\",\n    \"pubs\": \"1642\",\n    \"grants\": \"1584\",\n    \"name\": \"National Institute of Diabetes and Digestive and Kidney Diseases\"\n  },\n  {\n    \"funders\": \"grid.279885.9\",\n    \"pubs\": \"1641\",\n    \"grants\": \"1554\",\n    \"name\": \"National Heart Lung and Blood Institute\"\n  },\n  {\n    \"funders\": \"grid.416870.c\",\n    \"pubs\": \"717\",\n    \"grants\": \"673\",\n    \"name\": \"National Institute of Neurological Disorders and Stroke\"\n  },\n  {\n    \"funders\": \"grid.419475.a\",\n    \"pubs\": \"585\",\n    \"grants\": \"553\",\n    \"name\": \"National Institute on Aging\"\n  },\n  {\n    \"funders\": \"grid.14105.31\",\n    \"pubs\": \"547\",\n    \"grants\": \"431\",\n    \"name\": \"Medical Research Council\"\n  },\n  {\n    \"funders\": \"grid.54432.34\",\n    \"pubs\": \"512\",\n    \"grants\": \"447\",\n    \"name\": \"Japan Society for the Promotion of Science\"\n  },\n  {\n    \"funders\": \"grid.280785.0\",\n    \"pubs\": \"465\",\n    \"grants\": \"446\",\n    \"name\": \"National Institute of General Medical Sciences\"\n  },\n  {\n    \"funders\": \"grid.270680.b\",\n    \"pubs\": \"410\",\n    \"grants\": \"187\",\n    \"name\": \"European Commission\"\n  }\n]\n</code></pre>"},{"location":"queries/14/","title":"14. Extracting complex publications records","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/14/#description","title":"Description","text":"<p>The query below combines various techniques in order to extract full publication metadata records that include both single-value metadata and unpacked lists. </p> <p>Note: we use <code>LEFT JOIN</code> clauses in order to ensure we obtain all records, not just the ones that have non-null values in the nested objects.</p>"},{"location":"queries/14/#query","title":"Query","text":"<pre><code>SELECT\n p.id,\n p.title.preferred AS title,\n p.doi,\n p.year,\n COALESCE(p.journal.title, p.proceedings_title.preferred, p.book_title.preferred, p.book_series_title.preferred) AS venue,\n p.type,\n p.date AS date_publication,\n p.date_inserted,\n p.altmetrics.score AS altmetrics_score,\n p.metrics.times_cited,\n grid.id AS gridid,\n grid.name AS gridname,\n grid.address.country AS gridcountry,\n grid.address.city AS gridcity,\n open_access_categories,\n cat_for.name AS category_for,\nFROM `dimensions-ai.data_analytics.publications` p\nLEFT JOIN UNNEST(research_orgs) AS research_orgs_grids\nLEFT JOIN `dimensions-ai.data_analytics.grid` grid\n  ON grid.id=research_orgs_grids\nLEFT JOIN UNNEST(p.open_access_categories_v2) AS open_access_categories\nLEFT JOIN UNNEST(p.category_for.first_level.full) AS cat_for\nWHERE\n  EXTRACT(YEAR FROM date_inserted) &gt;= 2020\n</code></pre>"},{"location":"queries/14/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1124854415\",\n    \"title\": \"Gabinetto armonico pieno d'istromenti sonori\",\n    \"doi\": \"10.5479/sil.744616.39088011251444\",\n    \"year\": \"1722\",\n    \"venue\": null,\n    \"type\": \"monograph\",\n    \"date_publication\": \"1722\",\n    \"date_inserted\": \"2020-02-15 01:10:52 UTC\",\n    \"altmetrics_score\": null,\n    \"times_cited\": \"3\",\n    \"gridid\": null,\n    \"gridname\": null,\n    \"gridcountry\": null,\n    \"gridcity\": null,\n    \"open_access_categories\": \"oa_all\",\n    \"category_for\": null\n  },\n  {\n    \"id\": \"pub.1124854415\",\n    \"title\": \"Gabinetto armonico pieno d'istromenti sonori\",\n    \"doi\": \"10.5479/sil.744616.39088011251444\",\n    \"year\": \"1722\",\n    \"venue\": null,\n    \"type\": \"monograph\",\n    \"date_publication\": \"1722\",\n    \"date_inserted\": \"2020-02-15 01:10:52 UTC\",\n    \"altmetrics_score\": null,\n    \"times_cited\": \"3\",\n    \"gridid\": null,\n    \"gridname\": null,\n    \"gridcountry\": null,\n    \"gridcity\": null,\n    \"open_access_categories\": \"bronze\",\n    \"category_for\": null\n  },\n  // many more entries here...\n]\n</code></pre>"},{"location":"queries/15/","title":"15. Top N publications by citations percentile","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/15/#description","title":"Description","text":"<p>This query sorts all engineering publications from 2020 by their total citations and returns those in the top 1%. </p> <p>Engineering publications are determined by evaluating Field of Research classifications that are applied to the publications. <code>\"09\"</code> is the top-level code assigned to \"Engineering.\"</p>"},{"location":"queries/15/#query","title":"Query","text":"<pre><code>WITH pubs AS (\n  SELECT\n    p.id as id,\n    p.title.preferred as title,\n    p.citations_count as citations,\n  FROM\n    `dimensions-ai.data_analytics.publications` p\n  WHERE\n    year = 2020\n    AND \"09\" IN UNNEST(category_for.first_level.codes)\n),\nranked_pubs AS (\n  SELECT\n    p.*,\n    PERCENT_RANK() OVER (ORDER BY p.citations DESC) citation_percentile\n  FROM\n    pubs p\n)\n\nSELECT * FROM ranked_pubs\nWHERE\n  citation_percentile &lt;= 0.01\nORDER BY\n  citation_percentile ASC\n</code></pre>"},{"location":"queries/15/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1129408972\",\n    \"title\": \"Estimation of total flavonoid content in propolis by two complementary colometric methods\",\n    \"citations\": \"1014\",\n    \"citation_percentile\": \"0.0\"\n  },\n  {\n    \"id\": \"pub.1122861707\",\n    \"title\": \"Mercury 4.0: from visualization to analysis, design and prediction\",\n    \"citations\": \"517\",\n    \"citation_percentile\": \"1.4502085399880502E-6\"\n  },\n  {\n    \"id\": \"pub.1126110231\",\n    \"title\": \"Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks\",\n    \"citations\": \"373\",\n    \"citation_percentile\": \"2.9004170799761005E-6\"\n  },\n  {\n    \"id\": \"pub.1125814051\",\n    \"title\": \"Analysis and forecast of COVID-19 spreading in China, Italy and France\",\n    \"citations\": \"348\",\n    \"citation_percentile\": \"4.350625619964151E-6\"\n  },\n  {\n    \"id\": \"pub.1121839330\",\n    \"title\": \"A Vision of 6G Wireless Systems: Applications, Trends, Technologies, and Open Research Problems\",\n    \"citations\": \"327\",\n    \"citation_percentile\": \"5.800834159952201E-6\"\n  },\n  {\n    \"id\": \"pub.1125821215\",\n    \"title\": \"The Role of Telehealth in Reducing the Mental Health Burden from COVID-19\",\n    \"citations\": \"307\",\n    \"citation_percentile\": \"7.251042699940251E-6\"\n  },\n  // many more entries here...\n]\n</code></pre>"},{"location":"queries/16/","title":"16. Citations by journal, for a specific publisher","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/16/#description","title":"Description","text":"<p>This query returns a list of journals that have cited a publisher's articles in 2020, ordered by how many citations appeared in each journal.</p>"},{"location":"queries/16/#query","title":"Query","text":"<pre><code>WITH publisher_pubs AS (\n  -- get a list of all publication IDs associated with a single publisher\n  SELECT id FROM `dimensions-ai.data_analytics.publications`\n  WHERE\n    publisher.id = \"pblshr.1000340\"  -- Public Library of Science (PLoS)\n    AND type = \"article\"\n)\n\n-- then find all publications that CITE that publisher's papers\nSELECT\n  COUNT(p.id) as tot,\n  p.journal.title as journal\nFROM `dimensions-ai.data_analytics.publications` p,\n  UNNEST(p.reference_ids) r\nWHERE\n  p.year = 2020 AND p.type = \"article\"      -- restrict to articles with a published year of 2020\n  AND p.publisher.id &lt;&gt; \"pblshr.1000340\"    -- where the publisher is not the same as the pusblisher above\n  AND r IN (SELECT id FROM publisher_pubs)   -- the publication must reference a publishers publication\nGROUP BY journal\nORDER BY tot DESC\nLIMIT 10\n</code></pre>"},{"location":"queries/16/#results","title":"Results","text":"<pre><code>[\n  {\n    \"tot\": \"26309\",\n    \"journal\": \"Scientific Reports\"\n  },\n  {\n    \"tot\": \"18911\",\n    \"journal\": \"International Journal of Molecular Sciences\"\n  },\n  {\n    \"tot\": \"8533\",\n    \"journal\": \"Frontiers in Microbiology\"\n  },\n  {\n    \"tot\": \"7787\",\n    \"journal\": \"Frontiers in Immunology\"\n  },\n  {\n    \"tot\": \"6999\",\n    \"journal\": \"International Journal of Environmental Research and Public Health\"\n  },\n  {\n    \"tot\": \"6446\",\n    \"journal\": \"Nature Communications\"\n  },\n  {\n    \"tot\": \"6199\",\n    \"journal\": \"Cells\"\n  },\n  {\n    \"tot\": \"5706\",\n    \"journal\": \"Cancers\"\n  },\n  {\n    \"tot\": \"5036\",\n    \"journal\": \"Microorganisms\"\n  },\n  {\n    \"tot\": \"5019\",\n    \"journal\": \"Nutrients\"\n  }\n]\n</code></pre>"},{"location":"queries/17/","title":"17. One-degree citation network for a single publication","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/17/#description","title":"Description","text":"<p>This query generates a basic (incoming) citation network for a single publication (the \"root node\"). </p> <p>\"Level 1\" publications in the query are publications that are citing the root publication. \"Level 2\" publications are citing Level 1 publications. Each row describes a vertex in the network and what year the citation occurred.</p>"},{"location":"queries/17/#query","title":"Query","text":"<pre><code>WITH level1 AS (\n  SELECT \"pub.1099396382\" as citation_from, citations.id AS citation_to,\n    1 AS level, citations.year as citation_year\n  FROM `dimensions-ai.data_analytics.publications` p\n    CROSS JOIN UNNEST(citations) AS citations\n  WHERE p.id=\"pub.1099396382\" -- starting node defined here\n),\n\nlevel2 AS (\n  SELECT l.citation_to AS citation_from, citations.id AS citation_to,\n    2 AS level, citations.year AS citation_year\n  FROM `dimensions-ai.data_analytics.publications` p\n    CROSS JOIN UNNEST(citations) as citations, level1 l\n  where p.id = l.citation_to\n)\n\nSELECT * from level1\nUNION ALL\nSELECT * from level2\n</code></pre>"},{"location":"queries/17/#results","title":"Results","text":"<pre><code>[\n  {\n    \"citation_from\": \"pub.1084215961\",\n    \"citation_to\": \"pub.1135701707\",\n    \"level\": \"2\",\n    \"citation_year\": \"2021\"\n  },\n  {\n    \"citation_from\": \"pub.1084215961\",\n    \"citation_to\": \"pub.1126671825\",\n    \"level\": \"2\",\n    \"citation_year\": \"2020\"\n  },\n  {\n    \"citation_from\": \"pub.1084215961\",\n    \"citation_to\": \"pub.1101037901\",\n    \"level\": \"2\",\n    \"citation_year\": \"2018\"\n  },\n  {\n    \"citation_from\": \"pub.1084215961\",\n    \"citation_to\": \"pub.1120764290\",\n    \"level\": \"2\",\n    \"citation_year\": \"2019\"\n  },\n  {\n    \"citation_from\": \"pub.1084215961\",\n    \"citation_to\": \"pub.1103943561\",\n    \"level\": \"2\",\n    \"citation_year\": \"2018\"\n  },\n  // many more entries here...\n]\n</code></pre>"},{"location":"queries/17/#171-variant-one-degree-references-network-for-a-single-publication","title":"17.1 Variant: one-degree references network for a single publication","text":"<p>We could use the same approach in order to build a references network (=outgoing citations). </p> <p>This can be achieved via the publications field <code>references_ids</code>. </p> <pre><code>WITH level1 AS (\n  SELECT \"pub.1099396382\" as references_from, reference AS reference_to,\n    1 AS level\n  FROM `dimensions-ai.data_analytics.publications` p\n    CROSS JOIN UNNEST(reference_ids) AS reference\n  WHERE p.id=\"pub.1099396382\" -- starting node defined here\n),\n\nlevel2 AS (\n  SELECT l.reference_to AS reference_from, reference AS reference_to,\n    2 AS level\n  FROM `dimensions-ai.data_analytics.publications` p\n    CROSS JOIN UNNEST(reference_ids) as reference, level1 l\n  where p.id = l.reference_to\n)\n\nSELECT * from level1\nUNION ALL\nSELECT * from level2\n</code></pre>"},{"location":"queries/18/","title":"18. Incoming citations for a journal","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/18/#description","title":"Description","text":"<p>This query lists incoming citations per year for a single journal. Results are split out by year and by the type of publication that cited the journal (article, book, etc).</p> <p>Note</p> <p>There is an important clause in the <code>SELECT</code> statement that changes the behavior of this query: If you use <code>COUNT(DISTINCT id)</code>, the query counts unique publications that cited the selected journal. If you use <code>COUNT(id)</code> instead, this counts citations: If one publication cites multiple papers from a single journal, the latter query will count each citation separately.</p>"},{"location":"queries/18/#query","title":"Query","text":"<pre><code>SELECT\n  COUNT(DISTINCT id) AS totcount,  year, type\nFROM\n  `dimensions-ai.data_analytics.publications`\nWHERE\n  id IN (\n    SELECT citing_pubs.id\n    FROM\n      `dimensions-ai.data_analytics.publications`,\n      UNNEST(citations) AS citing_pubs\n    WHERE journal.id = \"jour.1115214\" -- Nature Biotechnology\n  )\n  AND year &gt;= 2005\nGROUP BY year, type\nORDER BY year, type\n</code></pre>"},{"location":"queries/18/#results","title":"Results","text":"<pre><code>[\n  {\n    \"totcount\": \"13064\",\n    \"year\": \"2005\",\n    \"type\": \"article\"\n  },\n  {\n    \"totcount\": \"12\",\n    \"year\": \"2005\",\n    \"type\": \"book\"\n  },\n  {\n    \"totcount\": \"1492\",\n    \"year\": \"2005\",\n    \"type\": \"chapter\"\n  },\n  {\n    \"totcount\": \"23\",\n    \"year\": \"2005\",\n    \"type\": \"monograph\"\n  },\n  {\n    \"totcount\": \"192\",\n    \"year\": \"2005\",\n    \"type\": \"proceeding\"\n  },\n  // more entries here...\n]\n</code></pre>"},{"location":"queries/19/","title":"19. Outgoing citations from a journal","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/19/#description","title":"Description","text":"<p>This query counts outgoing citations per year from a single journal. Results are broken down by year and by the type of publication being cited (article, book, etc).</p> <p>Note</p> <p>There is an important clause in the <code>SELECT</code> statement that changes the behavior of this query: If you use <code>COUNT(DISTINCT id)</code>, the query counts unique publications that have been cited in the selected journal. If you use <code>COUNT(id)</code> instead, this counts citations: If one publication it cited by multiple papers in a single journal, the latter query will count each citation separately.</p>"},{"location":"queries/19/#query","title":"Query","text":"<pre><code>SELECT\n  COUNT(DISTINCT id) AS totcount,  year, type\nFROM\n  `dimensions-ai.data_analytics.publications`\nWHERE\n  id IN (\n    SELECT DISTINCT reference_pubs\n    FROM\n      `dimensions-ai.data_analytics.publications`,\n      UNNEST(reference_ids) AS reference_pubs\n    WHERE journal.id = \"jour.1115214\" -- Nature Biotechnology\n  )\n  AND year &gt;= 2005\nGROUP BY year, type\nORDER BY year, type\n</code></pre>"},{"location":"queries/19/#results","title":"Results","text":"<pre><code>[\n  {\n    \"totcount\": \"3757\",\n    \"year\": \"2005\",\n    \"type\": \"article\"\n  },\n  {\n    \"totcount\": \"12\",\n    \"year\": \"2005\",\n    \"type\": \"book\"\n  },\n  {\n    \"totcount\": \"60\",\n    \"year\": \"2005\",\n    \"type\": \"chapter\"\n  },\n  {\n    \"totcount\": \"9\",\n    \"year\": \"2005\",\n    \"type\": \"monograph\"\n  },\n  {\n    \"totcount\": \"8\",\n    \"year\": \"2005\",\n    \"type\": \"proceeding\"\n  },\n  // more entries here...\n]\n</code></pre>"},{"location":"queries/20/","title":"20. International collaboration of an organisation in a field","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/20/#description","title":"Description","text":"<p>This query looks at international collaborations by year, with additional filters for institution and Field of Research. The <code>pubcounts</code> subquery counts the total number of relevant papers that have authors from multiple countries, then the final query divides this number by the total number of relevant papers in that category.</p>"},{"location":"queries/20/#query","title":"Query","text":"<pre><code>WITH pubcounts AS (\n  SELECT year,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &gt; 1) AS intl,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &lt;= 1) AS domestic\n  FROM `dimensions-ai.data_analytics.publications` p\n  WHERE\n    year &gt;= 2015\n    AND \"0601\" in UNNEST(category_for.second_level.codes) -- field defined here\n    AND \"grid.4991.5\" in UNNEST(research_orgs) -- org defined here\n  GROUP BY year\n  ORDER BY year DESC\n)\n\nSELECT\n  pubcounts.year, pubcounts.intl, pubcounts.domestic,\n  ROUND(\n    (pubcounts.intl*100)/(pubcounts.domestic + pubcounts.intl)\n  , 1) AS percentagecollab\nFROM pubcounts\nORDER BY year DESC\n</code></pre>"},{"location":"queries/20/#results","title":"Results","text":"<pre><code>[\n  {\n    \"year\": \"2021\",\n    \"intl\": \"184\",\n    \"domestic\": \"92\",\n    \"percentagecollab\": \"66.7\"\n  },\n  {\n    \"year\": \"2020\",\n    \"intl\": \"606\",\n    \"domestic\": \"307\",\n    \"percentagecollab\": \"66.4\"\n  },\n  {\n    \"year\": \"2019\",\n    \"intl\": \"534\",\n    \"domestic\": \"262\",\n    \"percentagecollab\": \"67.1\"\n  },\n  {\n    \"year\": \"2018\",\n    \"intl\": \"471\",\n    \"domestic\": \"246\",\n    \"percentagecollab\": \"65.7\"\n  },\n  {\n    \"year\": \"2017\",\n    \"intl\": \"460\",\n    \"domestic\": \"277\",\n    \"percentagecollab\": \"62.4\"\n  },\n  {\n    \"year\": \"2016\",\n    \"intl\": \"422\",\n    \"domestic\": \"235\",\n    \"percentagecollab\": \"64.2\"\n  },\n  {\n    \"year\": \"2015\",\n    \"intl\": \"369\",\n    \"domestic\": \"268\",\n    \"percentagecollab\": \"57.9\"\n  }\n]\n</code></pre>"},{"location":"queries/21/","title":"21. International collaboration rate of individuals, with context","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/21/#description","title":"Description","text":"<p>This query determines the yearly proportion of publications from a single author that include international collaborators. It also calculates the same rate for the author's current institution (for papers in the same field), and the author's current country. A few highlights:</p> <ul> <li>We can simplify the query by collecting all the author-specific data up front, in the <code>researcher_details</code> and <code>researcher_field</code> subqueries, and referring to it later simply as something like <code>(SELECT org FROM researcher_details)</code>.</li> <li> <p>When we calculate the percentage of papers that are international collaborations, it would be much simpler to simply write <code>(intl*100) / (intl + domestic)</code>. However, in situations where there are zero papers returned for that particular category, this will return an error because the query would call for dividing by zero. We can avoid this by using the <code>COALESCE</code> function.</p> </li> <li> <p>The <code>COUNTIF</code> function is used multiple times here\u2014it can be helpful in situations where you want to maintain separate counts for different conditionals without using <code>COUNT</code> and lots of separate subqueries.</p> </li> </ul>"},{"location":"queries/21/#query","title":"Query","text":"<pre><code>WITH researcher_details AS (\n  -- grab the basic metadata about the selected researcher\n  SELECT r.id, r.current_research_org AS org, grid.address.country\n  FROM `dimensions-ai.data_analytics.researchers` r\n  INNER JOIN `dimensions-ai.data_analytics.grid` grid\n    ON r.current_research_org=grid.id\n  WHERE r.id=\"ur.0761121015.96\" -- researcher defined here\n),\nresearcher_field AS (\n  -- determines the field of research code in which\n  -- the researcher has most frequently authored papers\n  SELECT for2, COUNT(DISTINCT p.id)\n  FROM `dimensions-ai.data_analytics.publications` p\n  CROSS JOIN UNNEST(category_for.second_level.codes) for2\n  WHERE (SELECT id FROM researcher_details) IN UNNEST(researcher_ids)\n  GROUP BY 1\n  ORDER BY 2 DESC\n  LIMIT 1\n),\ncounts_researcher AS (\n  -- count how many publications from the selected researcher\n  -- include authors from multiple countries\n  SELECT year,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &gt; 1) AS intl,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &lt;= 1) AS domestic\n  FROM `dimensions-ai.data_analytics.publications` p\n  WHERE\n    year &gt;= 2015\n    AND (SELECT id FROM researcher_details) IN UNNEST(researcher_ids)\n  GROUP BY year\n),\ncounts_org AS (\n  -- Count how many publications from the selected researcher's\n  -- CURRENT ORGANIZATION that include authors from multiple countries.\n  -- We count only publications in the author's primary field of\n  -- research, and EXCLUDE papers they co-authored.\n  SELECT year,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &gt; 1) AS intl,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &lt;= 1) AS domestic\n  FROM `dimensions-ai.data_analytics.publications` p\n  WHERE\n    year &gt;= 2015\n    AND (SELECT org FROM researcher_details) IN UNNEST(research_orgs)\n    AND (SELECT for2 FROM researcher_field) IN UNNEST(category_for.second_level.codes)\n    AND (SELECT id FROM researcher_details) NOT IN UNNEST(researcher_ids)\n  GROUP BY year\n),\ncounts_country AS (\n  -- Count how many publications from the selected researcher's\n  -- current COUNTRY that include authors from multiple countries.\n  -- We count only publications in the author's primary field of\n  -- research, and EXCLUDE papers they co-authored.\n  SELECT year,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &gt; 1) AS intl,\n    COUNTIF(ARRAY_LENGTH(p.research_org_countries) &lt;= 1) AS domestic\n  FROM `dimensions-ai.data_analytics.publications` p\n  WHERE\n    year &gt;= 2015\n    AND (SELECT country FROM researcher_details) IN UNNEST(research_org_country_names)\n    AND (SELECT for2 FROM researcher_field) IN UNNEST(category_for.second_level.codes)\n    AND (SELECT id FROM researcher_details) NOT IN UNNEST(researcher_ids)\n  GROUP BY year\n),\nraw_percents AS (\n  -- Divide international collabs by total collabs in each category\n  SELECT researcher.year,\n  (researcher.intl*100) /\n    COALESCE(researcher.intl + researcher.domestic, 1) AS intl_researcher,\n  (org.intl*100) /\n    COALESCE(org.intl + org.domestic, 1) AS intl_org,\n  (country.intl*100) /\n    COALESCE(country.intl + country.domestic, 1) AS intl_country,\n  FROM counts_researcher researcher\n  LEFT JOIN counts_org org ON researcher.year=org.year\n  LEFT JOIN counts_country country ON researcher.year=country.year\n  ORDER BY researcher.year DESC\n)\n\n-- Pull the percentages from the raw_percents table and round them\nSELECT year,\n  ROUND(intl_researcher, 1) AS intl_researcher,\n  ROUND(intl_org, 1) AS intl_org,\n  ROUND(intl_country, 1) AS intl_country\nFROM raw_percents\n</code></pre>"},{"location":"queries/21/#results","title":"Results","text":"<pre><code>[\n  {\n    \"year\": \"2021\",\n    \"intl_researcher\": \"44.4\",\n    \"intl_org\": \"53.8\",\n    \"intl_country\": \"59.0\"\n  },\n  {\n    \"year\": \"2020\",\n    \"intl_researcher\": \"50.0\",\n    \"intl_org\": \"54.7\",\n    \"intl_country\": \"48.7\"\n  },\n  {\n    \"year\": \"2019\",\n    \"intl_researcher\": \"53.6\",\n    \"intl_org\": \"48.9\",\n    \"intl_country\": \"45.8\"\n  },\n  {\n    \"year\": \"2018\",\n    \"intl_researcher\": \"57.6\",\n    \"intl_org\": \"49.5\",\n    \"intl_country\": \"42.5\"\n  },\n  {\n    \"year\": \"2017\",\n    \"intl_researcher\": \"53.3\",\n    \"intl_org\": \"42.9\",\n    \"intl_country\": \"43.4\"\n  },\n  {\n    \"year\": \"2016\",\n    \"intl_researcher\": \"40.6\",\n    \"intl_org\": \"37.4\",\n    \"intl_country\": \"39.4\"\n  },\n  {\n    \"year\": \"2015\",\n    \"intl_researcher\": \"38.5\",\n    \"intl_org\": \"41.5\",\n    \"intl_country\": \"40.2\"\n  }\n]\n</code></pre>"},{"location":"queries/22/","title":"22. Incoming citations for a single publication, by journal","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/22/#description","title":"Description","text":"<p>This query counts incoming citations for a single publication. </p> <p>Results are grouped by journal, and include the publisher of each journal in the list. </p> <p>The <code>publications</code> table has a <code>citations</code> field that includes the publication IDs of all incoming citations for a given paper. We fetch that list in the <code>citing</code> subquery below, then query the <code>publications</code> table for information about all the IDs in that list.</p> <p>The <code>COALESCE</code> function is used here to minimize the number of null fields in the final results\u2014if an incoming citation is published in a book rather than a journal, for example, then the <code>journal.title</code> field will be <code>NULL</code>, and the <code>boook_title.preferred</code> field is likely to have the value we want. Occasionally, the <code>publisher</code> field is unavailable, so we use <code>COALESCE(p.publisher.name, \"(unknown)\")</code> to make sure there aren't any blank fields.</p> <p>Note</p> <p>The list of citing publications is determined by the clause <code>WHERE p.id='pub.1113640622'</code>. This can be changed to be as broad or narrow as you wish\u2014changing it to something like <code>WHERE journal.title='eLife'</code>, for example, would return incoming citations to an entire journal rather than a single paper.</p>"},{"location":"queries/22/#query","title":"Query","text":"<pre><code>WITH citing AS (\n    SELECT citing_pubs.id\n    FROM `dimensions-ai.data_analytics.publications` p\n    CROSS JOIN UNNEST(citations) AS citing_pubs\n    WHERE p.id='pub.1113640622' -- publication of interest here\n)\n\nSELECT\n    COALESCE(\n        p.journal.title,\n        CONCAT(p.book_title.preferred, ' (book)'),\n        p.proceedings_title.preferred,\n        CONCAT(p.title.preferred, ' (book)') -- some books have this field instead of book_title\n    ) AS journal,\n    COALESCE(p.publisher.name, \"(unknown)\") AS publisher,\n    p.type AS pubtype, COUNT(p.id) AS citations\nFROM `dimensions-ai.data_analytics.publications` p\nWHERE p.id IN (SELECT id FROM citing)\nGROUP BY 1,2,3\nORDER BY 4 DESC\n</code></pre>"},{"location":"queries/22/#results","title":"Results","text":"<pre><code>[\n  {\n    \"journal\": \"bioRxiv\",\n    \"publisher\": \"Cold Spring Harbor Laboratory\",\n    \"pubtype\": \"preprint\",\n    \"citations\": \"15\"\n  },\n  {\n    \"journal\": \"PLOS Biology\",\n    \"publisher\": \"Public Library of Science (PLoS)\",\n    \"pubtype\": \"article\",\n    \"citations\": \"5\"\n  },\n  {\n    \"journal\": \"eLife\",\n    \"publisher\": \"eLife\",\n    \"pubtype\": \"article\",\n    \"citations\": \"3\"\n  },\n  {\n    \"journal\": \"medRxiv\",\n    \"publisher\": \"Cold Spring Harbor Laboratory\",\n    \"pubtype\": \"preprint\",\n    \"citations\": \"3\"\n  },\n  {\n    \"journal\": \"Scientometrics\",\n    \"publisher\": \"Springer Nature\",\n    \"pubtype\": \"article\",\n    \"citations\": \"3\"\n  },\n  // more entries here...\n]\n</code></pre>"},{"location":"queries/23/","title":"23. Citing authors by country","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/23/#description","title":"Description","text":"<p>This query counts incoming citations for a single publication. The final results count authors, rather than publications, and group counts by the country of each author's current affiliation. </p> <p>The strategy is similar to query 22, which groups citations by journal: the <code>publications</code> table has a <code>citations</code> field that includes the publication IDs of all incoming citations for a given paper. We fetch that list in the <code>citing</code> subquery below, then query the <code>publications</code> table to get a list of authors for all the publications citing the paper of interest. We join the <code>researchers</code> table to get each author's current affiliation. In the final query, we use the <code>grid</code> table to associate each author affiliation to a single country, then group all results by those countries.</p> <p>Note</p> <p>The list of citing publications is determined by the clause <code>WHERE p.id='pub.1113640622'</code>. This can be changed to be as broad or narrow as you wish\u2014changing it to something like <code>WHERE journal.title='eLife'</code>, for example, would return incoming citations to an entire journal rather than a single paper.</p>"},{"location":"queries/23/#query","title":"Query","text":"<pre><code>WITH citing AS (\n    SELECT citing_pubs.id\n    FROM `dimensions-ai.data_analytics.publications` p\n    CROSS JOIN UNNEST(citations) AS citing_pubs\n    WHERE p.id='pub.1113640622' -- publication of interest\n),\npeople_and_grids as (\n  SELECT COUNT(DISTINCT auth.researcher_id) AS people,\n    res.current_research_org AS gridid\n  FROM `dimensions-ai.data_analytics.publications` pubs\n  CROSS JOIN UNNEST(authors) as auth\n  INNER JOIN `dimensions-ai.data_analytics.researchers` res\n    ON res.id=auth.researcher_id\n  WHERE pubs.id IN (SELECT id FROM citing)\n  GROUP BY gridid\n)\n\nSELECT people, address.country\nFROM people_and_grids\nINNER JOIN `dimensions-ai.data_analytics.grid` gridinfo\n  ON gridinfo.id=people_and_grids.gridid\nORDER BY people DESC\n</code></pre>"},{"location":"queries/23/#results","title":"Results","text":"<pre><code>[\n  {\n    \"people\": \"10\",\n    \"country\": \"Brazil\"\n  },\n  {\n    \"people\": \"5\",\n    \"country\": \"United States\"\n  },\n  {\n    \"people\": \"5\",\n    \"country\": \"United States\"\n  },\n  {\n    \"people\": \"5\",\n    \"country\": \"United States\"\n  },\n  {\n    \"people\": \"4\",\n    \"country\": \"Croatia\"\n  },\n  // more results here...\n]\n</code></pre>"},{"location":"queries/24/","title":"24. Organizations and sub-organizations","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/24/#description","title":"Description","text":"<p>This query shows how to extract information about a GRID organization together with all of its sub-organizations. </p> <p>Many organizations in the GRID database include parent-child relationships. For example, grid.495456.f (the United States Department of the Air Force page) has both parent and children institutions, so one can use the hierarchy when querying related data e.g. the total number of publications for each of these organizations. </p> <p>The query below shows how to leverage the <code>organization_recursive_child_ids</code> field in the <code>grid</code> table in order to achieve that. This field is prepopulated with all children institutions GRID IDs (recursively), hence it makes it easier to run this type of analyses. </p>"},{"location":"queries/24/#query","title":"Query","text":"<pre><code>WITH hierarchy AS (\n\n  SELECT\n    g.id AS parent,\n    g.name AS parent_name,\n    children,\n    g2.name AS children_name,\n  FROM\n    `dimensions-ai.data_analytics.grid` g\n  CROSS JOIN\n    UNNEST(organization_recursive_child_ids) AS children\n  INNER JOIN\n    `dimensions-ai.data_analytics.grid` g2\n  ON\n    g2.id = children \n  WHERE\n    g.id=\"grid.495456.f\" -- United States Department of the Air Force\n\n)\n\nSELECT\n  hierarchy.*,\n  COUNT(DISTINCT p.id) as pubs\nFROM\n  hierarchy \nINNER JOIN\n  `dimensions-ai.data_analytics.publications` p\n  on hierarchy.children in UNNEST(p.research_orgs)\nGROUP by 1, 2, 3, 4\n</code></pre>"},{"location":"queries/24/#breaking-it-down","title":"Breaking it down","text":"<p>The key part of the query uses a <code>CROSS JOIN</code> on the <code>organization_recursive_child_ids</code> field to retrieve all the descendants of the chosen organization:</p> <pre><code>SELECT\n  g.id AS parent,\n  children,\nFROM\n  `dimensions-ai.data_analytics.grid` g\nCROSS JOIN\n  UNNEST(organization_recursive_child_ids) AS children\nWHERE\n  g.id=\"grid.495456.f\"\n</code></pre> <p>Furthermore, in order to get more organization metadata e.g. the name, an inner self-join is introduced: </p> <pre><code>SELECT\n  g.id AS parent,\n  g.name AS parent_name,\n  children,\n  g2.name AS children_name,\nFROM\n  `dimensions-ai.data_analytics.grid` g\nCROSS JOIN\n  UNNEST(organization_recursive_child_ids) AS children\nINNER JOIN\n  `dimensions-ai.data_analytics.grid` g2\nON\n  g2.id = children \nWHERE\n  g.id=\"grid.495456.f\" \n</code></pre> <p>The final step is to join also the <code>publications</code> table, so to get the total publications count for each organization. </p> <pre><code>WITH hierarchy AS (\n  -- the grid query above\n)\n\nSELECT\n  hierarchy.*,\n  COUNT(DISTINCT p.id) as pubs\nFROM\n  hierarchy \nINNER JOIN\n  `dimensions-ai.data_analytics.publications` p\n  on hierarchy.children in UNNEST(p.research_orgs)\nGROUP by 1, 2, 3, 4\n</code></pre>"},{"location":"queries/24/#results","title":"Results","text":"<pre><code>[\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.494596.3\",\n    \"children_name\": \"Edwards Air Force Base\",\n    \"pubs\": \"467\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.461685.8\",\n    \"children_name\": \"Joint Base San Antonio\",\n    \"pubs\": \"1031\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.265457.7\",\n    \"children_name\": \"United States Air Force Academy\",\n    \"pubs\": \"3137\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.431316.2\",\n    \"children_name\": \"Grand Forks Air Force Base\",\n    \"pubs\": \"4\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.499282.c\",\n    \"children_name\": \"Maxwell Air Force Base\",\n    \"pubs\": \"201\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.417730.6\",\n    \"children_name\": \"United States Air Force Research Laboratory\",\n    \"pubs\": \"24279\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.453002.0\",\n    \"children_name\": \"United States Air Force\",\n    \"pubs\": \"2545\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.461677.5\",\n    \"children_name\": \"Eglin Air Force Base\",\n    \"pubs\": \"575\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.507554.6\",\n    \"children_name\": \"United States Air Force Office of Scientific Research\",\n    \"pubs\": \"543\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.465246.7\",\n    \"children_name\": \"Air University\",\n    \"pubs\": \"47\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.472535.2\",\n    \"children_name\": \"Kirtland Air Force Base\",\n    \"pubs\": \"801\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.461680.d\",\n    \"children_name\": \"Hanscom Air Force Base\",\n    \"pubs\": \"400\"\n  },\n  {\n    \"parent\": \"grid.495456.f\",\n    \"parent_name\": \"United States Department of the Air Force\",\n    \"children\": \"grid.427848.5\",\n    \"children_name\": \"Air Force Institute of Technology\",\n    \"pubs\": \"4028\"\n  }\n  // more results here...\n]\n</code></pre>"},{"location":"queries/25/","title":"25. Grants for an organization","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/25/#description","title":"Description","text":"<p>This query counts all grants received by a selected GRID organization and sums up the total amount of funding received, based on the grants <code>start_year</code>.</p>"},{"location":"queries/25/#query","title":"Query","text":"<pre><code>SELECT\n  COUNT(*) AS total_grants,\n  SUM(funding_usd) AS total_grants_amount_usd\nFROM\n  `dimensions-ai.data_analytics.grants`\nWHERE\n  \"grid.10837.3d\" IN UNNEST(research_orgs)\n  AND (start_year &gt;= 2009\n    AND start_year &lt;= 2020)\n</code></pre>"},{"location":"queries/25/#results","title":"Results","text":"<pre><code>[\n  {\n    \"total_grants\": \"731\",\n    \"total_grants_amount_usd\": \"8.4692966E8\"\n  }\n]\n</code></pre>"},{"location":"queries/26/","title":"26. Field Citation Ratio (FCR) median average","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/26/#description","title":"Description","text":"<p>This query calculates the Field Citation Ratio (FCR) mean per year for publications from a chosen country (eg <code>'CA'</code>). FCR Mean is the average Field Citation Ratio (FCR), which indicates the relative citation performance of an article, when compared to similarly-aged articles in its Fields of Research (FoR) category. </p> <p>To calculate the geometric mean, we use the following approach, as documented in Thelwall &amp; Fairclough (2015):</p> <ul> <li>For a set of documents, all citation counts are incremented by 1.</li> <li>We calculate the natural log of the citations counts.</li> <li>We add these values together, and divide by the number of documents.</li> <li>We calculate the exponential of this value (reversing the log effect).</li> <li>We reduce the final value by 1.</li> </ul> <p>For more background, see this article: What is the FCR? How is it calculated?.</p> <p>Note</p> <p>The equivalent Dimensions API query is: <pre><code>search publications\nwhere research_org_countries = \"CA\"\nreturn year aggregate fcr_gavg\nsort by fcr_gavg\n</code></pre></p>"},{"location":"queries/26/#query","title":"Query","text":"<pre><code>SELECT\n  year,\n  COUNT(*) AS pub_count,\n  (EXP(AVG(LN(metrics.field_citation_ratio + 1))) - 1) AS fcr_gavg\nFROM\n  `dimensions-ai.data_analytics.publications` AS p\nWHERE\n  'CA' IN UNNEST(research_org_countries)\n  AND year &gt;= 2000\n  AND year &lt;= 2019\nGROUP BY\n  year\nORDER BY\n  year DESC\n</code></pre>"},{"location":"queries/26/#results","title":"Results","text":"<pre><code>[\n  {\n    \"year\": \"2019\",\n    \"pub_count\": \"126595\",\n    \"fcr_gavg\": \"1.895202772875376\"\n  },\n  {\n    \"year\": \"2018\",\n    \"pub_count\": \"123072\",\n    \"fcr_gavg\": \"2.063853669997279\"\n  },\n  {\n    \"year\": \"2017\",\n    \"pub_count\": \"116099\",\n    \"fcr_gavg\": \"2.173716507769935\"\n  },\n  {\n    \"year\": \"2016\",\n    \"pub_count\": \"110619\",\n    \"fcr_gavg\": \"2.2466482612818988\"\n  },\n  {\n    \"year\": \"2015\",\n    \"pub_count\": \"105632\",\n    \"fcr_gavg\": \"2.3125283272558983\"\n  },\n  {\n    \"year\": \"2014\",\n    \"pub_count\": \"103433\",\n    \"fcr_gavg\": \"2.3687156780522907\"\n  },\n  {\n    \"year\": \"2013\",\n    \"pub_count\": \"98650\",\n    \"fcr_gavg\": \"2.467124347845802\"\n  },\n  {\n    \"year\": \"2012\",\n    \"pub_count\": \"94561\",\n    \"fcr_gavg\": \"2.544932622059771\"\n  },\n  {\n    \"year\": \"2011\",\n    \"pub_count\": \"90036\",\n    \"fcr_gavg\": \"2.5889138063645207\"\n  },\n  {\n    \"year\": \"2010\",\n    \"pub_count\": \"86903\",\n    \"fcr_gavg\": \"2.6670704464809525\"\n  },\n  {\n    \"year\": \"2009\",\n    \"pub_count\": \"83592\",\n    \"fcr_gavg\": \"2.686271282515282\"\n  },\n  {\n    \"year\": \"2008\",\n    \"pub_count\": \"79437\",\n    \"fcr_gavg\": \"2.622504341868477\"\n  },\n  {\n    \"year\": \"2007\",\n    \"pub_count\": \"72631\",\n    \"fcr_gavg\": \"2.6134419550710426\"\n  },\n  {\n    \"year\": \"2006\",\n    \"pub_count\": \"70521\",\n    \"fcr_gavg\": \"2.5591136625055593\"\n  },\n  {\n    \"year\": \"2005\",\n    \"pub_count\": \"63057\",\n    \"fcr_gavg\": \"2.6001407619739907\"\n  },\n  {\n    \"year\": \"2004\",\n    \"pub_count\": \"55570\",\n    \"fcr_gavg\": \"2.6729901969921546\"\n  },\n  {\n    \"year\": \"2003\",\n    \"pub_count\": \"50762\",\n    \"fcr_gavg\": \"2.6458690167701433\"\n  },\n  {\n    \"year\": \"2002\",\n    \"pub_count\": \"46301\",\n    \"fcr_gavg\": \"2.5733709228189774\"\n  },\n  {\n    \"year\": \"2001\",\n    \"pub_count\": \"42605\",\n    \"fcr_gavg\": \"2.7140322564560413\"\n  },\n  {\n    \"year\": \"2000\",\n    \"pub_count\": \"41724\",\n    \"fcr_gavg\": \"2.6536040582685607\"\n  }\n]\n</code></pre>"},{"location":"queries/27/","title":"27. List of corresponding authors","text":"<p>Level: Easy</p> <p>This query is suitable for new users of Dimensions on Google BigQuery</p>"},{"location":"queries/27/#description","title":"Description","text":"<p>Extract corresponding authors for the <code>Nature Medicine</code> journal.</p>"},{"location":"queries/27/#query","title":"Query","text":"<pre><code>SELECT\n  id,\n  doi,\n  title.preferred,\n  a.first_name,\n  a.last_name,\n  a.corresponding,\n  journal.title AS journal_title\nFROM\n  `dimensions-ai.data_analytics.publications`,\n  UNNEST(authors) AS a\nWHERE\n  journal.id = \"jour.1113716\"\n  AND a.corresponding = TRUE\nLIMIT\n  10\n</code></pre>"},{"location":"queries/27/#results","title":"Results","text":"<pre><code>[\n  {\n    \"id\": \"pub.1000366392\",\n    \"doi\": \"10.1038/74704\",\n    \"preferred\": \"Inhibitory Fc receptors modulate in vivo cytoxicity against tumor targets\",\n    \"first_name\": \"Jeffrey V.\",\n    \"last_name\": \"Ravetch\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1002332484\",\n    \"doi\": \"10.1038/74689\",\n    \"preferred\": \"The tyrosine kinase p56lck is essential in coxsackievirus B3-mediated heart disease\",\n    \"first_name\": \"Josef M.\",\n    \"last_name\": \"Penninger\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1002696559\",\n    \"doi\": \"10.1038/75068\",\n    \"preferred\": \"Blockade of interleukin 6 trans signaling suppresses T-cell resistance against apoptosis in chronic intestinal inflammation: Evidence in Crohn disease and experimental colitis in vivo\",\n    \"first_name\": \"M.F.\",\n    \"last_name\": \"Neurath\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1001207508\",\n    \"doi\": \"10.1038/74918\",\n    \"preferred\": \"Human neural progenitor cells: better blue than green?\",\n    \"first_name\": \"Alberto\",\n    \"last_name\": \"Mart\u00ednez-Serrano\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1000887404\",\n    \"doi\": \"10.1038/73213\",\n    \"preferred\": \"Transdermal monitoring of glucose and other analytes using ultrasound\",\n    \"first_name\": \"Joseph\",\n    \"last_name\": \"Kost\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1000887404\",\n    \"doi\": \"10.1038/73213\",\n    \"preferred\": \"Transdermal monitoring of glucose and other analytes using ultrasound\",\n    \"first_name\": \"Robert\",\n    \"last_name\": \"Langer\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1000556369\",\n    \"doi\": \"10.1038/71527\",\n    \"preferred\": \"PR39, a peptide regulator of angiogenesis\",\n    \"first_name\": \"Michael\",\n    \"last_name\": \"Simons\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1001854662\",\n    \"doi\": \"10.1038/72262\",\n    \"preferred\": \"Protection from septic shock by neutralization of macrophage migration inhibitory factor\",\n    \"first_name\": \"Thierry\",\n    \"last_name\": \"Calandra\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1000791818\",\n    \"doi\": \"10.1038/76267\",\n    \"preferred\": \"Immunologic \u2018ignorance\u2019 of vascularized organ transplants in the absence of secondary lymphoid tissue\",\n    \"first_name\": \"Fadi G.\",\n    \"last_name\": \"Lakkis\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  },\n  {\n    \"id\": \"pub.1002229158\",\n    \"doi\": \"10.1038/72329\",\n    \"preferred\": \"Molecular mimicry mediated by MHC class Ib molecules after infection with Gram-negative pathogens\",\n    \"first_name\": \"Mark J.\",\n    \"last_name\": \"Soloski\",\n    \"corresponding\": true,\n    \"journal_title\": \"Nature Medicine\"\n  }\n]\n</code></pre>"},{"location":"queries/28/","title":"28. Researcher collaborations","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/28/#description","title":"Description","text":"<p>This query generates two-author pairs and counts how many publications are shared between them. The <code>WHERE</code> clause can be used to define a subset of the Dimensions publication dataset to examine\u2014without that, the query will generate co-authorship counts for every author of all 120+ million publications.</p> <p>Note</p> <p>Each two-author pair will have two entries in the results: One for <code>(Researcher 1, Researcher 2)</code> and another for <code>(Researcher 2, Researcher 1)</code>. If you want to prevent duplicates, adding something like <code>WHERE researcher1_id &gt; researcher2_id</code> will arbitrarily pick one of the combinations to display.</p>"},{"location":"queries/28/#query","title":"Query","text":"<pre><code>SELECT\n  ,CONCAT(r1.first_name, \" \", r1.last_name) AS researcher1\n  ,g1.name AS researcher1_org\n  ,CONCAT(r2.first_name, \" \", r2.last_name) AS researcher2\n  ,g2.name AS researcher2_org\n  ,COUNT(p.id) AS collaborations\nFROM `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(researcher_ids) researcher1_id\nCROSS JOIN UNNEST(researcher_ids) researcher2_id\nINNER JOIN dimensions-ai.data_analytics.researchers r1\n  ON researcher1_id=r1.id\nINNER JOIN dimensions-ai.data_analytics.researchers r2\n  ON researcher2_id=r2.id\nINNER JOIN dimensions-ai.data_analytics.grid g1\n  ON r1.current_research_org=g1.id\nINNER JOIN dimensions-ai.data_analytics.grid g2\n  ON r2.current_research_org=g2.id\nWHERE\n  AND researcher1_id &lt;&gt; researcher2_id\n  ---- Here is where you can add filters for which\n  --   publications to evaluate\n  p.year &gt;= 2019\n  AND '0604' IN UNNEST(category_for.second_level.codes) -- genetics\nGROUP BY 1,2,3,4\nORDER BY 5 DESC\n</code></pre>"},{"location":"queries/28/#results","title":"Results","text":"Row researcher1 researcher1_org researcher2 researcher2_org collaborations 0 Vasileios A Bampidis International Hellenic University Roberto Edoardo Villa University of Milan 67 1 Roberto Edoardo Villa University of Milan Vasileios A Bampidis International Hellenic University 67 2 Alexander Sergeevich Galushko Agrophysical Research Institute Jan Kuever Leibniz Institute for Materials Engineering 66 3 Jan Kuever Leibniz Institute for Materials Engineering Alexander Sergeevich Galushko Agrophysical Research Institute 66 4 Jerome I Rotter Harbor\u2013UCLA Medical Center Kent D Taylor Harbor\u2013UCLA Medical Center 65"},{"location":"queries/29/","title":"29. Institutions collaborations","text":"<p>Level: Medium</p> <p>This query requires basic knowledge of SQL and the Dimensions data model</p>"},{"location":"queries/29/#description","title":"Description","text":"<p>This query generates pairs of institutions and counts how many publications are shared between authors from those institutions. For example, if authors from the University of Cambridge published 15 papers with authors from the University of Maribor, the Cambridge/Maribor collaborations count would be 15. This does not account for number of authors from these institutions\u2014each publication is counted once per pair of institutions.</p> <p>The <code>WHERE</code> clause can be used to define a subset of the Dimensions publication dataset to examine\u2014without that, the query will generate co-authorship counts for every author of all 120+ million publications.</p> <p>Note</p> <p>Each two-institution pair will have two entries in the results: One for <code>(Institution 1, Institution 2)</code> and another for <code>(Institution 2, Institution 1)</code>. If you want to prevent duplicates, adding something like <code>WHERE org1_id &gt; org2_id</code> will arbitrarily pick one of the combinations to display.</p>"},{"location":"queries/29/#query","title":"Query","text":"<pre><code>SELECT g1.name AS org1 ,g2.name AS org2\n  ,COUNT(p.id) AS collaborations\nFROM `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(research_orgs) org1_id\nCROSS JOIN UNNEST(research_orgs) org2_id\nINNER JOIN dimensions-ai.data_analytics.grid g1\n  ON org1_id=g1.id\nINNER JOIN dimensions-ai.data_analytics.grid g2\n  ON org2_id=g2.id\nWHERE org1_id &lt;&gt; org2_id\n  ---- Here is where you can add filters for which\n  --   publications to evaluate\n  AND p.year &gt;= 2019\n  AND '2101' IN UNNEST(p.category_for.second_level.codes) -- Archaeology\nGROUP BY 1,2\nORDER BY 3 DESC\n</code></pre>"},{"location":"queries/29/#results","title":"Results","text":"Row org1 org2 collaborations 0 Rovira i Virgili University Catalan Institute of Human Paleoecology and Social Evolution 444 1 Catalan Institute of Human Paleoecology and Social Evolution Rovira i Virgili University 444 2 Institute of Vertebrate Paleontology and Paleoanthropology University of Chinese Academy of Sciences 134 3 University of Chinese Academy of Sciences Institute of Vertebrate Paleontology and Paleoanthropology 134 4 Rovira i Virgili University National Research Center on Human Evolution 113"},{"location":"queries/30/","title":"30. Publication volume growth rate","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/30/#description","title":"Description","text":"<p>This query calculates the growth rate of the number of publications in a particular field over time. Using annual numbers can make visualizations almost impossible to read, since year-to-year numbers can be noisy and include large swings in either direction. To avoid this, the query below uses a sliding five-year window: For example, the publication count for 2005 would include all of the publications between 2001 and 2005.</p> <p>The other way the results are smoothed is in our choice of comparison: Rather than comparing the 2005 total to the 2004 total, we compare 2005 to the previous 5-year window, which ended in 2000. Both of these strategies can be adjusted in the query:</p> <ul> <li>To adjust the \"width\" of the window, modify the line that says <code>4 PRECEDING</code> to whatever number of years works best for your data.</li> <li>To adjust which windows are compared to get the growth rate, adjust the line that says <code>LAG(running_total, 5)</code> by changing the <code>5</code> to a different value. For example, a <code>5</code> here means we compare the 2005 total to the 2000 total, but, changing this to <code>LAG(running_total, 1)</code> would compare 2005 to 2004.</li> </ul>"},{"location":"queries/30/#unusual-features","title":"Unusual features","text":"<p>There are a few components of this query that may not be intuitive, or use a complicated syntax:</p> <ul> <li>The <code>SUM(num) OVER ...</code> clause is part of a window function that includes the <code>num</code> field of the previous four rows (ordered by year). The BigQuery documentation has more information about using functions like <code>SUM() OVER</code>.</li> <li>The <code>LAG(running_total, 5)</code> clause does something similar, but instead of adding the previous four rows together, it only retrieves the value from the <code>running_total</code> field from the row five positions higher when ordered by year. This is called a \"navigation function\" and is also discussed in the BigQuery documentation.</li> <li>The <code>allyears</code> subquery is a clunky workaround for situations in which a particular year has no publications of interest. The <code>pub_counts</code> subquery returns a row for each year, and a count of publications in that year, but a year only appears if there's at least one publication. Since we're counting rows in the <code>SUM</code> and <code>LAG</code> functions described above, it would be a big problem if we're calculating a \"five-year window\" that actually stretches over eight years because three years in the window have no publications. <code>allyears</code> returns the exact same numbers as the <code>pubcounts</code> subquery, but it guarantees every year will have a row.<ul> <li>The start and end years for this step are defined in the <code>params</code> subquery. Ideally, we could just use the first and last years of the publications in the list, but BigQuery doesn't support \"correlated subqueries\" that reference each other, so this simpler method is used here instead.</li> </ul> </li> <li>The line that includes <code>COALESCE(NULLIF(prev_total, 0), 1)</code> is a bit of a hack. Ideally, the <code>rate</code> field would be calculated simply using <code>(running_total - prev_total) / prev_total</code>. However, there may be cases where <code>prev_total</code> is zero, which would throw an error for dividing by zero. Because there is no function for \"change this value if it is zero,\" it has to happen in two steps:<ul> <li><code>NULLIF(prev_total, 0)</code>: If <code>prev_total</code> is equal to <code>0</code>, this step returns <code>NULL</code> instead of <code>0</code>.</li> <li><code>COALESCE(NULLIF(prev_total, 0), 1)</code>: If the previous step returns <code>NULL</code>, this step will return <code>1</code> instead.</li> <li>The end result is that all non-zero values of <code>prev_total</code> are used without modification, but a value of <code>0</code> is changed to a value of <code>1</code>. This isn't perfect, since the real answer is an infinite growth rate.</li> </ul> </li> </ul>"},{"location":"queries/30/#query","title":"Query","text":"<pre><code>WITH params AS (\n    SELECT\n    -- Define the start and end points for your rate calculations.\n    -- Publications outside of this range will be ignored.\n        1990 AS minyear\n       ,2020 AS maxyear\n),\npub_counts AS (\n  SELECT\n    p.year AS pubyear, COUNT(p.id) AS num\n  FROM `dimensions-ai.data_analytics.publications` p\n  WHERE\n    p.year &gt;= (SELECT minyear FROM params)\n    AND p.year &lt;= (SELECT maxyear FROM params)\n    ---- HERE is where to define your publications\n    ---- of interest\n    AND '2101' IN UNNEST(p.category_for.second_level.codes) -- Archaeology\n  GROUP BY 1\n),\nallyears AS (\n    SELECT year, COALESCE(p.num, 0) AS num\n    FROM UNNEST(GENERATE_ARRAY(\n        (SELECT minyear FROM params),\n        (SELECT maxyear FROM params)\n    )) year\n    LEFT JOIN pub_counts p\n        ON p.pubyear=year\n),\npub_window AS (\n    SELECT year, num\n      ,SUM(num) OVER(ORDER BY year ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) AS running_total\n    FROM allyears\n),\npub_collate AS (\n    SELECT year, num, running_total\n        ,LAG(running_total, 5) OVER(ORDER BY year ASC) AS prev_total\n    FROM pub_window\n)\n\nSELECT year\n    ,num AS pub_count\n    ,(running_total - prev_total) / COALESCE(NULLIF(prev_total, 0), 1) AS rate\nFROM pub_collate\nWHERE year &gt;= (SELECT minyear FROM params)+10\nORDER BY 1 ASC\n</code></pre>"},{"location":"queries/30/#results","title":"Results","text":"Row year pub_count rate 0 2000 4404 0.1616 1 2001 4152 0.1404 2 2002 4477 0.1413 3 2003 4328 0.1047 4 2004 4190 0.0397"},{"location":"queries/31/","title":"31. Country-level publication activity over time","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/31/#description","title":"Description","text":"<p>This query returns the number of publications in a selected field over time, broken down by the countries with which the authors are affiliated. Both the annual and cumulative totals are available for each country for each year. The example below examines publications in archaeology starting in 1970, which means the row for \"Australia\" in 1981 will include two numbers:</p> <ol> <li><code>pubs</code>, which is the number of publications published in that year that include at least one author affiliated with an institution in Australia. (If a publication has authors from multiple countries, that publication is counted once for each country.)</li> <li><code>running_total</code>, which indicates the total publications attributed to Australia from 1970 through the current year, in this case 1981.</li> </ol> <p>This example returns yearly numbers only for the top eight countries by publication count\u2014the <code>top_countries</code> subquery holds the logic for this step. The rest of the country totals are combined as \"Others\" using the <code>everybody_else</code> subquery.</p>"},{"location":"queries/31/#query","title":"Query","text":"<pre><code>WITH results AS (\n  SELECT DISTINCT\n    id, year, country AS country\n  FROM `dimensions-ai.data_analytics.publications` p\n  CROSS JOIN UNNEST(p.research_org_country_names) country\n  WHERE\n    ---- HERE is where to define your publications\n    ---- of interest\n    p.year &gt;= 1970\n    AND '2101' IN UNNEST(p.category_for.second_level.codes) -- Archaeology\n),\ncounts AS (\n  SELECT year, country, COUNT(DISTINCT id) AS pubs\n  FROM results\n  GROUP BY 1,2\n),\ntop_countries AS (\n  SELECT country, SUM(pubs) AS total_pubs\n  FROM counts\n  GROUP BY 1\n  ORDER BY 2 DESC\n  LIMIT 8\n),\neverybody_else AS (\n  SELECT 'Others' AS country, year, SUM(pubs) AS pubs\n  FROM counts\n  WHERE counts.country NOT IN (SELECT country FROM top_countries)\n  GROUP BY 2\n),\neverybody_else_all_years AS (\n  -- We add a CROSS JOIN with the years from the results to make\n  -- sure there's an entry for \"Others\" even in years where that\n  -- number is zero.\n  SELECT base.country, year.year AS year, COALESCE(everybody_else.pubs,0) AS pubs\n  FROM (SELECT 'Others' AS country) base\n  CROSS JOIN (SELECT DISTINCT year FROM results) year\n  LEFT JOIN everybody_else\n    ON year.year=everybody_else.year\n),\ncounts_consolidated AS (\n  -- This query makes sure we have an entry for each country\n  -- in each year\n  SELECT top_countries.country, year.year AS year, COALESCE(counts.pubs,0) AS pubs\n  FROM top_countries\n  CROSS JOIN (SELECT DISTINCT year FROM results) year\n  LEFT JOIN counts\n    ON top_countries.country=counts.country\n    AND year.year=counts.year\n  --Then we add everybody else:\n  UNION ALL\n  SELECT * FROM everybody_else_all_years\n)\nSELECT year, country, pubs,\n  SUM (pubs) OVER (PARTITION BY country ORDER BY year) AS running_total\nFROM counts_consolidated\nORDER BY year, country\n</code></pre>"},{"location":"queries/31/#results","title":"Results","text":"Row year country pubs running_total 0 Australia 1970 15 15 1 Canada 1970 14 14 2 France 1970 6 6 more entries here... 101 Australia 1981 65 360 102 Canada 1981 26 247"},{"location":"queries/32/","title":"32. Creating a concepts network","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/32/#description","title":"Description","text":"<p>This query generates two-concept pairs and counts how many publications are shared between these concepts (note: concepts in Dimensions are publication-level keywords normalised and weighted based on a relevancy score).</p> <p>The query includes a subquery (<code>user-provided-subquery</code>) that can be customised so to focus on arbitrary subsets of the Dimensions database. Parameters values for max number of nodes and min weight of edges to be included in the result (@max_nodes, @min_edge_weight) can be fine-tuned as needed. </p> <p>The gist of the query lies in the double CROSS JOIN UNNEST. This mechanism allows to traverse a potentially very large number of relationships in seconds and to expose all relevant combinations of co-authoring organisations within the same data structure.</p> <p>For more details, see also: </p> <ul> <li>Generating large-scale network analyses of scientific landscapes in seconds using Dimensions on Google BigQuery, International Conference on Science, Technology and Innovation Indicators (STI 2022) Granada September 2022</li> <li>Python tool to generate network visualizations: website and source code in Github </li> </ul>"},{"location":"queries/32/#query","title":"Query","text":"<p>As an example, we generate a concepts cooccurence network for all publications from the last 30 days that have an Altmetric Attention Score greater than 10:</p> <pre><code>WITH subset AS (\n\n          -- USER-PROVIDED-SUBQUERY\n          -- add any query in this section, as long as it returns publication IDs\n          -- EG here we get all papers from the last 30 days with Altmetric attention &gt; 10\n\n          SELECT\n              id\n          FROM\n              `dimensions-ai.data_analytics.publications`\n          WHERE\n              EXTRACT(\n                  DATE\n                  FROM\n                      date_inserted\n              ) &gt;= DATE_ADD(CURRENT_DATE(), INTERVAL -30 DAY)\n              AND altmetrics.score &gt; 10\n\n        ),\n        papercount AS (\n            SELECT concept.concept, COUNT(p.id) AS papers,\n            FROM `dimensions-ai.data_analytics.publications` p\n            INNER JOIN subset ON p.id=subset.id\n            CROSS JOIN UNNEST(p.concepts) concept\n            WHERE\n                year &gt;= 1965\n                AND concept.relevance &gt;= 0.5 -- @min_link_relevance\n            GROUP BY 1\n        ),\n        filtered AS (\n            SELECT *\n            FROM papercount\n            WHERE papers &gt;= 5 --@min_concept_frequency\n            ORDER BY papers DESC\n            LIMIT 500 -- @max_nodes\n        ),\n        results AS (\n        SELECT concept1.concept AS concept_a, concept2.concept AS concept_b,\n            COUNT(p.id) AS overlap,\n        FROM `dimensions-ai.data_analytics.publications` p\n        INNER JOIN subset ON p.id=subset.id\n        CROSS JOIN UNNEST(p.concepts) concept1\n        CROSS JOIN UNNEST(p.concepts) concept2\n        INNER JOIN filtered pcount1 ON concept1.concept=pcount1.concept\n        INNER JOIN filtered pcount2 ON concept2.concept=pcount2.concept\n        WHERE year &gt;= 1965\n            AND concept1.relevance &gt;= 0.5 --@min_link_relevance\n            AND concept2.relevance &gt;= 0.5 --@min_link_relevance\n            AND concept1.concept &lt;&gt; concept2.concept\n        GROUP BY 1,2\n        )\n        SELECT *\n        FROM results\n        WHERE overlap &gt;= 3 --@min_edge_weight\n</code></pre>"},{"location":"queries/32/#results","title":"Results","text":"<p>A sample of the results</p> concept_a concept_b overlap concept_a concept_b overlap odds ratio subgroup analysis 5 randomized clinical trials patients 18 population-based study Main Outcomes 3 systematic review Web of Science 24 intervention care 18 depression participants 20 exercise muscle 5 heart failure observational study 3 non-small cell lung cancer therapy 3 type 2 diabetes cohort study 7 quality of life primary outcome 8 Cox proportional hazards models women 5 plants cells 3 nurses outcomes 6 COVID-19 intervention 3 COVID-19 pandemic well-being 3 clinical trials randomized clinical trials 46 subgroup analysis meta-analysis 17 odds ratio logistic regression models 3 more entries here..."},{"location":"queries/32/#vosviewer","title":"VOSViewer","text":"<p>A VOSViewer visualization using the results of this query can be explored online (note: the visualization uses data from the COVID-19 dataset)</p>"},{"location":"queries/33/","title":"33. Calculating disruption indices (CD index)","text":"<p>Level: Advanced</p> <p>This query requires a good understanding of SQL and the Dimensions data model</p>"},{"location":"queries/33/#description","title":"Description","text":"<p>Evaluating the disruptive nature of academic ideas is a new area of research evaluation that moves beyond standard citation-based metrics by taking into account the broader citation context of publications or patents. The \"CD index\" and a number of related indicators have been proposed in order to characterise mathematically the disruptiveness of scientific publications or patents. </p> <p>This research area has generated a lot of attention in recent years, yet there is no general consensus on the significance and reliability of disruption indices. More experimentation and evaluation would be desirable, however is hampered by the fact that these indicators are expensive and time-consuming to calculate, especially if done at scale on large citation networks. </p> <p>The query below takes advantage of Dimensions on BigQuery scalable architecture and reduces the computational time taken to produce such indices by an order of magnitude. This approach makes it possible to calculate e.g. CD5 index for all journal articles with references in Dimensions in less than 5 hours.</p> <p>For more details, see also: </p> <ul> <li>Dimensions: Calculating Disruption Indices at Scale, Arxiv, 2023.</li> <li>The paper accompanying source code in Github </li> </ul> <p>Note</p> <p>Please make sure you are familiar with BigQuery billing methods and quota before running large queries like the ones below.</p>"},{"location":"queries/33/#query","title":"Query","text":"<pre><code>CREATE OR REPLACE TABLE `{your-gbq-project}.{you-gbq-dataset}.publications_cd_index_all`\nCLUSTER BY id\nAS\n(\n  WITH publications AS\n  (\n    SELECT id, year, citations, reference_ids\n    FROM `dimensions-ai.data_analytics.publications`\n    WHERE year IS NOT NULL \n  )\n\n  SELECT focal_id AS id,\n  (SUM(score)/COUNT(DISTINCT citation_id))+2 AS cd_5,\n  COUNTIF(score = -1)*((SUM(score)/COUNT(DISTINCT citation_id))+2) AS mcd_5\n  FROM\n  (\n    (\n      SELECT DISTINCT publications.id AS focal_id,\n      citation.id AS citation_id,\n      -1 AS score\n      FROM publications\n      LEFT JOIN UNNEST(publications.citations) AS citation\n      WHERE citation.year - publications.year BETWEEN 1 AND 5\n    )\n    UNION ALL\n    (\n      SELECT DISTINCT publications.id AS focal_id,\n      reference_citation.id as citation_id,\n      -2 as score\n      FROM publications\n      LEFT JOIN UNNEST(publications.reference_ids) AS reference_id\n      INNER JOIN publications AS references\n      ON references.id = reference_id\n      LEFT JOIN UNNEST(references.citations) AS reference_citation\n      WHERE reference_citation.year - publications.year BETWEEN 1 AND 5\n    )\n  )\n  GROUP BY 1\n)\n</code></pre> <p>NOTE: it is easy to create variations of the query above using a more restrictive set of input documents. </p> <p>For example, if we want to generate the CD index only for journal publications: </p> <pre><code>CREATE OR REPLACE TABLE `{your-gbq-project}.{you-gbq-dataset}.publications_cd_index_journals` \nCLUSTER BY id \nAS\n(\n  WITH publications AS\n  (\n    SELECT id, year, citations, reference_ids\n    FROM `dimensions-ai.data_analytics.publications`\n    WHERE year IS NOT NULL AND type =\"article\" AND ARRAY_LENGTH(reference_ids)&gt;=10 AND journal.id IS NOT NULL\n  )\n\n  SELECT focal_id AS id,\n  (SUM(score)/COUNT(DISTINCT citation_id))+2 AS cd_5,\n  COUNTIF(score = -1)*((SUM(score)/COUNT(DISTINCT citation_id))+2) AS mcd_5\n  FROM\n  (\n    (\n      SELECT DISTINCT publications.id AS focal_id, \n      citation.id AS citation_id,\n      -1 AS score\n      FROM publications\n      LEFT JOIN UNNEST(publications.citations) AS citation\n      WHERE citation.year - publications.year BETWEEN 1 AND 5\n    )\n    UNION ALL\n    (\n      SELECT DISTINCT publications.id AS focal_id, \n      reference_citation.id as citation_id,\n      -2 as score\n      FROM publications\n      LEFT JOIN UNNEST(publications.reference_ids) AS reference_id\n      INNER JOIN publications AS references \n      ON references.id = reference_id\n      LEFT JOIN UNNEST(references.citations) AS reference_citation\n      WHERE reference_citation.year - publications.year BETWEEN 1 AND 5\n    )\n  )\n  GROUP BY 1\n)\n</code></pre>"},{"location":"queries/33/#results","title":"Results","text":"<p>The query results have the following format:</p> <pre><code>[{\n  \"id\": \"pub.1065228952\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"18.0\"\n}, {\n  \"id\": \"pub.1065228398\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"27.0\"\n}, {\n  \"id\": \"pub.1065228573\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"29.0\"\n}, {\n  \"id\": \"pub.1065125068\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"26.0\"\n}, {\n  \"id\": \"pub.1113010424\",\n  \"cd_5\": \"0.125\",\n  \"mcd_5\": \"1.125\"\n}, {\n  \"id\": \"pub.1083797867\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"194.0\"\n}, {\n  \"id\": \"pub.1078318645\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"108.0\"\n}, {\n  \"id\": \"pub.1078221791\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"28.0\"\n}, {\n  \"id\": \"pub.1078052782\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"19.0\"\n}, {\n  \"id\": \"pub.1071156881\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"21.0\"\n}, {\n  \"id\": \"pub.1071156878\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"56.0\"\n}, {\n  \"id\": \"pub.1071156873\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"56.0\"\n}, {\n  \"id\": \"pub.1071156882\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"27.0\"\n}, {\n  \"id\": \"pub.1070917761\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"17.0\"\n}, {\n  \"id\": \"pub.1071163762\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"144.0\"\n}, {\n  \"id\": \"pub.1071053457\",\n  \"cd_5\": \"1.0\",\n  \"mcd_5\": \"44.0\"\n},\n.......\n</code></pre>"},{"location":"tutorials/","title":"About Tutorials","text":"<p>The Tutorials section contains guides that focus on specific topics or use cases, e.g. how to deal with a specific data type, or how to use Google BigQuery in combination with other technologies.</p> <p>Note</p> <p>See also the Dimensions on BigQuery official documentation for more information about how to access the data.</p>"},{"location":"tutorials/01-connection/","title":"Verifying your connection","text":"<p>In this tutorial we will show how to connect to Dimensions on Google BigQuery using Python, so that we can then run a few sample queries.</p> <p>Note</p> <p>This tutorial is intended for people who want to query BigQuery via a notebook, but the SQL queries in this lab can also be run directly from the BigQuery console.</p>"},{"location":"tutorials/01-connection/#prerequisites","title":"Prerequisites","text":"<p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> <li>You have some basic familiarity with Python and Jupyter notebooks.</li> </ul> <p>(This tutorial is based on a Jupyter notebook that is available directly via GitHub.)</p>"},{"location":"tutorials/01-connection/#connection-methods","title":"Connection methods","text":"<p>There are a few options available:</p> <ol> <li>Use Google Colaboratory and your personal credentials. This option is the simplest of all, as it doesn't require you to install anything on your computer. It is normally ok for small to mid-sized projects that can live in the cloud.</li> <li>Use a local Jupyter environment and your personal credentials. This option requires you to install the Google Cloud SDK in order to authenticate. It is the best option if you want to work locally and/or have other Python libraries or services that you need to access.</li> <li>Use a local Jupyter environment and a service account. This option is really a variance on the option 2, for those users that must use a service account.</li> </ol> <p>NOTE All of these options require you to first set up a GCP project (if you haven't done it already) and provide your project ID. E.g.:</p> <pre><code>MY_PROJECT_ID = \"my-cool-gbq-project\"\n</code></pre>"},{"location":"tutorials/01-connection/#option-1-using-google-colaboratory-and-your-personal-credentials","title":"Option 1: using Google Colaboratory and your personal credentials","text":"<p>Google Colaboratory is a free cloud-based Jupyter environment from Google. This option provides an easy service allowing you to get started with notebooks.</p> <p>Using your Google Account you can create notebooks, execute BigQuery queries and share these with other Google Accounts quickly and easily.</p> <pre><code># authentication happens via your browser\nfrom google.colab import auth\nauth.authenticate_user()\nprint('Authenticated')\n\nMY_PROJECT_ID = \"my-cool-gbq-project\"\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=MY_PROJECT_ID)\n</code></pre>"},{"location":"tutorials/01-connection/#option-2-using-a-local-jupyter-and-your-personal-credentials","title":"Option 2: using a local Jupyter and your personal credentials","text":"<p>A Google Account represents a developer, an administrator, or any other person who interacts with Google Cloud. This is normally the Google account one has used to get access to the Dimensions on BigQuery product.</p> <p>In order to configure programmatic access for local development, the easiest way is to authenticate using the Google Cloud SDK.</p> <pre><code>$ gcloud auth application-default login\n</code></pre> <p>Note: the command above should be run from a Terminal or console. This will generate a JSON file that is used as the default application credentials for the account that was selected in the above login process. When using the default Client for each Google provided package (such as BigQuery) they should automatically authenticate using these default credentials.</p> <pre><code># install python client library\n!pip install google-cloud-bigquery -U --quiet\n</code></pre> <pre><code>from google.cloud import bigquery\n\nMY_PROJECT_ID = \"my-cool-gbq-project\"\nclient = bigquery.Client(project=MY_PROJECT_ID)\n</code></pre>"},{"location":"tutorials/01-connection/#option-3-using-a-local-jupyter-and-a-service-account","title":"Option 3: using a local Jupyter and a service account","text":"<p>A service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person.</p> <p>Each service account is associated with two sets of public/private RSA key pairs that are used to authenticate to Google: Google-managed keys, and user-managed keys.</p> <p>When using a service account you'd just have to point your client object to the a key file.</p> <pre><code>from google.cloud import bigquery\ncredentials_file = 'my-awesome-gbq-project-47616836.json'\n\nMY_PROJECT_ID = \"my-cool-gbq-project\"\n\n# Explicitly use service account credentials by specifying the private key file\nclient = bigquery.Client.from_service_account_json(credentials_file)\n</code></pre>"},{"location":"tutorials/01-connection/#running-queries","title":"Running queries","text":"<p>Once the connection is set up, all you have to do is to type in a SQL query and run it using the <code>client</code> object.</p> <pre><code># Query: Top publications from Oxford univ. by Altmetric Score in 2020\n\nquery_1 = \"\"\"\nSELECT\n    id,\n    title.preferred as title,\n    ARRAY_LENGTH(authors) as authors_count,\n    CAST(altmetrics.score as INT64) as altmetric_score\nFROM\n    `dimensions-ai.data_analytics.publications`\nWHERE\n    year = 2020 AND 'grid.4991.5' in UNNEST(research_orgs)\nORDER BY\n    altmetric_score DESC\nLIMIT 5\"\"\"\n\n# 1 - main syntax\n\nquery_job = client.query(query_1)\n\nresults = query_job.result()  # Waits for job to complete.\n\nfor row in results:\n  print(\"&gt; {} : {}\\n\\tAuthors: {}\\n\\tAltmetric Score: {}\".format(row.id, row.title, row.authors_count, row.altmetric_score))\n</code></pre> <pre><code>&gt; pub.1129493369 : Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial\n    Authors: 366\n    Altmetric Score: 15451\n&gt; pub.1130340155 : Two metres or one: what is the evidence for physical distancing in covid-19?\n    Authors: 6\n    Altmetric Score: 15125\n&gt; pub.1127239818 : Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial\n    Authors: 46\n    Altmetric Score: 12675\n&gt; pub.1131721397 : Scientific consensus on the COVID-19 pandemic: we need to act now\n    Authors: 31\n    Altmetric Score: 10192\n&gt; pub.1126016857 : Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing\n    Authors: 9\n    Altmetric Score: 8320\n</code></pre> <p>An slighly alternative syntax is also possible</p> <pre><code># 2 - omit calling result()\n\nquery_job = client.query(query_1)\nfor row in query_job:\n    print(row)\n</code></pre> <pre><code>Row(('pub.1129493369', 'Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial', 366, 15451), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3})\nRow(('pub.1130340155', 'Two metres or one: what is the evidence for physical distancing in covid-19?', 6, 15125), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3})\nRow(('pub.1127239818', 'Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial', 46, 12675), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3})\nRow(('pub.1131721397', 'Scientific consensus on the COVID-19 pandemic: we need to act now', 31, 10192), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3})\nRow(('pub.1126016857', 'Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing', 9, 8320), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3})\n</code></pre> <p>Another quite handy feature is to transform data direclty into Pandas dataframes</p> <p><pre><code># 3 - return a dataframe\n\nquery_job = client.query(query_1).to_dataframe()\nquery_job\n</code></pre> | Row | id    | title | authors_count | altmetric_score | | --- | ----- | ----- | ------------- | --------------- | | 1 | pub.1129493369 | Safety and immunogenicity of the ChAdOx1 nCoV-... | 366 | 15451 | | 2 | pub.1130340155 | Two metres or one: what is the evidence for ph... | 6 | 15125 | | 3 | pub.1127239818 | Remdesivir in adults with severe COVID-19: a r... | 46 | 12675 | | 4 | pub.1131721397 | Scientific consensus on the COVID-19 pandemic:... | 31 | 10192 | | 5 | pub.1126016857 | Quantifying SARS-CoV-2 transmission suggests e... | 9 | 8320 |</p>"},{"location":"tutorials/01-connection/#advanced-bigquery-magic-command-and-dynamic-parameters","title":"Advanced: BigQuery magic command and dynamic parameters","text":"<p>The Google BigQuery library comes with a magic command that is essentially a nice shortcut method for running queries.</p> <p>This extensions needs to be loaded sepately e.g.:</p> <pre><code>%load_ext google.cloud.bigquery\n</code></pre> <p>We can then set up a couple of query parameters for the query itself, as well as the usual project ID value.</p> <pre><code>project_id = MY_PROJECT_ID\nbq_params = {}\nbq_params[\"journal_id\"] = \"jour.1115214\"\n</code></pre> <p>Finally we can query by starting a cell with the command <code>%%bigquery ...</code>:</p> <pre><code>%%bigquery --params $bq_params --project $project_id\n\n# Publications per year for Nature Biotechnology\n\nSELECT\n    count(*) as pubs, year, journal.title\nFROM\n    `dimensions-ai.data_analytics.publications`\nWHERE\n    year &gt;= 2010\n    AND journal.id = @journal_id\nGROUP BY\n    year, journal.title\nORDER BY\n    year DESC\n</code></pre> <pre><code>Query complete after 0.02s: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 699.28query/s]\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02&lt;00:00,  4.31rows/s]\n</code></pre> Row pubs year title 1 438 2020 Nature Biotechnology 2 386 2019 Nature Biotechnology 3 374 2018 Nature Biotechnology 4 380 2017 Nature Biotechnology 5 436 2016 Nature Biotechnology 6 467 2015 Nature Biotechnology 7 475 2014 Nature Biotechnology 8 462 2013 Nature Biotechnology 9 507 2012 Nature Biotechnology 10 459 2011 Nature Biotechnology 11 486 2010 Nature Biotechnology"},{"location":"tutorials/01-connection/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Query fails wit <code>to_dataframe() ArrowNotImplementedError</code><ul> <li>Try reinstalling pyarrow ie <code>pip install pyarrow -U</code></li> </ul> </li> <li>Query fails with <code>AttributeError: 'NoneType' object has no attribute 'transport'</code><ul> <li>Try <code>pip install google-cloud-bigquery-storage -U</code> and restarting the notebook</li> </ul> </li> </ul>"},{"location":"tutorials/02-dsl/","title":"From the DSL API to Google BigQuery","text":"<p>This tutorial demonstrates how to perform a full-text search in Dimensions using the Analytics API and then export the data to Google BigQuery for further analysis.</p> <p>This technique allows to take advantage of the strengths of each of these data products:</p> <ul> <li>The Analytics API allows to run full-text searches over the hundreds of millions documents stored in the Dimensions database. This makes it an ideal tool for identifying a corpus of documents using collections of keywords and/or other filters (note: this is the same functionality available when you search on app.dimensions.ai)</li> <li>The Dimensions on Google BigQuery database allows to run SQL queries of any complexity using a cloud-based environment containing all of the metadata available in Dimensions, thus removing the need to download/analyse the data offline first. This makes is the perfect solution for advanced analytics tasks such as benchmarking, metrics calculations or impact analyses.</li> </ul> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> <li>You have a valid Dimensions API account.</li> <li>You have some basic familiarity with Python and Jupyter notebooks.</li> </ul> <p>(This tutorial is based on a Jupyter notebook that is available directly via GitHub.)</p>"},{"location":"tutorials/02-dsl/#example-profiling-researchers-linked-to-a-topic","title":"Example: profiling researchers linked to a topic","text":"<p>The concrete usecase we'll be looking at involves running a full-text search for \"moon landing\" publications using the DSL API, then creating a corpus in BigQuery based on this search (eg see this search).</p> <p>Once we have the publication corpus available in BigQuery, we will extract all associated researchers (=authors). At the same time, we are going to use SQL in order to enrich the results using other metrics (eg more <code>researchers</code> metadata including citations &amp; altmetric).</p>"},{"location":"tutorials/02-dsl/#getting-started","title":"Getting started","text":"<p>The following code will load the Python BigQuery library and authenticate you as a valid user.</p> <pre><code>!pip install google-cloud-bigquery -U --quiet\n%load_ext google.cloud.bigquery\n\nimport sys\nprint(\"==\\nAuthenticating...\")\nif 'google.colab' in sys.modules:\n    from google.colab import auth\n    auth.authenticate_user()\n    print('..done (method: Colab)')\nelse:\n    from google.cloud import bigquery\n    print('..done (method: local credentials)')\n\n#\n# PLEASE UPDATE USING YOUR CLOUD PROJECT ID (= the 'billing' account)\n#\n\nMY_PROJECT_ID = \"ds-data-solutions-gbq\"\n\nprint(\"==\\nTesting connection..\")\nclient = bigquery.Client(project=MY_PROJECT_ID)\ntest = client.query(\"\"\"\n    SELECT COUNT(*) as pubs\n    from `dimensions-ai.data_analytics.publications`\n    \"\"\")\nrows = [x for x in test.result()]\nprint(\"...success!\")\nprint(\"Total publications in Dimensions: \", rows[0]['pubs'])\n</code></pre> <pre><code>The google.cloud.bigquery extension is already loaded. To reload it, use:\n  %reload_ext google.cloud.bigquery\n==\nAuthenticating...\n..done (method: local credentials)\n==\nTesting connection..\n...success!\nTotal publications in Dimensions:  115963650\n</code></pre>"},{"location":"tutorials/02-dsl/#1-connecting-to-the-dsl-api","title":"1. Connecting to the DSL API","text":"<p>For more background on the Analytics API and how to work with it, see this tutorial</p> <pre><code>!pip install dimcli --quiet\n\nimport dimcli\nfrom dimcli.utils import *\nimport json\nimport sys\nimport pandas as pd\n#\n\nENDPOINT = \"https://app.dimensions.ai\"\nUSERNAME, PASSWORD  = \"\", \"\"\ndimcli.login(USERNAME, PASSWORD, ENDPOINT)\ndsl = dimcli.Dsl()\n</code></pre> <p>Let's try running a sample query.</p> <p>TIP Review the full text search syntax of the Dimensions Search Language.</p> <pre><code>%%dsldf\n\nsearch publications for\n  \" \\\"moon landing\\\" AND Moon AND \\\"lunar surface\\\" \"\nreturn publications limit 10\n</code></pre> <pre><code>Returned Publications: 10 (total = 11305)\n\u001b[2mTime: 0.72s\u001b[0m\n</code></pre> Row type volume pages id year author_affiliations title journal.id journal.title issue 0 article 122 100692 pub.1134954138 2021 `[[{'raw_affiliation': ['Department of Aerospac...` Review of space habitat designs for long term ... jour.1139377 Progress in Aerospace Sciences NaN 1 article 181 167-189 pub.1130384298 2021 `[[{'raw_affiliation': ['U.S. Naval War College...` Joseph G. Gavin, Jr. and MIT\u2019s contribution to... jour.1134138 Acta Astronautica NaN 2 article 180 650-678 pub.1134475636 2021 `[[{'raw_affiliation': ['Skolkovo Institute of ...` Regolith-based additive manufacturing for sust... jour.1134138 Acta Astronautica NaN 3 article NaN 1-13 pub.1135101079 2021 `[[{'raw_affiliation': ['Centre for Teaching an...` Looking at Gail Jones\u2019s \u201cThe Man in the Moon\u201d ... jour.1137860 Journal of Australian Studies NaN 4 article 21 959 pub.1135057882 2021 `[[{'raw_affiliation': ['School of Artificial I...` Three-Dimensional Model of the Moon with Seman... jour.1033312 Sensors 3"},{"location":"tutorials/02-dsl/#2-exporting-dsl-results-to-google-bigquery","title":"2. Exporting DSL results to Google BigQuery","text":"<p>First off, we want to run the full-text search so to extract all relevant publications IDs.</p> <p>Second, we will export the publications IDs to Google BigQuery. NOTE: Pandas provides a handy command to move data to BigQuery: DataFrame.to_gbq.</p> <pre><code>%%dslloopdf\n\nsearch publications for\n  \" \\\"moon landing\\\" AND Moon AND \\\"lunar surface\\\" \"\nreturn publications[id]\n</code></pre> <pre><code>Starting iteration with limit=1000 skip=0 ...\n0-1000 / 11201 (0.43s)\n1000-2000 / 11201 (1.01s)\n2000-3000 / 11201 (0.69s)\n3000-4000 / 11201 (1.03s)\n4000-5000 / 11201 (1.30s)\n5000-6000 / 11201 (1.58s)\n6000-7000 / 11201 (0.33s)\n7000-8000 / 11201 (0.25s)\n8000-9000 / 11201 (0.27s)\n9000-10000 / 11201 (0.64s)\n10000-11000 / 11201 (1.05s)\n11000-11201 / 11201 (1.80s)\n===\nRecords extracted: 11201\n</code></pre> Row id 0 pub.1128771471 1 pub.1130814402 2 pub.1131658726 3 pub.1124123379 4 pub.1131232278 ... ... 11196 pub.1061739351 11197 pub.1025947790 11198 pub.1091822752 11199 pub.1025757974 11200 pub.1023928923 <p>11201 rows \u00d7 1 columns</p> <pre><code>df = dsl_last_results\n</code></pre> <p>The command below will add a new table <code>moonlanding</code> to the <code>demo_dsl</code> dataset in GQB.</p> <p>That destination table is entirely up to you of course, so you need to make sure you have write access to the database.</p> <pre><code>DATASET = \"demo_dsl\"\ntable_id = DATASET + \".moonlanding\"\ndf.to_gbq(table_id, project_id = PROJECTID, if_exists=\"replace\")\n</code></pre> <pre><code>1it [00:05,  5.05s/it]\n</code></pre> <p>That's it - you should now be able to go to the online BigQuery console and see the new <code>demo_dsl.moonlanding</code> dataset.</p>"},{"location":"tutorials/02-dsl/#3-querying-your-new-dataset-using-a-join-on-dimensions","title":"3. Querying your new dataset using a JOIN on Dimensions","text":"<p>We can now use the publications IDs we imported in order to create a JOIN query on the main Dimensions dataset. This is a bit like creating a 'view' of Dimensions corresponding to the full-text search we have done above.</p> <p>GOAL: Roughly, the results should be the same as the 'publication year' facet in the webapp</p> <pre><code>%%bigquery --project $PROJECTID\n\nWITH mypubs AS (\n  SELECT dim_pubs.*\n  FROM\n    `dimensions-ai.data_analytics.publications` dim_pubs\n  JOIN\n    `ds-data-solutions-gbq.demo_dsl.moonlanding` dslexport\n  ON\n    dim_pubs.id = dslexport.id\n)\n\nSELECT\n  COUNT(id) as tot,\n  year\nFROM mypubs\nGROUP BY year\nORDER BY tot DESC\n</code></pre> Row tot year 0 10052 2003 1 133 2020 2 97 2019 3 70 2015 4 66 2017 5 65 2018 6 63 2009 7 59 2013 rows truncated for display"},{"location":"tutorials/02-dsl/#4-using-google-bigquery-to-generate-researcher-statistics","title":"4. Using Google BigQuery to generate researcher statistics","text":"<p>The goal is to generate a table just like the one in the 'researchers' analytical view in the webapp.</p> <p>For each researcher we want to display some extra information:</p> <ul> <li>the total number of publications</li> <li>the citations count</li> <li>the total Altmetric Attention Score</li> </ul> <pre><code>%%bigquery --project $PROJECTID\n\nWITH mypubs AS (\n  SELECT dim_pubs.*\n  FROM\n    `dimensions-ai.data_analytics.publications` dim_pubs\n  JOIN\n    `ds-data-solutions-gbq.demo_dsl.moonlanding` dslexport\n  ON\n    dim_pubs.id = dslexport.id\n),\n\nresearchers_metrics AS (\n  SELECT researcher_id,\n    COUNT(id) as publications_count,\n    SUM(citations_count) as citations_count,\n    SUM(altmetrics.score) as altmetric_sum\n  FROM\n    mypubs,\n    UNNEST( researcher_ids ) as researcher_id\n  GROUP BY researcher_id\n)\n\nSELECT * FROM researchers_metrics\nORDER BY publications_count DESC\n</code></pre> Row researcher_id publications_count citations_count altmetric_sum 0 ur.01056354465.10 11 21.0 49.0 1 ur.014402173273.44 6 42.0 10.0 2 ur.012373502003.54 4 63.0 NaN 3 ur.010534421371.14 4 63.0 NaN 4 ur.015145367415.34 4 44.0 1.0 ... ... ... ... ... 1080 ur.0767272510.86 1 5.0 NaN 1081 ur.07637166751.28 1 3.0 NaN 1082 ur.012762707227.21 1 3.0 NaN 1083 ur.010101533313.52 1 NaN 1.0 1084 ur.016406136233.64 1 NaN 16.0 <p>Final step: let's add researchers names and current organization details by joining up data from the GRID table.</p> <pre><code>%%bigquery --project $PROJECTID\n\nWITH mypubs AS (\n\n  SELECT dim_pubs.*\n  FROM\n    `dimensions-ai.data_analytics.publications` dim_pubs\n  JOIN\n    `ds-data-solutions-gbq.demo_dsl.moonlanding` dslexport\n  ON\n    dim_pubs.id = dslexport.id\n\n),\n\nresearchers_metrics AS (\n\n  SELECT researcher_id,\n    COUNT(id) as publications_count,\n    SUM(citations_count) as citations_count,\n    SUM(altmetrics.score) as altmetric_sum\n  FROM\n    mypubs,\n    UNNEST( researcher_ids ) as researcher_id\n  GROUP BY researcher_id\n\n),\n\nresearchers_full AS (\n\n  SELECT researchers_metrics.*,\n    r.first_name, r.last_name, r.total_grants,\n    grid.id as grid_id,\n    grid.name as grid_name,\n    grid.address.city as grid_city,\n    grid.address.country as grid_country\n  FROM\n    researchers_metrics\n  JOIN\n    `dimensions-ai.data_analytics.researchers` r\n    ON researchers_metrics.researcher_id = r.id\n  JOIN\n    `dimensions-ai.data_analytics.grid` grid\n    ON grid.id = r.current_research_org\n\n)\n\n\n\nSELECT * FROM researchers_full\nORDER BY publications_count DESC\n</code></pre> Row researcher_id publications_count citations_count altmetric_sum first_name last_name total_grants grid_id grid_name grid_city grid_country 0 ur.01056354465.10 11 21.0 49.0 Roger D Launius 4 grid.1214.6 Smithsonian Institution Washington D.C. United States 1 ur.014402173273.44 6 42.0 10.0 Joseph N Pelton 0 grid.33224.34 International Space University Illkirch-Graffenstaden France 2 ur.010243405673.63 4 14.0 NaN Sachiko Wakabayashi 1 grid.62167.34 Japan Aerospace Exploration Agency Tokyo Japan 3 ur.012503545245.69 4 12.0 1.0 Stephan Theil 2 grid.7551.6 German Aerospace Center Cologne Germany 4 ur.0720745255.73 4 8.0 28.0 Chun-Lai Li 3 grid.450302.0 National Astronomical Observatories Beijing China ... ... ... ... ... ... ... ... ... ... ... ... 886 ur.011430662526.22 1 10.0 NaN Gang Lei 0 grid.458502.e Technical Institute of Physics and Chemistry Beijing China 887 ur.015477605337.38 1 1.0 NaN Olivier Dubois-Matra 0 grid.424669.b European Space Research and Technology Centre Noordwijk-Binnen Netherlands 888 ur.013214745135.53 1 NaN NaN Catherine L Newell 0 grid.26790.3a University of Miami Coral Gables United States 889 ur.014464032227.37 1 41.0 141.0 Andrew M Carton 0 grid.25879.31 University of Pennsylvania Philadelphia United States 890 ur.010610572173.89 1 10.0 NaN Zhan Liu 0 grid.411510.0 China University of Mining and Technology Xuzhou China"},{"location":"tutorials/03-dates/","title":"Working with dates","text":"<p>Each publication has various dates available.</p> <ul> <li><code>date</code>, <code>year</code>, <code>date_normal</code>, <code>date_online</code>, <code>date_print</code> refer to the publication object. See the documentation to find out more about their meaning.</li> <li><code>date_imported_gbq</code> refers to when this record was last added to Google BigQuery - this date can be handy if you want to synchronize an external data source to BigQuery.</li> <li><code>date_inserted</code>: this refers to when this records was originally added to Dimensions. This date does not change, even if the record is later adjusted.</li> </ul> <p>The following examples show how to work with publications dates.</p> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> </ul> <p>The online BigQuery console can be used to test the queries below.</p>"},{"location":"tutorials/03-dates/#comparing-date-fields","title":"Comparing date fields","text":""},{"location":"tutorials/03-dates/#description","title":"Description","text":"<p>We'll get started by pulling a selection of the date fields to see their formats: <pre><code>SELECT doi,\n       date,\n       date_normal,\n       year,\n       date_online,\n       date_print,\n       date_imported_gbq,\n       date_inserted\nFROM   `dimensions-ai.data_analytics.publications`\nWHERE year = 2010\n      AND journal.id = \"jour.1115214\"\nORDER BY citations_count DESC\nLIMIT 10\n</code></pre></p> <p>Results</p> Row doi date date_normal year date_online date_print date_imported_gbq date_inserted 0 10.1038/nbt.1621 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 1 10.1038/nbt.1630 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 2 10.1038/nbt.1614 2010-03 2010-03-01 2010 null 2010-03 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 3 10.1038/nbt.1685 2010-10-13 2010-10-13 2010 2010-10-13 2010-10 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 4 10.1038/nbt1210-1248 2010-12-07 2010-12-07 2010 2010-12-07 2010-12 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 5 10.1038/nbt.1755 2010-12-22 2010-12-22 2010 2010-12-22 2011-02 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 6 10.1038/nbt1010-1045 2010-10-13 2010-10-13 2010 2010-10-13 2010-10 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 7 10.1038/nbt.1633 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 8 10.1038/nbt.1667 2010-07-19 2010-07-19 2010 2010-07-19 2010-08 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 9 10.1038/nbt.1641 2010-05-23 2010-05-23 2010 2010-05-23 2010-06 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 <p>The first thing to stick out is that some of the dates are actually timestamps: <code>date_imported_gbq</code> and <code>date_inserted</code> have times attached to the dates. The other important caveat is that some dates aren't actually whole dates: Some values in the <code>date</code> and <code>date_print</code> fields have only a year and month. One of the reasons these different types are important is because can add an extra step when you compare fields to each other. For example, if we wanted to count how many publications were added to Dimensions before their \"publication\" date, it would be intuitive to write a query like this:</p> <pre><code>SELECT COUNT(id)\nFROM `dimensions-ai.data_analytics.publications`\nWHERE\n  year = 2020\n  AND date &gt; date_inserted\n</code></pre> <p>However, we get an error from BigQuery: <code>No matching signature for operator &gt; for argument types: STRING, TIMESTAMP. Supported signature: ANY &gt; ANY at [12:11]</code>. BigQuery won't do the comparison because both sides of the comparison aren't of the same type: The <code>date</code> field is of type <code>STRING</code>, since it doesn't always have a day (or month) attached. The <code>date_normal</code> field solves this for us: It uses the same information as the <code>date</code> field, but it fills in the gaps to make a full <code>DATE</code> entry\u2014so <code>\"2010-03\"</code> in the <code>date</code> field becomes <code>2010-03-01</code> in <code>date_normal</code>. But swapping that in doesn't fix our problems either:</p> <pre><code>SELECT COUNT(id)\nFROM `dimensions-ai.data_analytics.publications`\nWHERE\n  year = 2020\n  AND date_normal &gt; date_inserted\n</code></pre> <p>We run into a new variant of the issue now: <code>No matching signature for operator &gt; for argument types: DATE, TIMESTAMP. Supported signature: ANY &gt; ANY at [5:7]</code>. Now <code>date_normal</code> gives us a <code>DATE</code>, but we can't compare that to a <code>TIMESTAMP</code>. Generally, you can mitigate most issues with comparing date fields by converting one of them to match the other, and BigQuery supports multiple functions for manipulating dates and datetimes. This one should do the trick:</p> <pre><code>SELECT COUNT(id)\nFROM `dimensions-ai.data_analytics.publications`\nWHERE\n  year = 2020\n  AND date_normal &gt; DATE(date_inserted)\n</code></pre> <p>Results:</p> Row f0_ 1 859011"},{"location":"tutorials/03-dates/#number-of-publications-added-to-dimensions-by-month","title":"Number of publications added to Dimensions by month","text":""},{"location":"tutorials/03-dates/#description_1","title":"Description","text":"<p>Next, we'll use the <code>date_inserted</code> field to count the number of publications added to the Dimensions database per month. <code>date_inserted</code> is of type <code>DATETIME</code>, so we choose from the datetime manipulation functions to round down all dates to the first of the month:</p> <pre><code>SELECT\n  DATETIME_TRUNC(date_inserted, MONTH) as added_date,\n  COUNT(id) as countDim\nFROM\n  `dimensions-ai.data_analytics.publications`\nGROUP BY added_date\nORDER BY added_date DESC\nLIMIT 5\n</code></pre> <p>Results</p> Row added_date countDim 1 2021-04-01 00:00:00 UTC 534043 2 2021-03-01 00:00:00 UTC 746963 3 2021-02-01 00:00:00 UTC 661575 4 2021-01-01 00:00:00 UTC 687764 5 2020-12-01 00:00:00 UTC 828307 <p>We can see the dates have all been collapsed into the first of the month for each paper, but those timestamps that are attached are unhelpful. We can get rid of them by converting <code>date_inserted</code> to a <code>DATE</code> first, and switch to using the <code>DATE_TRUNC</code> function instead:</p> <pre><code>SELECT\n  DATE_TRUNC(DATE(date_inserted), MONTH) as added_date,\n  COUNT(id) as countDim\nFROM\n  `dimensions-ai.data_analytics.publications`\nGROUP BY added_date\nORDER BY added_date DESC\nLIMIT 5\n</code></pre> <p>Results</p> Row added_date countDim 1 2021-04-01 534043 2 2021-03-01 746963 3 2021-02-01 661575 4 2021-01-01 687764 5 2020-12-01 828307 <p>That looks much better. If we want to manipulate different parts of the dates separately, we can also use <code>EXTRACT</code> to split things up:</p> <pre><code>SELECT\n  EXTRACT(MONTH FROM date_inserted) as added_month,\n  EXTRACT(YEAR FROM date_inserted) as added_year,\n  COUNT(id) as countDim\nFROM\n  `dimensions-ai.data_analytics.publications`\nGROUP BY added_month, added_year\nORDER BY added_year DESC, added_month DESC\nLIMIT 5\n</code></pre> <p>Results</p> Row added_month added_year countDim 1 4 2021 534043 2 3 2021 746963 3 2 2021 661575 4 1 2021 687764 5 12 2020 828307"},{"location":"tutorials/04-nested/","title":"Working with nested and repeated fields","text":"<p>A prominent feature of Google BigQuery is their addition of nested and repeated fields to what may otherwise be a familiar SQL paradigm. Both present opportunities to reorganize data within single tables in novel ways, but they can take some time to get used to. Below, we explain the basics of nested and repeated fields, work through several examples, and provide links to external resources that we've found helpful.</p> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> </ul> <p>The online Google BigQuery console can be used to test the queries below.</p>"},{"location":"tutorials/04-nested/#what-are-they","title":"What are they?","text":""},{"location":"tutorials/04-nested/#repeated-fields","title":"Repeated fields","text":"<p>Repeated fields approximate a \"one-to-many\" relationship and provide an opportunity to define a field that can hold multiple values per row. We can demonstrate this by running a query against the <code>publications</code> table for values in the <code>clinical_trial_ids</code> field:</p> <pre><code>SELECT\n  id, LEFT(title.preferred, 25) AS title, clinical_trial_ids\nFROM `dimensions-ai.data_analytics.publications`\nWHERE ARRAY_LENGTH(clinical_trial_ids) &gt; 0\nLIMIT 10\n</code></pre> <p>The (heavily truncated) results look something like this:</p> Row id title clinical_trial_ids 1 pub.1003360568 A Randomized, Controlled... NCT00014989 2 pub.1003935609 8568 Prophylactic swallow... NCT00332865 3 pub.1004269292 Clinical Trial Alert... NCT00953940 NCT00970073 NCT00994253 NCT00987103 NCT00974636 4 pub.1004095142 6502 A double-blinded, pl... NCT00219557 NCT00428597 5 pub.1004119511 Intrathecal morphine in a... NCT00119184 <p>You can see that rows 3 and 4 have multiple values in the <code>clinical_trial_ids</code> field, despite all values getting listed within a single row number.</p>"},{"location":"tutorials/04-nested/#nested-fields","title":"Nested fields","text":"<p>Nested fields, on their own, are much simpler: They are fields that are linked together as a single entity, like a struct or an object. The <code>title</code> field in the <code>publications</code> table is a good example of this: Rather than a single string indicating the title of the publication, it is a nested field that has two strings within it: <code>\"original\"</code> and <code>\"preferred\"</code>, mostly to accommodate titles expressed in multiple languages. Querying nested fields looks almost identical to querying more conventional ones. For example, with the title field:</p> <pre><code>SELECT id, title\nFROM `dimensions-ai.data_analytics.publications`\nLIMIT 4\n</code></pre> <p>Results:</p> Row id title.preferred title.original 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... \"1+N\"\u5ef6\u4f38\u62a4\u7406\u6a21\u5f0f\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7528\u836f\u4f9d\u4ece\u6027\u53ca\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u7684\u5f71\u54cd 2 pub.1123897378 Clinical observation of the prevention of pressu... \u9f99\u8840\u7aed\u9884\u9632\u6076\u6027\u80bf\u7624\u5f3a\u8feb\u4f53\u4f4d\u60a3\u8005\u538b\u75ae\u7684\u4e34\u5e8a\u89c2\u5bdf 3 pub.1039091814 IV. A new improved silk-reel. null 4 pub.1123920716 Effect of resina draconis for external applica... \u9f99\u8840\u7aed\u80f6\u56ca\u7c89\u5916\u6577\u6cbb\u7597\u538b\u75ae\u7597\u6548\u7684Meta\u5206\u6790"},{"location":"tutorials/04-nested/#repeated-nested-fields","title":"Repeated nested fields","text":"<p>This is where things get a little more complicated: One of the main ways nested fields make themselves useful is when they're repeated: So while a repeated field might be an array of strings (clinical trial IDs, for example), they can also be an array of objects. The <code>authors</code> field of the publications table is a good example of this:</p> <pre><code>SELECT id, title.preferred, authors\nFROM `dimensions-ai.data_analytics.publications`\nLIMIT 3\n</code></pre> <p>Results:</p> Row id title.preferred authors.first_name authors.last_name authors.researcher_id 1 pub.1001350088 The T-120/130-12.8 and PT... G.D. Barinberg ur.012510636551.40 A.E. Valamin ur.012211770163.32 Yu. A. Sakhnin ur.010306240353.29 A. Yu. Kultyshev ur.014402311563.25 2 pub.1000116807 Application of Electrorhe... Ken'ichi Koyanagi ur.013307555250.87 Yasuhiro Kakinuma ur.013275435603.18 <p>You can see here that the author information appears the same way as the clinical trial IDs above, except each repeated entry within a row has multiple fields about each author. (There are many more fields that will appear if you query authors; they've been removed here for clarity.) The useful part about using nested fields for the authors, rather than a bunch of repeated fields alone (one for <code>first_name</code>, another repeated field for <code>last_name</code>, etc) is because those nested fields will stay together: For the publication in row 1, the <code>\"A.E.\"</code> first name will always appear alongside the <code>\"Valamin\"</code> last name, rather than shuffling them around like what may happen if you queried them separately.</p>"},{"location":"tutorials/04-nested/#querying-nested-fields","title":"Querying nested fields","text":"<p>We'll start writing queries with nested fields alone first, since it's the simplest to do. We actually did it several times in the above examples: The <code>title</code> field in the publications table is a nested field with two fields in it: <code>original</code> and <code>preferred</code>. If you don't specify which values you want, you'll get them all, like this:</p> <pre><code>SELECT id, title\nFROM `dimensions-ai.data_analytics.publications`\nLIMIT 4\n</code></pre> <p>Results:</p> Row id title.preferred title.original 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... \"1+N\"\u5ef6\u4f38\u62a4\u7406\u6a21\u5f0f\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7528\u836f\u4f9d\u4ece\u6027\u53ca\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u7684\u5f71\u54cd 2 pub.1123897378 Clinical observation of the prevention of pressu... \u9f99\u8840\u7aed\u9884\u9632\u6076\u6027\u80bf\u7624\u5f3a\u8feb\u4f53\u4f4d\u60a3\u8005\u538b\u75ae\u7684\u4e34\u5e8a\u89c2\u5bdf 3 pub.1039091814 IV. A new improved silk-reel. null 4 pub.1123920716 Effect of resina draconis for external applica... \u9f99\u8840\u7aed\u80f6\u56ca\u7c89\u5916\u6577\u6cbb\u7597\u538b\u75ae\u7597\u6548\u7684Meta\u5206\u6790 <p>If you wanted only the <code>preferred</code> field of the <code>title</code>, you can specify that using periods. Nested fields can have more nested fields within them, so there may be multiple entries. Luckily, we only need one period for the title:</p> <pre><code>SELECT id, title.preferred\nFROM `dimensions-ai.data_analytics.publications`\nLIMIT 4\n</code></pre> <p>Results:</p> Row id title.preferred 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... 2 pub.1123897378 Clinical observation of the prevention of pressu... 3 pub.1039091814 IV. A new improved silk-reel. 4 pub.1123920716 Effect of resina draconis for external applica..."},{"location":"tutorials/04-nested/#querying-repeated-fields","title":"Querying repeated fields","text":"<p>Repeated fields are where we need to start using more exotic patterns to extract information. The <code>UNNEST</code> function is the primary tool for the job here\u2014it converts an array of values into rows in a table, which, if necessary, can then be joined to the original table you're querying.</p>"},{"location":"tutorials/04-nested/#example-1-checking-contents-of-array","title":"Example 1: Checking contents of array","text":"<p>We'll start with a simple one: the <code>funder_orgs</code> field in the <code>publications</code> table, which lists GRID IDs indicating which organizations funded the research in the publication. IF we wanted to find publications funded by the Brazilian Agricultural Research Corporation, for example, we can use its GRID ID (grid.460200.0) in a <code>WHERE</code> clause:</p> <pre><code>SELECT type, COUNT(id) AS funded_pubs\nFROM `dimensions-ai.data_analytics.publications`\nWHERE 'grid.460200.0' IN UNNEST(funder_orgs)\nGROUP BY type\n</code></pre> <p>Results</p> Row type funded_pubs 1 preceeding 23 2 article 6042 3 preprint 21 4 chapter 33"},{"location":"tutorials/04-nested/#example-2-joining-tables-using-a-repeated-field","title":"Example 2: Joining tables using a repeated field","text":"<p>Queries can also return the contents of repeated fields. Using a <code>CROSS JOIN</code>, the information can be distributed into separate rows, rather than arrays inside single rows. For this example, we'll look at organizations that have funded recent articles published in eLife, a life sciences journal. We'll start by selecting the information we can get from the publications table:</p> <pre><code>SELECT p.id, forg\nFROM `dimensions-ai.data_analytics.publications` AS p\nCROSS JOIN UNNEST(funder_orgs) AS forg -- This is the important line\nWHERE type='article'\n  AND journal.id='jour.1046517' -- eLife\n</code></pre> Row id forg 1 pub.1000035854 grid.14105.31 2 pub.1000321327 grid.48336.3a 3 pub.1000131550 grid.422384.b 4 pub.1000131550 grid.419475.a 5 pub.1000131550 grid.453152.4 6 pub.1000131550 grid.280362.d 7 pub.1000131550 grid.416870.c <p>There are a few things to point out here: First, notice that we're querying a nested field within the <code>journal</code> field on the final line\u2014we want only publications in which the <code>journal</code> field lists an <code>id</code> that matches the one assigned to eLife. We're also using a <code>CROSS JOIN</code> with the <code>funder_orgs</code> field. A cross join returns the Cartesian product of the two tables being joined\u2014to wit, every value on one side of the join (in this case, the <code>publications</code> table) will appear with every matching value from the right side of the join (the \"table\" created by the call to <code>UNNEST(funder_orgs)</code>). This is demonstrated in lines 3 through 7 of the results above\u2014publication id \"pub.1000131550\" has five different strings in its <code>funder_orgs</code> field, so when we unnest that field, the results contain multiple rows for \"pub.1000131550,\" one for each value unnested from <code>funder_orgs</code>.</p> <p>We're not done yet, however\u2014we have a table that associates every eLife paper with each of its funders, but that's not really useful on its own. If we use group by the <code>forg</code> field (the values unnested from <code>funder_orgs</code>), we can get a count for each organization, like this:</p> <pre><code>SELECT forg, COUNT(p.id) AS funded_pubs\nFROM `dimensions-ai.data_analytics.publications` AS p\nCROSS JOIN UNNEST(funder_orgs) AS forg -- This is the important line\nWHERE type='article'\n  AND journal.id='jour.1046517' -- eLife\nGROUP BY forg\nORDER BY funded_pubs DESC\nLIMIT 5\n</code></pre> <p>Results:</p> Row forg funded_pubs 1 grid.280785.0 2329 2 grid.416870.c 1090 3 grid.413575.1 1078 4 grid.452896.4 976 5 grid.48336.3a 893 <p>This is getting better! Now we have the GRID ID of each funder, paired with the number of eLife publications it's funded. However, GRID IDs aren't very readable. We can get organization names by pulling them in from the <code>grid</code> table of organizations data:</p> <pre><code>SELECT forg, grid.name, COUNT(p.id) AS funded_pubs\nFROM `dimensions-ai.data_analytics.publications` AS p\nCROSS JOIN UNNEST(funder_orgs) AS forg\nINNER JOIN `dimensions-ai.data_analytics.grid` AS grid -- THIS IS NEW!\n  ON forg=grid.id\nWHERE type='article'\n  AND journal.id='jour.1046517'\nGROUP BY forg, grid.name\nORDER BY funded_pubs DESC\nLIMIT 5\n</code></pre> <p>Results:</p> Row forg name funded_pubs 1 grid.280785.0 National Institute of General Medical Sciences 2329 2 grid.416870.c National Institute of Neurological Disorders and Stroke 1090 3 grid.413575.1 Howard Hughes Medical Institute 1078 4 grid.452896.4 European Research Council 976 5 grid.48336.3a National Cancer Institute 893 <p>Now we have the table we wanted: We unnest the values in the <code>funder_orgs</code> field, use those to join the <code>grid</code> table, and return the name of each funder and how many publications it's funded in eLife.</p>"},{"location":"tutorials/04-nested/#example-3-querying-repeated-nested-fields","title":"Example 3: Querying repeated nested fields","text":"<p>Let's pull everything together using the task outlined in example 3 from the query library: combining all author names of a paper into a single string. As described above, the <code>authors</code> field is complicated because it's a repeated field in which each value is a nested field: Each repeat of <code>authors</code> has its own <code>first_name</code> field, its own <code>last_name</code>, and so on. It's easier to see the structure if we start with a simpler query:</p> <pre><code>  SELECT id, authors\n  FROM `dimensions-ai.data_analytics.publications`\n  WHERE id = 'pub.1132070778'\n</code></pre> <p>Results (truncated for simplicity):</p> Row id authors.first_name authors.last_name authors.researcher_id 1 pub.1132070778 O Gr\u00e5n\u00e4s ur.01027021415.21 A Mocellin ur.01316620417.40 E S Cardoso null F Burmeister ur.0631574677.49 C Caleman ur.0745346134.45 O Bj\u00f6rneholm ur.0603171002.99 A Naves de Brito ur.01206174227.82 <p>So if we want to bring all the authors together into a single string, there are a lot of discrete steps to take care of:</p> <ol> <li>Pull out the <code>first_name</code> and <code>last_name</code> fields for each author in the <code>authors</code> repeated field.</li> <li>Make a new string for each author that combines their first and last name together.</li> <li>Pull together each of these full author names into a new array we'll call <code>author_names</code>. So we go from an array of <code>author</code> objects, each with its own collection of nested fields, into an array of strings, each one representing a single author.</li> <li>Combine all elements in the <code>author_names</code> into one long string.</li> </ol> <p>First, we try to make things more readable by using a <code>WITH</code> clause to emulate a temporary table: Within this query, there's a \"table\" called <code>author_array</code> filled with the results of this subquery:</p> <pre><code>SELECT\n  id,\n  ARRAY(\n    SELECT CONCAT(first_name, \" \", last_name)\n    FROM UNNEST(authors)\n  ) AS authors\nFROM `dimensions-ai.data_analytics.publications`\nWHERE id = 'pub.1132070778'\n</code></pre> <p>This is important, because it's where most of the work happens. We start in the middle and work our way outward.</p> <p>This piece takes an array (<code>authors</code>) and uses the <code>UNNEST</code> function to create a new table in which each row is a separate author. Then, we take each row of this temporary \"authors\" table and combine each first name with each last name:</p> <pre><code>SELECT CONCAT(first_name, \" \", last_name)\nFROM UNNEST(authors)\n</code></pre> <p>So we now have a table with a single field\u2014a full name\u2014and each row is one author. We then convert this back into an array:</p> <pre><code>ARRAY(\n    SELECT CONCAT(first_name, \" \", last_name)\n    FROM UNNEST(authors)\n) AS authors\n</code></pre> <p>The outermost piece of this subquery is just to tie each array of author names to the publication that they authored: <pre><code>WITH author_array AS (\n  SELECT\n    id,\n    ARRAY(\n      SELECT CONCAT(first_name, \" \", last_name)\n      FROM UNNEST(authors)\n    ) AS author_names\n  FROM `dimensions-ai.data_analytics.publications`\n  WHERE id = 'pub.1132070778'\n)\n</code></pre></p> <p>So now we have a table called <code>author_array</code> in which each publication ID is associated with an array of author names. It looks like this):</p> Row id author_names 1 pub.1132070778 O Gr\u00e5n\u00e4s A Mocellin E S Cardoso F Burmeister C Caleman O Bj\u00f6rneholm A Naves de Brito <p>Now that we have the author names pulled out of the author objects, we're almost done. The last step is to iterate through each publication ID, take each entry in the <code>author_names</code> array, and push them all together using the <code>ARRAY_TO_STRING</code> function:</p> <pre><code>SELECT\n  id,\n  ARRAY_TO_STRING(author_names, '; ') AS authors_list\nFROM author_array\n</code></pre> <p>Results</p> Row id authors_list 1 pub.1132070778 O Gr\u00e5n\u00e4s; A Mocellin; E S Cardoso; F Burmeister; C Caleman; O Bj\u00f6rneholm; A Naves de Brito"},{"location":"tutorials/04-nested/#be-careful","title":"Be careful","text":"<p>There are a few pitfalls to be aware of when working with nested and repeated fields; we outline some of the most common below.</p>"},{"location":"tutorials/04-nested/#example-4-repeated-fields-with-null-values","title":"Example 4: Repeated fields with null values","text":"<p>The trouble with using <code>CROSS JOIN</code> clauses in queries is that they omit all records for which the repeated field has no values: If a paper has zero authors listed, for example, including <code>CROSS JOIN UNNEST(authors)</code> in your query means there won't be any rows for that paper. We can examine this further using the <code>research_org_country_names</code> repeated field:</p> <pre><code>SELECT COUNT(DISTINCT p.id) AS tot_articles\nFROM\n  `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(research_org_country_names) AS unnested_countries\nWHERE year = 2000\n</code></pre> <p>Results</p> Row tot_articles 1 1063394 <p>We then run the same query without the <code>UNNEST</code> clause:</p> <pre><code>SELECT COUNT(DISTINCT p.id) AS tot_articles\nFROM\n  `dimensions-ai.data_analytics.publications` p\nWHERE year = 2000\n</code></pre> <p>Results</p> Row tot_articles 1 1760397 <p>So without the <code>UNNEST</code>, the total publication count is just over 1.7 million. With the unnest, however, it's less than 1.1 million. The gap is explained by publications that have an ID (that's what we're counting), but that do not have any values in the <code>research_org_country_names</code> field.</p> <p>So how can we be sure we aren't excluding records we actually want? In this case, a <code>LEFT JOIN</code> is the way to go:</p> <pre><code>SELECT\n  COUNT(DISTINCT p.id) AS tot_articles\nFROM\n  `dimensions-ai.data_analytics.publications` p\nLEFT JOIN UNNEST(research_org_country_names) AS unnested_countries\nWHERE year = 2000\n</code></pre> <p>Results</p> Row tot_articles 1 1760397 <p>Using <code>LEFT JOIN UNNEST(x)</code> instead of <code>CROSS JOIN UNNEST(x)</code> ensures that entries in which <code>x</code> is <code>NULL</code> will still be returned\u2014those will simply have null listed in the <code>unnested_countries</code> field.</p>"},{"location":"tutorials/04-nested/#example-5-counting-entries-too-many-times","title":"Example 5: Counting entries too many times","text":"<p>While it's helpful that <code>CROSS JOIN UNNEST()</code> gives us all relevant combinations of the selected fields, it can also present hazards if you don't account for which fields may have multiple entries. For this example, we want to examine how many papers were published in PLOS ONE that include an author with the surname \"Smith.\"</p> <p>This query will get us most of the way there:</p> <pre><code>SELECT\n  p.year, COUNT(p.id) AS totcount\nFROM `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(authors) author\nWHERE\n  journal.id='jour.1037553' -- PLOS ONE\n  AND year &gt;= 2018\n  AND year &lt;= 2020\n  AND author.last_name='Smith'\nGROUP BY year\nORDER BY year\n</code></pre> <p>Results:</p> Row year totcount 1 2018 196 2 2019 151 3 2020 155 <p>We start with all publications published in PLOS ONE between 2018 and 2020, then unnest the <code>authors</code> field so we can get to the <code>last_name</code> field. We then include only entries in which <code>last_name='Smith'</code>.</p> <p>However, these yearly totals aren't correct: We're counting the number of entries in the table, and we only have entries in which an author's last name is \"Smith.\" But some papers may have been written by more than one Smith. We can account for this by adding a <code>DISTINCT</code> clause, like this:</p> <pre><code>SELECT\n  p.year, COUNT(DISTINCT p.id) AS totcount -- CHANGE IS HERE!\nFROM `dimensions-ai.data_analytics.publications` p\nCROSS JOIN UNNEST(authors) author\nWHERE\n  journal.id='jour.1037553'\n  AND year &gt;= 2018\n  AND year &lt;= 2020\n  AND author.last_name='Smith'\nGROUP BY year\nORDER BY year\n</code></pre> <p>Results:</p> Row year totcount 1 2018 189 2 2019 144 3 2020 154 <p>Comparing these results to the previous ones, we can see that there are usually more than 140 papers published with \"Smith\" authors every year, and several papers per year authored by multiple Smiths. This was a straightforward example, but <code>DISTINCT</code> clauses can be a valuable check in more convoluted queries in which you may have multiple cross joins, or you have a cross join in a subquery that is later joined to another table.</p>"},{"location":"tutorials/05-topic_clusters/","title":"Basic Topic Clustering using TensorFlow and BigQuery ML","text":"<p>In this tutorial we will implement a basic topic clustering on publications, generating text embeddings using a pre-trained TensorFlow model and creating the groupings via K-means clustering provided by BigQuery ML. This tutorial utilises datasets which are only available to Dimensions on BigQuery customers.</p> <p>For this specific example we will be analysing the publications of New Zealand\u2019s top 8 universities from 2016 onwards. The example below is based around using a Python Notebook which utilises BigQuery iPython magic commands to execute BigQuery SQL statements.</p> <p>The basic steps taken are:</p> <ul> <li>Setup the Python environment and BigQuery access.</li> <li>Extract titles/abstracts for the publications of interest from Big Query.</li> <li>Use TensorFlow to generate word embeddings from the titles/abstracts.</li> <li>Export these embedding vectors back into Google BigQuery and cleanup the format of them.</li> <li>Create k-means models using BigQuery ML (multiple models for different cluster counts).</li> <li>Compare the different k-means models and select the most appropriate.</li> <li>Associate publications back to each cluster.</li> <li>Determine the topics/concepts associated with each cluster.</li> </ul> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> <li>You have some basic familiarity with Python and Jupyter notebooks.</li> </ul>"},{"location":"tutorials/05-topic_clusters/#1-setup-google-bigquery-access","title":"1. Setup Google BigQuery access","text":"<p>The initial setup is to authenticate your Google Account for accessing GCP resources. This account must be the account which has access to the Dimensions BigQuery datasets. It must also must be allowed to execute and pay for Google BigQuery queries. In the code example below <code>project_id</code> should be replaced with the GCP project identifier which you own or can access BigQuery resources on.</p> <pre><code>from google.colab import auth\nauth.authenticate_user()\nproject_id = \"my-gbq-project\"  # replace with GCP project to bill against\n%load_ext google.cloud.bigquery\n</code></pre> <p>The project referenced in the examples below is <code>my-gbq-project</code>, anytime that is contained within Python code or SQL statements it should be replaced with your own projects identifier. The SQL and Python code below also assumes that two datasets exist within the Google BigQuery project (<code>project_id</code>).</p> <ul> <li> <p><code>temp</code> which holds intermediate tables created during the setup phase.</p> </li> <li> <p><code>clustering</code> which holds the tables we will create models from and the final models we create.</p> </li> </ul>"},{"location":"tutorials/05-topic_clusters/#2-gather-titlesabstracts-for-publications","title":"2. Gather titles/abstracts for publications","text":"<p>The first step we will undertake is building a DataFrame which contains the titles and abstracts from publications. We will use the following BigQuery SQL statement to extract the data:</p> <pre><code>WITH target_orgs AS (\n  SELECT org FROM UNNEST([\n    \"grid.252547.3\", -- AUT\n    \"grid.9654.e\", -- UoA\n    \"grid.21006.35\", -- Canterbury\n    \"grid.29980.3a\", -- Otago\n    \"grid.267827.e\", -- Vic\n    \"grid.148374.d\", -- Massey\n    \"grid.49481.30\", -- Waikato\n    \"grid.16488.33\" -- Lincoln\n  ]) org\n)\nSELECT\n  id, ANY_VALUE(title) as title, ANY_VALUE(abstract) as abstract, ARRAY_AGG(org) as orgs\nFROM (\n  SELECT\n    id, title.preferred as title, abstract.preferred as abstract, org as org\n  FROM `dimensions-ai.data_analytics.publications` p\n  CROSS JOIN UNNEST(research_orgs) org\n  RIGHT JOIN target_orgs t ON t.org = org\n  WHERE p.year &gt; 2015 AND abstract.preferred is not null\n)\nGROUP BY id\n</code></pre> <p>To execute the SQL statement above and save the results into a DataFrame we can use the BigQuery iPython magic command:</p> <pre><code>%%bigquery --params $bq_params --project $project_id pubs\n# SQL statement from above\n</code></pre> <p>Taking a look at the generated DataFrame we can see what is present and verify that the data matches our expectations. At this point is may also be useful to save the data off to a \u201cpickle\u201d file allowing quick reloading of the dataset and skipping this step in the future when re-running the steps below.</p> <p></p>"},{"location":"tutorials/05-topic_clusters/#3-generate-word-embeddings-for-each-publication","title":"3. Generate word embeddings for each publication","text":"<p>The next stage is to generate word embeddings for each publication\u2019s title and abstract. Word embedding models attempt to map words or phrases from a vocabulary into to vectors of real numbers. These word embeddings can then used in language modelling and feature learning natural language processing (NLP) techniques. Ideally, the end result is that publications with abstracts representing the same topics and concepts will be near one another within the reduced vector space generated through the word embedding process.</p> <p>For this example we will use TensorFlow and the Universal Sentence Encoder model to generate our word embeddings. The input into the model is variable length English text and it will generate a 512 dimensional vector. The embeddings should result in a vector representation which provides a reasonable approximation to semantic similarity between publications based on the content of the abstracts.</p> <pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\n\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\ndef get_embed_title(titles):\n    return embed(titles)\n\ndef get_embed_abstract(abstracts, delimiter=\"\\n\"):\n    def _map_fn(a):\n        t = tf.cast(a, tf.string)\n        t = tf.strings.split(t, sep=delimiter)\n        e = embed(t)\n        e = tf.reduce_mean(e, axis=0)\n        return tf.squeeze(e)\n    return tf.map_fn(_map_fn, abstracts, dtype=tf.float32)\n\ndef process(titles, abstracts):\n    title_embed = get_embed_title(titles)\n    abstract_embed = get_embed_abstract(abstracts)\n\n    output_features = {\n        'title_embed': title_embed,\n        'abstract_embed': abstract_embed,\n    }\n    return output_features\n\n\nr = process(pubs[\"title\"], pubs[\"abstract\"])\n</code></pre> <p>Note</p> <p>The <code>hub.load</code> function execution may take a little while on the first run as it needs to download the USE model files.</p> <p>Taking a look at the generated embeddings, we can see that for each publication we have created two arrays of floating point values each 512 items long. The next step is to save these back into a temporary table in BigQuery so that we can apply k-means clustering to attempt to find groups of semantic similar publications.</p> <p></p>"},{"location":"tutorials/05-topic_clusters/#4-save-embedding-vectors-to-a-bigquery-table","title":"4. Save embedding vectors to a BigQuery table","text":"<p>The next stage is to save the generated title and abstract embedding vectors back into a BigQuery table.</p> <pre><code>new_pubs = pubs.copy()\ntable_name = \"temp.semantic_clustering\"\n\n# Insert into the new DataFrame columns for title and abstract embeddings.\nnew_pubs[\"title_embed\"] = r[\"title_embed\"].numpy().tolist()\nnew_pubs[\"abstract_embed\"] = r[\"abstract_embed\"].numpy().tolist()\n\n# Save the DataFrame to a pickle file and into a BigQuery table.\nnew_pubs.to_pickle(\"/content/new_pubs.pkl\")\nnew_pubs.to_gbq(table_name, project_id, chunksize=2500)\n</code></pre> <p>One issue with the imported data is that the vectors for the embeddings is a string rather than a field containing repeated floating-point values. The SQL statement below however creates a new table that converts the string into a proper array of real numbers.</p> <pre><code>CREATE OR REPLACE TABLE `my-gbq-project.clustering.nz_pubs_with_embeddings`\nAS\nSELECT\n  id, title, abstract,\n  REGEXP_EXTRACT_ALL(orgs, r\"\\'([^\\s,\\']+)\\'\") as orgs,\n  (\n    SELECT ARRAY_AGG(CAST(v as FLOAT64) ORDER BY o asc)\n    FROM UNNEST(SPLIT(REGEXP_EXTRACT(title_embed, r\"^\\[(.*)\\]$\"), \", \")) v WITH OFFSET o\n  ) AS title_embed,\n  (\n    SELECT ARRAY_AGG(CAST(v as FLOAT64) ORDER BY o asc)\n    FROM UNNEST(SPLIT(REGEXP_EXTRACT(abstract_embed, r\"^\\[(.*)\\]$\"), \", \")) v WITH OFFSET o\n  ) AS abstract_embed\nFROM `my-gbq-project.temp.semantic_clustering`\n</code></pre> <p>Taking a look at the table within Google BigQuery (web interface) we can see the schema. It contains <code>title_embed</code> and <code>abstract_embed</code> both of which are repeated fields of FLOAT type.</p> <p></p>"},{"location":"tutorials/05-topic_clusters/#5-create-k-means-cluster-models","title":"5. Create k-means cluster models","text":"<p>Warning</p> <p>The example BigQuery SQL statement below use BigQuery ML which has a different charging model when it comes to creating models. Please keep this in mind and approximate how much it could potentially cost before executing any of the SQL statements. Pricing details are available from Google here: BigQuery ML Pricing.</p> <p>The most time consuming and computationally expensive part of this example is the clustering process itself. Luckily we can utilise BigQuery ML to create the models and create the clusters of publications based on the word embeddings we have created previously.</p> <p>In this example we will use k-means clustering to attempt to assign each publication to a grouping of semantically similar publications (based on abstracts). Essentially k-means clustering attempts to partition the individual items using Euclidean distance as the metric and minimising the within cluster sum of squares (ie. minimise squared errors).</p> <p>The primary input parameter that controls the k-means models is the number of partitions (ie. how many clusters we want to partition into). We can use some crude hyper-parameter tuning through creating numerous k-means models on our dataset, attempting to determine what a \u201cgood\u201d number of clusters may look like for our set of data. It is important to understand however that because of the approach k-means clustering utilises when assigning items to partitions it is important to analysis the results as a local minimums are a possibility.</p> <pre><code>DECLARE NUM_CLUSTERS INT64 DEFAULT 10;\nDECLARE MODEL_NAME STRING;\n\nWHILE NUM_CLUSTERS &lt; 60 DO\nSET MODEL_NAME = CONCAT('my-gbq-project.clustering.model_nz_pubs_',\n                            CAST(NUM_CLUSTERS AS STRING));\nEXECUTE IMMEDIATE format(\"\"\"\nCREATE OR REPLACE MODEL `%s`\nOPTIONS(model_type='kmeans',\n        num_clusters = %d,\n        DISTANCE_TYPE = 'cosine',\n        kmeans_init_method = 'KMEANS++') AS\nWITH data AS (\n  SELECT\n    abstract_embed[OFFSET(0)] as abstract0,\n    abstract_embed[OFFSET(1)] as abstract1,\n    abstract_embed[OFFSET(2)] as abstract2,\n    abstract_embed[OFFSET(3)] as abstract3,\n    abstract_embed[OFFSET(4)] as abstract4,\n    abstract_embed[OFFSET(5)] as abstract5,\n    abstract_embed[OFFSET(6)] as abstract6,\n    abstract_embed[OFFSET(7)] as abstract7,\n...\n    abstract_embed[OFFSET(510)] as abstract510,\n    abstract_embed[OFFSET(511)] as abstract511\n  FROM `my-gbq-project.clustering.nz_pubs_with_embeddings`\n)\nSELECT * FROM data;\n\"\"\", MODEL_NAME, NUM_CLUSTERS);\n\nSET NUM_CLUSTERS = NUM_CLUSTERS + 5;\nEND WHILE\n</code></pre> <p>Note</p> <p>The SQL statement above has been truncated. The <code>abstract_embed[OFFSET(n)] as abstract_n</code> lines have been omitted and goes from 0 to 511 uninterrupted and has been abbreviated for the purposes of brevity in the tutorial write-up. The full SQL is available here.</p> <p>Creating the models may take a little while as it must apply the k-means algorithm over our dataset for each of the different cluster count parameter values. The next step is to analyse the results for all of the models created. We can evaluate each of the models, returning the Davies\u2013Bouldin index and the mean squared distance. Determining the optimal number of clusters is outside the scope of this tutorial, however a common approach is using the \u201cElbow method\u201d.</p> <pre><code>#StandardSQL\nSELECT 60 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_60`)\nUNION ALL\nSELECT 55 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_55`)\nUNION ALL\nSELECT 50 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_50`)\nUNION ALL\nSELECT 45 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_45`)\nUNION ALL\nSELECT 40 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_40`)\nUNION ALL\nSELECT 35 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_35`)\nUNION ALL\nSELECT 30 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_30`)\nUNION ALL\nSELECT 25 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_25`)\nUNION ALL\nSELECT 20 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_20`)\nUNION ALL\nSELECT 15 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_15`)\nUNION ALL\nSELECT 10 as cluster_size, * FROM ML.EVALUATE(MODEL `my-gbq-project.clustering.model_nz_pubs_10`)\n</code></pre>"},{"location":"tutorials/05-topic_clusters/#6-publication-to-cluster-assignment","title":"6. Publication to cluster assignment","text":"<p>Using k=55 (<code>my-gbq-project.clustering.model_nz_pubs_55</code>) as the cluster count from above we can now use the k-means model we created in BigQuery to determine which publications are assigned to each different centroid (cluster). Note below that the input data into the model matches the data structure that was used during the creation of the model itself (the abstracts embedding vector exploded into 512 individual string inputs).</p> <pre><code>CREATE TABLE `my-gbq-project.clustering.pubs_assigned_55`\nAS\n\nWITH data AS (\n  SELECT\n    id,\n    abstract_embed[OFFSET(0)] as abstract0,\n    abstract_embed[OFFSET(1)] as abstract1,\n    abstract_embed[OFFSET(2)] as abstract2,\n    ...\n    abstract_embed[OFFSET(510)] as abstract510,\n    abstract_embed[OFFSET(511)] as abstract511\n  FROM `my-gbq-project.clustering.nz_pubs_with_embeddings`\n)\n\nSELECT id, CENTROID_ID, NEAREST_CENTROIDS_DISTANCE FROM\nML.PREDICT(MODEL `my-gbq-project.clustering.model_nz_pubs_55`,\n(\n    SELECT * FROM data\n))\n</code></pre> <p>Taking a look at the resultant table we see that each publication has been assigned to a primary cluster (centroid identifier) as well as a listing of the closest centroids as well as the Euclidean distance to the centroid.</p> <p></p>"},{"location":"tutorials/05-topic_clusters/#7-determine-conceptstopics-for-each-cluster","title":"7. Determine concepts/topics for each cluster","text":"<p>The next stage is to try and determine the topics and concepts associated with each of the 55 clusters identified within the assigned publications above. Some publications have concepts extracted via NLP processing of the full-text. One approach to determining the concepts associated with each cluster is to aggregate these extracted concepts from for all publications within the same cluster.</p> <pre><code>SELECT\n    ANY_VALUE(centroid) as centroid_id,\n    cluster_id as cluster_name,\n    ARRAY_AGG(concept order by ordering) as concepts\nFROM (\n    SELECT * FROM (\n      SELECT *, ROW_NUMBER() OVER (PARTITION BY cluster_id ORDER BY count desc) as ordering\n      FROM (\n        SELECT ANY_VALUE(centroid) as centroid, a.cluster_id as cluster_id, concept, COUNT(*) as count\n        FROM (\n          SELECT\n            a.id,\n            a.CENTROID_ID as centroid,\n            CONCAT(\"Cluster-\", CAST(a.CENTROID_ID as STRING)) as cluster_id,\n            (SELECT ARRAY_AGG(LOWER(c.concept)) FROM UNNEST(p.concepts) c) as concetps\n          FROM `my-gbq-project.clustering.pubs_assigned_55` a\n          LEFT JOIN `dimensions-ai.data_analytics.publications` p\n            ON p.id = a.id\n        ) a, UNNEST(a.concetps) concept\n        GROUP BY a.cluster_id, concept\n      )\n      ORDER BY cluster_id desc, ordering asc\n    )\n    WHERE ordering &lt;= 20\n)\nGROUP BY cluster_id\n</code></pre> <p>Taking a look at the results we can see for each centroid the 20 top concepts (by occurrence over all publications).</p> <p></p>"},{"location":"tutorials/06-funder_grant_pubs/","title":"Identify a funder's grants and publications","text":""},{"location":"tutorials/06-funder_grant_pubs/#use-case","title":"Use case","text":"<p>How many grants have resulted in publications? This notebook serves two main purposes:</p> <p>i. Enables identification of a funder's grants and resulting publications.</p> <p>ii. Prepares a summary table of grants and publications (per year and in total), and a bar graph showing number of grants by year with and without publications.</p> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> <li>You have a valid Dimensions API account.</li> <li>You have some basic familiarity with Python and Jupyter notebooks.</li> </ul> <p>This tutorial is based on a Jupyter notebook that is available directly via GitHub.</p>"},{"location":"tutorials/06-funder_grant_pubs/#method","title":"Method","text":"<p>This notebook retrieves grants and publication data from Dimensions the world's largest linked research information dataset. For more about Dimensions visit - https://www.dimensions.ai/</p> <p>To complete this analysis the following steps are taken:</p> <ol> <li>Connect to the Dimensions database.</li> <li>Find funder's unique GRID ID. The Global Research Identifier Database (GRID) is a database of research-related organisations each with a unique and persistent identifier. For more about GRID visit - https://grid.ac/.</li> <li>Generate list of grants and resulting publications.</li> <li>(OPTIONAL) Export list of grants and resulting publications to excel.</li> <li>Summary table showing number of grants and publications in total and by year.</li> <li>Graph showing number of grants that have yielded one or more publications by year.</li> </ol>"},{"location":"tutorials/06-funder_grant_pubs/#using-this-notebook","title":"Using this notebook","text":"<p>Before running some cells you will have to fill in information about the funder and year range. When required instructions are set out in info boxes.</p> <p>This notebook assumes installation of python packages <code>pandas</code> and <code>plotly</code>.</p>"},{"location":"tutorials/06-funder_grant_pubs/#1-connect-to-dimensions","title":"1. Connect to Dimensions","text":"<p>You will need to be authenticated to run these queries\u2014see the \"Verifying your connection tutorial for options.</p> <p>Run this cell if you're connecting via a Google Colab notebook:</p> <pre><code>from google.colab import auth\nauth.authenticate_user()\nprint('Authenticated')\n</code></pre> <pre><code>Authenticated\n</code></pre> <pre><code>import pandas as pd\nfrom google.cloud import bigquery\n\nGBQ_PROJECT_ID = 'ds-gov-funder-gbq' # &lt;----- Input your project ID here\nclient = bigquery.Client(project=GBQ_PROJECT_ID)\n</code></pre>"},{"location":"tutorials/06-funder_grant_pubs/#2-find-funders-unique-grid-id","title":"2. Find funder's unique GRID ID","text":"<p>To identify funder's grants we need the funder's unique GRID ID. The following cell searches on the funders names and returns the associated GRID ID(s).</p> <p>Instructions</p> <ol> <li>Before running the code in the following cell input funder's name</li> <li>Funder's name might return multiple GRID IDs, note all relevant GRID IDs as they are needed for the next step.</li> </ol> <p>If no GRID IDs are returned, try searching different name variants for the funder. Avoid acronyms.</p> <pre><code>funder_name = 'Wellcome Trust' # Input funder name here\n\nfunder_name = funder_name.lower()\nquery = f\"\"\"\n    select\n        id as grid_id\n        , name\n    from `dimensions-ai.data_analytics.grid`\n    where lower(name) like '%{funder_name}%'\n\"\"\"\n\nfunder = pd.read_gbq(query, project_id=GBQ_PROJECT_ID)\nfunder.head(10)\n</code></pre> Row grid_id name 0 grid.52788.30 Wellcome Trust 1 grid.478079.5 NIHR Wellcome Trust Southampton Clinical Resea... 2 grid.484745.e Wellcome Trust/DBT India Alliance 3 grid.482844.6 Wellcome Trust Centre for the History of Medicine 4 grid.419393.5 Malawi-Liverpool-Wellcome Trust Clinical Resea..."},{"location":"tutorials/06-funder_grant_pubs/#3-generate-list-of-grants-and-publications-identifiers","title":"3. Generate list of grants and publications identifiers","text":"<p>The following cell generates a table of funders grants and any resulting publications for grants started in a defined time period. Only the first five rows of the table will be shown in this notebook, however the whole table can be exported to excel in the following step.</p> <p>Table columns:</p> <ul> <li><code>dimensions_grant_id</code> - Unique identifier for grant in Dimensions</li> <li><code>grant_start_year</code> - Year the grant started</li> <li><code>grant_number</code> - Grant number used by funder</li> <li><code>dimensions_publication_id</code> - Unique identifier for publications in Dimensions</li> <li><code>doi</code> - Digital Object Identifier</li> </ul> <p>Instructions</p> <p>Before running the code in the following cell, where indicated:</p> <ol> <li>input funders GRID ID between <code>('')</code> e.g. <code>('grid.52788.30')</code>. If funder has two or more GRID IDs then list within brackets separated by commas e.g. <code>('grid.453157.1', 'grid.453157.1')</code>.</li> <li>Update year range, note that we can only look at full years and cannot be filtered by month or day.</li> </ol> <pre><code>query = f\"\"\"\nwith pubs as\n  (select id, doi from `dimensions-ai.data_analytics.publications`)\n, grants as\n  (\n  select\n    id as dimensions_grant_id\n    , start_year as grant_start_year\n    , grant_number\n    , pub_id as dimensions_publication_id\n  from `dimensions-ai.data_analytics.grants` g left join unnest(g.resulting_publication_ids) as pub_id\n   where funder_org in ('grid.52788.30')  -- ADD GRID ID HERE\n      and start_year between 2011 and 2015 -- UPDATE YEAR RANGE\n    )\nSelect\n  grants.*\n  , pubs.doi\nfrom grants\n  left join pubs\n    on pubs.id = grants.dimensions_publication_id\n\"\"\"\ngrants = pd.read_gbq(query)\ngrants.head()\n</code></pre> Row dimensions_grant_id grant_start_year grant_number dimensions_publication_id doi 0 grant.3627143 2012 082265/Z/07/A pub.1105902779 10.1016/j.jaci.2018.07.011 1 grant.3635772 2012 098274/Z/12/Z pub.1124346329 10.1016/j.bpj.2020.01.019 2 grant.3636003 2012 097899/Z/11/Z pub.1062809971 10.1016/j.jaci.2018.07.011 3 grant.3642390 2012 098649/Z/12/Z None None 4 grant.3638982 2012 099618/Z/12/Z pub.1023307051 10.1007/s00234-016-1648-3"},{"location":"tutorials/06-funder_grant_pubs/#4-optional-export-list-of-grants-and-resulting-publications-to-excel","title":"4. (OPTIONAL) Export list of grants and resulting publications to excel","text":"<p>The following cell creates new excel file of grants and resulting publications as output from previous cell. The new file will appear in the same location as this notebook is saved.</p> <p>Instructions</p> <p>Before running code the code in the following cell replace <code>FILE_NAME</code> with new file name.</p> <pre><code># Creates new excel file.\nwriter = pd.ExcelWriter('FILE_NAME.xlsx',  engine='openpyxl')\n\ngrants.to_excel(writer, sheet_name='grants_pubs')\n\nwriter.save()\n</code></pre>"},{"location":"tutorials/06-funder_grant_pubs/#5-summary-table-of-number-of-grants-and-publications-in-total-and-by-year","title":"5. Summary table of number of grants and publications in total and by year","text":"<p>The following cell creates a table with columns, for all years and per year:</p> <ul> <li><code>grant_start_year</code> - first year of the grant</li> <li><code>grants</code> - number of grants awarded by funder</li> <li><code>publications</code> - number of publications resulting from awards by funder</li> <li><code>grants_without_publications</code> - number of grants that have not resulted in a publication</li> <li><code>perc_without_publications</code> - percentage of grants that have not resulted in a publication</li> <li><code>avg_publications_per_grants</code> - average (mean) number of publications per grant</li> </ul> <p>Instructions</p> <p>Before running code the code in the following cell, where indicated:</p> <ol> <li>input funders GRID ID between <code>('')</code> e.g. <code>('grid.52788.30')</code>. If funder has two or more GRID IDs then list within brackets separated by commas e.g. <code>('grid.453157.1', 'grid.453157.1')</code>.</li> <li>Update year range</li> </ol> <pre><code>query = f\"\"\"\nwith grant_pub_map as\n  (\n  select\n    id\n    , start_year\n   , pub_id  -- is this the best way of finding publications? or search for grant or\n  from `dimensions-ai.data_analytics.grants` g left join unnest(g.resulting_publication_ids) as pub_id\n    where funder_org = 'grid.52788.30'  -- ADD GRID IDs HERE\n      and start_year between 2011 and 2015 -- UPDATE YEAR RANGE\n  )\nselect\n  'All years' as grant_start_year\n  , count(distinct id) as grants\n  , count(pub_id) as publications\n  , countif(pub_id is null) as grants_without_publications\n  , round(((countif(pub_id is null)/count(distinct id))*100), 2) as perc_without_publications\n  , round((count(pub_id)/(count(distinct id) )),2) as avg_publications_per_grants\nfrom grant_pub_map\n union all\nselect\n  cast(start_year as string) as grant_start_year\n  , count(distinct id) as grants\n  , count(pub_id) as publications\n  , countif(pub_id is null) as grants_without_publications\n  , round(((countif(pub_id is null)/count(distinct id))*100), 2) as perc_without_publications\n  , round((count(pub_id)/(count(distinct id) )),2) as avg_publications_per_grants\nfrom grant_pub_map\n  group by start_year\norder by 1 desc ;\n\"\"\"\n\ngrants1 = pd.read_gbq(query)\ngrants1.head(5)\n</code></pre> Row grant_start_year grants publications grants_without_publications perc_without_publications avg_publications_per_grants 0 All years 6423 45840 3731 58.09 7.14 1 2015 1417 6901 833 58.79 4.87 2 2014 1327 8746 791 59.61 6.59 3 2013 1200 8217 690 57.50 6.85 4 2012 1159 8515 689 59.45 7.35"},{"location":"tutorials/06-funder_grant_pubs/#6-bar-graph-showing-number-of-grants-that-have-yielded-one-or-more-publications-by-year","title":"6. Bar graph showing number of grants that have yielded one or more publications by year","text":"<p>Using the table above the following cell generates a bar graph showing the number of grants with and without publications per year.</p> <pre><code># Remove first row that counts all grants over all years\ndf = grants1.iloc[1:]\n\n# Reverse rows so earlist year shown first on bar graph\ndf = df[::-1].reset_index()\n\n# Calculate number of grants with publications\ndf.insert(5, 'grant_pubs', df[\"grants\"] - df[\"grants_without_publications\"])\n\n\nimport plotly.graph_objs as go\n\n\nx = df.grant_start_year\n\ntrace1 = {\n    'x' : x,\n    'y' : df.grant_pubs,\n    'name' : 'Grants with publications',\n    'type' : 'bar'\n};\ntrace2 = {\n    'x' : x,\n    'y' : df.grants_without_publications,\n    'name' : 'Grants without publications',\n    'type' : 'bar'\n};\ndata = [trace1, trace2];\nlayout = {\n    'xaxis' : {'title' : 'Year'},\n    'yaxis' : {'title' : 'Number of grants'},\n    'barmode' : 'relative',\n    'title': 'Grants with and without publications'\n};\nfig = go.Figure(data = data, layout = layout)\nfig.show()\n</code></pre> <p></p>"},{"location":"tutorials/07-retrieve_patents/","title":"Retrieve patents linked to a set of grants","text":"<p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project.</li> <li>You have a valid Dimensions API account.</li> <li>You have some basic familiarity with Python and Jupyter notebooks.</li> </ul> <p>This example uses the Dimensions database to determine the number of patents generated by a set of grants, providing another perspective on the impact of funding from different organizations. There are two types of linkages considered here:</p> <ul> <li>Patents that directly acknowledge grants. Since the enactment of the Bayh-Dole Act in 1980, all recipients of U.S. federal funding are legally obligated to disclose government support that led to any inventions they produce. This is useful when funders are most interested in the patents directly supported by the grants.</li> <li>Patents that cite publications supported by grants. This can be considered by funders who are interested in understanding patents that build less directly on research that they supported.</li> </ul> <p>Note</p> <p>Non-patent literatures in the patent references section are not only provided by the inventors, but also added by patent examiners during the patent examination process.</p> <p>This tutorial is based on a Jupyter notebook that is available directly via GitHub.</p> <pre><code># general import\nimport pandas as pd\nimport numpy as np\nimport sys, time, json\nimport plotly.express as px\n\n# authentication happens via your browser\nfrom google.colab import auth\nauth.authenticate_user()\nprint('Authenticated')\n</code></pre> <pre><code>Authenticated\n</code></pre> <pre><code># GBQ import\nfrom google.cloud import bigquery\n\nBQ_PROJECT_ID = \"ds-gov-funder-gbq\"  # remember to change this to your project ID\nclient = bigquery.Client(project=BQ_PROJECT_ID)\n</code></pre>"},{"location":"tutorials/07-retrieve_patents/#an-example","title":"An example","text":"<p>For this example, we will be using the Dimensions database to extract patents that are linked to the grants funded by NSF Directorate for Engineering and started in 2015.</p> <p>The patents, publications, and grants datasets will be used in the example to retrieve patents through two types of linkages.</p>"},{"location":"tutorials/07-retrieve_patents/#retrieve-patents-that-directly-acknowledged-grants","title":"Retrieve patents that directly acknowledged grants","text":"<p>First, let's get all patents that directly acknowledged the NSF Directorate for Engineering grants that started in 2015</p> <p>Tips</p> <ul> <li>Unnest the <code>funding_details</code> in <code>dimensions-ai.data_analytics.patents</code> table to get the funder's GRID ID and Dimensions Grant ID</li> <li>Join with <code>dimensions-ai.data_analytics.grants</code> table and limit to the grants that started in 2015</li> <li>The GRID ID of NSF Directorate for Engineering can be found in Global Research Identifier Database</li> </ul> <pre><code># build the search string\nsearch_string = \"\"\"\nSELECT\n  DISTINCT pat.id AS patent_id,\n  f.grant_id,\n  pat.family_id,\n  CAST(pat.priority_year AS string) priority_year,\n  'supported' AS link_type\nFROM\n  `dimensions-ai.data_analytics.patents` pat\nCROSS JOIN\n  UNNEST(funding_details) f     -- unnest the field to get the funder and grant number acknowledged by the patent\nJOIN\n  `dimensions-ai.data_analytics.grants` g\nON\n  g.id = f.grant_id\nWHERE\n  f.grid_id = 'grid.457810.f'    -- specify funder's GRID ID\n  AND g.start_year = 2015        -- specify the grant start year\n\"\"\"\n\n# retrieve from BigQuery and make it a pandas dataframe\nnsf_grant_patents = client.query(search_string).to_dataframe()\n</code></pre> <pre><code># get a quick preview of the patents directly linked to the grants\nnsf_grant_patents.head()\n</code></pre> Row patent_id grant_id family_id priority_year link_type 0 US-20200303900-A1 grant.4179692 60479104 2016 supported 1 US-20190224370-A1 grant.4178213 61562333 2016 supported 2 US-20160236141-A1 grant.3852639 56620694 2014 supported 3 US-20190193116-A1 grant.3982138 60663733 2016 supported 4 US-20200061618-A1 grant.4318677 69584146 2018 supported <pre><code># get a quick count of how many patents were retrieved\nprint(nsf_grant_patents['patent_id'].nunique())\n</code></pre> <pre><code>581\n</code></pre>"},{"location":"tutorials/07-retrieve_patents/#retrieve-patents-that-cited-the-publications-funded-by-the-same-set-of-grants","title":"Retrieve patents that cited the publications funded by the same set of grants","text":"<p>Then, we can get all patents that cited publications which were funded by the same set of grants (i.e. grants funded by NSF Directorate for Engineering started in 2015)</p> <p>Tips</p> <ul> <li>unnest the <code>resulting_publication_ids</code> in <code>dimensions-ai.data_analytics.grants</code> table to get the publication ids funded by a set of grants, setting the funder's GRID ID and Start year the same as the above query</li> <li>unnest the <code>publication_ids</code> in <code>dimensions-ai.data_analytics.patents</code> table which contains publication ids cited by patents</li> <li>the publication_ids unnested from the above are used as an intermediate link between patents and grants</li> </ul> <pre><code># build the search string\nsearch_string = \"\"\"\nWITH\n  grant_pubs AS (\n  SELECT\n    DISTINCT pub_id,\n    g.id AS grant_id\n  FROM\n    `dimensions-ai.data_analytics.grants` g\n  CROSS JOIN\n    UNNEST(resulting_publication_ids) pub_id   -- unnest the publication ids resulting from grants\n  WHERE\n    g.funder_org = 'grid.457810.f'     -- specify funder grid id\n    AND g.start_year = 2015)           -- specify grant start year\nSELECT\n  DISTINCT pat.id AS patent_id,\n  gp.grant_id,\n  pat.family_id,\n  CAST(pat.priority_year AS string) priority_year,\n  'pub_reference' AS link_type\nFROM\n  `dimensions-ai.data_analytics.patents` pat\nCROSS JOIN\n  UNNEST(publication_ids) pub_ref     -- unnest the publication ids cited by patents\nJOIN\n  grant_pubs gp\nON\n  gp.pub_id = pub_ref                 -- join on publicaiton id\n\"\"\"\n\n# retrieve from BigQuery and make it a pandas dataframe\nnsf_pub_ref_patents = client.query(search_string).to_dataframe()\n\n# get a quick preview of the patents that cited publications which were funded by NSF grants\nnsf_pub_ref_patents.head()\n</code></pre> Row patent_id grant_id family_id priority_year link_type 0 WO-2019202933-A1 grant.4179511 68239624 2018 pub_reference 1 US-10528687-B2 grant.3861071 60158395 2016 pub_reference 2 DE-102017002874-A1 grant.3981846 61731654 2017 pub_reference 3 US-10725209-B2 grant.4312170 62908810 2017 pub_reference 4 US-10196708-B2 grant.4312419 62782292 2017 pub_reference <pre><code># get a quick count of how many patents were retrieved\nprint(nsf_pub_ref_patents['patent_id'].nunique())\n</code></pre> <pre><code>224\n</code></pre>"},{"location":"tutorials/07-retrieve_patents/#merge-results","title":"Merge results","text":"<p>Now we will merge two data frames to have a complete set of patents that are directly and indirectly linked to the set of grants</p> <pre><code>nsf_patents = pd.concat([nsf_grant_patents, nsf_pub_ref_patents]).reset_index()\n\n# get a quick count of how many patents in total\nprint(nsf_patents['patent_id'].nunique())\n</code></pre> <pre><code>799\n</code></pre>"},{"location":"tutorials/07-retrieve_patents/#quick-overview-of-patents","title":"Quick overview of patents","text":"<p>Lastly, we can examine the trends in the patents by priority year</p> <p>Tips</p> <ul> <li><code>family_id</code> was used to deduplicate the patent documents, one patent family is a collection of patent documents that are considered to cover a single invention (see definition in DOCDB Simple patent family)</li> <li><code>priority_year</code> was used to aggregate the patents, since it indicates the time when the invention was established. All patent documents in one patent family share the same priority date</li> </ul> <pre><code>nsf_patents.groupby(['link_type','priority_year'], as_index = False).agg({'family_id':'nunique'})\\\n           .rename(columns={'family_id':'n_pat_families'})\\\n           .pivot (values = 'n_pat_families', index = 'priority_year', columns = 'link_type')\n</code></pre> link_type pub_reference supported priority_year 2004 3.0 NaN 2005 1.0 NaN 2009 1.0 NaN 2010 NaN 1.0 2011 1.0 NaN 2012 1.0 1.0 2013 4.0 5.0 2014 8.0 15.0 2015 10.0 42.0 2016 41.0 90.0 2017 57.0 123.0 2018 57.0 63.0 2019 15.0 42.0 2020 1.0 1.0 <p>In addition, we can also create a quick visualization of the trends</p> <pre><code>plot_data = nsf_patents.groupby(['link_type','priority_year'], as_index = False)\\\n           .agg({'family_id':'nunique'})\\\n           .rename(columns={'family_id':'n_patent_families'})\n\n# create line plot by using plotly express\nfig = px.line(plot_data, x=\"priority_year\", y=\"n_patent_families\", color = \"link_type\", title='Trends in patents supported by NSF Directorate for Engineering grants starting 2015')\nfig.show()\n</code></pre>"},{"location":"tutorials/08-research_integrity/","title":"Usage of Trust Markers in research","text":""},{"location":"tutorials/08-research_integrity/#use-case","title":"Use case","text":"<p>'Trust Markers' are indicators of integrity, professionalism and reproducibility in scientific research. They also highlight the level of research transparency within the document, and reduce the risks of allowing non-compliance to research integrity policies to go unobserved.</p> <p>This notebook takes you through a few examples which address the above questions. It makes use of the Dimensions Research Integrity Dataset, an additional module to Dimensions on Google Big Query (GBQ).</p> <p>Using the dataset, you can answer questions such as:</p> <ul> <li>How many research articles use Trust Markers?</li> <li>How does coverage of Trust Markers differ across publishers, funders and research organisations?</li> <li>If researchers are using Trust Markers (eg, data availability statements), how many are putting their data in repositories (and which repositories)?</li> </ul> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that:</p> <ul> <li>You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project. This must include access to the Dimensions Research Integrity Dataset.</li> <li>You have some basic familiarity with Python and Jupyter notebooks.</li> </ul>"},{"location":"tutorials/08-research_integrity/#about-trust-markers","title":"About Trust Markers","text":"<p>The Trust Markers in the Dataset represent the integrity and reproducibility of scientific research. Trust Markers represent a contract between authors and readers that proper research practices have been observed. They also highlight the level of research transparency within the document, and reduce the risks of allowing non-compliance to research integrity policies to go unobserved.</p> <p>To read definitions of specific Trust Markers, see the GBQ schema documentation.</p>"},{"location":"tutorials/08-research_integrity/#method","title":"Method","text":"<p>This notebook retrieves data about trust marker and publication data from Dimensions, the world's largest linked research information datatset. In particular the Trust Markers are taken from the DRI module.</p> <p>To complete the analysis the following steps are taken:</p> <ol> <li>Connect to the Dimensions database.</li> <li>Gather information about general use of Trust Markers, broken down by publisher.</li> <li>Look at how usage of Trust Markers breaks down by research organisations in the US, by joining on data from GRID.</li> <li>Find out some of the most commonly claimed contributions by inviduals to research across different fields.</li> <li>Understand the usage of repositories across funders, research orgs and research articles.</li> </ol>"},{"location":"tutorials/08-research_integrity/#1-connect","title":"1. Connect","text":"<p>You will need to be authenticated to run these queries - see the \"Verifying yout connection\" tutorial for options.</p> <pre><code>from google.colab import auth\nauth.authenticate_user()\nprint('Authenticated')\n</code></pre> <pre><code>Authenticated\n</code></pre> <pre><code>#import other packages/modules needed for analysis\nfrom google.cloud import bigquery\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objs as go\n</code></pre> <pre><code>#config to avoid having to declare parameters multiple times\nproject_id = \"ds-ripeta-gbq\" #replace 'project' with the required project associate with your account\n\nfrom google.cloud.bigquery import magics\nmagics.context.project = project_id\n\nclient = bigquery.Client(project = project_id)\n</code></pre>"},{"location":"tutorials/08-research_integrity/#2-trust-markers-by-publisher","title":"2. Trust Markers by publisher","text":""},{"location":"tutorials/08-research_integrity/#write-and-run-a-query","title":"Write and run a query","text":"<p>In this instance we will limit data to 2022 and 10 publishers to keep things manageable.</p> <pre><code>#write the query - we're limiting results here to keep things easy to follow\nqry = client.query(\"\"\"\nSELECT\n    p.publisher.name,\n    100 * COUNTIF(tm.data.data_availability_statement.present)/ COUNT(p.id) AS data_availability,\n    100 * COUNTIF(tm.code.code_availability_statement.present)/COUNT(p.id) AS code_availability,\n    100 * COUNTIF(tm.authors.author_contribution_statement.present)/COUNT(p.id) AS author_contributions,\n    100 * COUNTIF(tm.authors.conflict_of_interest_statement.present)/COUNT(p.id) AS conflict_interest,\n    100 * COUNTIF(tm.funding.funding_statement.present)/COUNT(p.id) AS funding_statement,\n    #note here we are only counting articles with mesh terms of 'animal' or 'human' as if this criteria isn't met it is unlikely an ethics statement would be expected\n    100 * COUNTIF((tm.ethical_approval.ethical_approval_statement.present AND (('Humans' IN UNNEST(p.mesh_terms)) OR ('Animals' IN UNNEST(p.mesh_terms)))) )/\n        NULLIF(COUNTIF(('Humans' in UNNEST(p.mesh_terms)) OR ('Animals' IN unnest(p.mesh_terms))), 0) AS ethics_approval\nFROM\n    dimensions-ai.data_analytics.publications p\nINNER JOIN `dimensions-ai-integrity.data.trust_markers` tm\n    ON p.id = tm.id\nWHERE p.year = 2022 AND p.document_type.classification = 'RESEARCH_ARTICLE'\nGROUP BY 1\nORDER BY COUNT(p.id) DESC #order by number of publications in the trust marker dataset\n--To keep things manageable for display purposes, we'll only look at 10 publishers for now\nLIMIT 10\n\"\"\")\n\n#get the results\nresults = qry.result().to_dataframe() #may take a while depending on how much data your return\n\n#take a peak\nresults\n</code></pre> name data_availability code_availability author_contributions conflict_interest funding_statement ethics_approval 0 Elsevier 22.890000 1.597544 17.378246 80.405263 57.918947 18.943601 1 Springer Nature 49.878036 8.742873 52.908014 40.728133 71.049643 65.175558 2 MDPI 88.760718 1.339980 97.192921 99.243270 91.880857 13.336054 3 Wiley 39.376133 1.139870 20.882104 7.671746 58.562563 16.154602 4 Frontiers 97.583713 1.031047 99.547012 6.398587 79.234019 75.641116 5 Taylor &amp; Francis 17.195639 0.605616 11.305576 83.174962 52.912265 28.457937 6 Institute of Electrical and Electronics Engine... 0.300312 0.723896 0.208514 0.599313 3.561780 5.201794 7 American Chemical Society (ACS) 2.348741 0.794581 42.137319 1.914696 75.219210 4.040979 8 Hindawi 85.378842 1.029393 23.275609 85.217410 52.681454 23.068174 9 SAGE Publications 9.527544 0.604082 18.841010 4.317599 90.031582 33.596158"},{"location":"tutorials/08-research_integrity/#visualize-your-results","title":"Visualize your results","text":"<pre><code>#make data 'long'\nlong_results = pd.melt(\n    results,\n    id_vars = \"name\",\n    value_vars = results.columns.tolist()[1:7],\n    var_name = \"trust_marker\",\n    value_name = \"pct\"\n    )\n\n#convert your data to a dictionary so plotly can handle it\nresult_dict = {\n    \"z\": long_results.pct.tolist(),\n    \"x\": long_results.trust_marker.tolist(),\n    \"y\": long_results.name.tolist()\n}\n\n#plot\nplot = go.Figure(\n    data = go.Heatmap(\n        result_dict,\n        colorscale = \"Blues\"\n        )\n    )\n\nplot.show()\n</code></pre>"},{"location":"tutorials/08-research_integrity/#3-trust-markers-by-research-org","title":"3. Trust Markers by research org","text":"<p>There are plenty of other analyses we can carry out using DRI. We are also not obliged to pull in aggregated data - we can also pull in data 'row-by-row' and analyse further in Python. We'll do this in the next example to look at the proportion of articles using Trust Markers at US universities.</p> <p>Note that this example is set up to work generally. In a notebook environment (like Colab or Jupyter) you can use magic commands to reduce the code down to just the query and store as a summary dataframe. Google has guidance on doing this. We will use this magic commands in future examples.</p> <pre><code>%%bigquery markers_by_us_uni\n\nSELECT\n    p.id AS pub_id,\n    p.year,\n    orgs AS org_id,\n    CONCAT(g.name, ' (', g.address.city, ')') AS org_name,\n    tm.data.data_availability_statement.present AS das,\n    tm.code.code_availability_statement.present AS cas,\n    tm.authors.author_contribution_statement.present AS auth_cont,\n    tm.authors.conflict_of_interest_statement.present AS conflict_int,\n    tm.funding.funding_statement.present AS funding,\n    CASE\n      WHEN tm.ethical_approval.ethical_approval_statement.present IS TRUE AND ('Humans' IN UNNEST(p.mesh_terms) OR 'Animals' IN UNNEST(p.mesh_terms)) THEN TRUE\n      WHEN tm.ethical_approval.ethical_approval_statement.present IS FALSE AND ('Humans' IN UNNEST(p.mesh_terms) OR 'Animals' IN UNNEST(p.mesh_terms)) THEN FALSE\n      ELSE NULL\n      END AS ethics\nFROM dimensions-ai.data_analytics.publications p,\n     UNNEST(research_orgs) orgs\nINNER JOIN `dimensions-ai-integrity.data.trust_markers` tm\n    ON p.id = tm.id\nINNER JOIN dimensions-ai.data_analytics.grid g\n    ON orgs = g.id\n    AND 'Education' IN UNNEST(g.types)\n    AND g.address.country = \"United States\"\nWHERE\n    p.year = 2022\n    AND p.document_type.classification = 'RESEARCH_ARTICLE'\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> <pre><code>#you can see we've got the result straight to a df; take a look\nmarkers_by_us_uni.head(5)\n</code></pre> pub_id year org_id org_name das cas auth_cont conflict_int funding ethics 0 pub.1141586193 2022 grid.36425.36 Stony Brook University (Stony Brook) False False False False True &lt;NA&gt; 1 pub.1141411763 2022 grid.5288.7 Oregon Health &amp; Science University (Portland) False False True True False False 2 pub.1141060825 2022 grid.262273.0 Queens College, CUNY (New York) True False False True False False 3 pub.1140891786 2022 grid.255935.d Fisk University (Nashville) False False False True False &lt;NA&gt; 4 pub.1141168735 2022 grid.152326.1 Vanderbilt University (Nashville) False False False False False True <pre><code>#now we'll manipulate as required in Python\nmarker_df = markers_by_us_uni\n\n#flag marker cols\nmarker_cols = [\"das\", \"cas\", \"auth_cont\", \"conflict_int\", \"funding\", \"ethics\"]\n\n#work out if there is a least one marker\nmarker_df[\"tm\"] = marker_df[marker_cols].eq(1).any(axis = 1)\n\n#institutions w/ &lt;=1,000 publications\ngt1000 = (marker_df.\n          groupby([\"org_id\"], as_index = False).\n          agg({\"pub_id\": \"count\"})\n          )\n\ngt1000 = gt1000[gt1000[\"pub_id\"] &gt;= 1000][\"org_id\"].to_list()\n\n#summary\nmarker_sum = (marker_df.\n              groupby([\"org_id\", \"org_name\", \"tm\"], as_index = False).\n              agg({\"pub_id\": \"count\"})\n)\n\n#add on %\nmarker_sum[\"pct\"] = 100 * marker_sum[\"pub_id\"] / marker_sum.groupby([\"org_id\"])[\"pub_id\"].transform(\"sum\")\n\n#remove institutions w/ &lt;=1000 pubs and tm = False rows\nmarker_sum = marker_sum[(marker_sum[\"org_id\"].isin(gt1000)) &amp; (marker_sum[\"tm\"] == True)]\n\n#sort and slice to keep data manageable, pick the top 10 by number of publications...\nmarker_sum = marker_sum.sort_values(\"pub_id\", ascending = False).head(10)\n#...then order by pct for purposes of plot\nmarker_sum = marker_sum.sort_values(\"pct\", ascending = False)\n\nmarker_sum\n</code></pre> org_id org_name tm pub_id pct 1730 grid.38142.3c Harvard University (Cambridge) True 15532 85.797934 1309 grid.266102.1 University of California, San Francisco (San F... True 5955 85.511200 101 grid.21107.35 Johns Hopkins University (Baltimore) True 8246 84.800494 3785 grid.5386.8 Cornell University (Ithaca) True 5846 83.765582 1722 grid.34477.33 University of Washington (Seattle) True 7825 83.475571 47 grid.168010.e Stanford University (Stanford) True 7731 83.030824 713 grid.25879.31 University of Pennsylvania (Philadelphia) True 6571 82.747765 3647 grid.47100.32 Yale University (New Haven) True 5992 82.488987 85 grid.19006.3e University of California, Los Angeles (Los Ang... True 6714 82.329859 115 grid.214458.e University of Michigan\u2013Ann Arbor (Ann Arbor) True 7672 80.301444 <pre><code>#plot the data\nplot = px.bar(marker_sum, x = \"pct\", y = \"org_name\")\nplot.show()\n</code></pre>"},{"location":"tutorials/08-research_integrity/#4-author-contributions-to-articles","title":"4. Author contributions to articles","text":"<p>You can go beyond just the 'basic' Trust Markers with Dimensions Research Integrity. You can also look at related data, such as recorded contributions to papers by individuals, or at which repositories data is being deposited in.</p> <p>Let's take a look at author contributions by research categorisation (note: articles falling under multiple categories will be counted once in each category. Articles mentioning the same verb more than once are only counted once per category). This will help understand acknowledgement patterns in research and possibly identify discipline areas where practice is 'ahead of the curve'.</p> <pre><code>%%bigquery cont_df\n\nSELECT\n    p.year,\n    cat.name,\n    contributor_verbs,\n    COUNT(DISTINCT p.id) publications\nFROM dimensions-ai.data_analytics.publications p,\n    UNNEST(category_for.first_level.full) cat\nINNER JOIN `dimensions-ai-integrity.data.trust_markers` tm\n    ON p.id = tm.id,\n    UNNEST(tm.authors.author_roles.keywords) contributor_verbs\nWHERE  p.type= 'article' and p.year between 2011 and 2022\ngroup by 1, 2, 3\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> <pre><code>#see what we get\ncont_df.head(5)\n</code></pre> year name contributor_verbs publications 0 2021 Biomedical And Clinical Sciences using 2733 1 2016 Biomedical And Clinical Sciences using 1108 2 2022 Psychology using 315 3 2015 Biomedical And Clinical Sciences using 723 4 2021 Agricultural, Veterinary And Food Sciences using 456 <p>There are a lot of variables (field and verbs) here and we can't visualise them all in one go. Instead, let's identify the top five verbs and six fields (by number of publications) and stick with just those for now.</p> <pre><code>#get the most common verbs\ncommon_verbs = (cont_df.\n                groupby([\"contributor_verbs\"], as_index = False).\n                agg({\"publications\": \"sum\"}).\n                sort_values(\"publications\", ascending = False).\n                head(5)\n                )[\"contributor_verbs\"].to_list()\n\ncommon_verbs\n</code></pre> <pre><code>['performed', 'wrote', 'designed', 'contributed', 'approved']\n</code></pre> <pre><code>#and the most common fields\ncommon_fields = (cont_df.\n                groupby([\"name\"], as_index = False).\n                agg({\"publications\": \"sum\"}).\n                sort_values(\"publications\", ascending = False).\n                head(6)\n                )[\"name\"].to_list()\n\ncommon_fields\n</code></pre> <pre><code>['Biomedical And Clinical Sciences',\n 'Biological Sciences',\n 'Engineering',\n 'Health Sciences',\n 'Chemical Sciences',\n 'Agricultural, Veterinary And Food Sciences']\n</code></pre> <pre><code>#filter the data accordingly and sort\ncont_df = cont_df[(cont_df[\"contributor_verbs\"].isin(common_verbs)) &amp; (cont_df[\"name\"].isin(common_fields))].sort_values(\"year\")\n\n#and now plot the results\nplot = px.line(\n    cont_df,\n    x = \"year\",\n    y = \"publications\",\n    color = \"contributor_verbs\",\n    facet_col = \"name\",\n    facet_col_wrap = 3\n)\nplot.show()\n</code></pre>"},{"location":"tutorials/08-research_integrity/#5-repository-usage","title":"5. Repository usage","text":"<p>Now we'll take a look at how we can access the repositories data. Suppose we want to see the breadth of usage of specific repositories across work across funders and research orgs, to understand how wide the usage of the most common repositories is.</p> <pre><code>%%bigquery repo_df\n\n--the first two tables created by the WITH statements are needed to get full count of repositories based on names and URLs found\nWITH\n  common_keywords AS(\n  SELECT\n    kw,\n    COUNT(DISTINCT id) AS pubs\n  FROM `dimensions-ai-integrity.data.trust_markers` tm,\n    UNNEST(tm.data.data_locations.repositories) kw\n  GROUP BY 1\n),\nrepositories AS(\n  SELECT\n  id,\n  kw,\n  'url' isin\n    FROM `dimensions-ai-integrity.data.trust_markers` tm,\n      UNNEST(tm.data.data_locations.repository_urls) url\n    INNER JOIN common_keywords\n      ON REGEXP_CONTAINS(REPLACE(url,'10.17632','mendeley'), LOWER(kw))\n    UNION DISTINCT\n      SELECT\n        id,\n          replace(\n            replace(\n              #replace(\n                replace(kw,'open science framework','osf'),\n              #'gene','geo'),\n              'gene expression omnibus','geo'),\n            'sequence read archive','sra') kw,\n        'keyword' isin\n        FROM `dimensions-ai-integrity.data.trust_markers` tm,\n          UNNEST(tm.data.data_locations.repositories) kw\n        WHERE kw != 'board'\n        ),\n  funders AS(\n    SELECT\n    pubs.id AS pub_id,\n    fund.grid_id AS funder_id\n    FROM `dimensions-ai.data_analytics.publications` pubs,\n      UNNEST(funding_details) fund\n    WHERE pubs.year = 2021\n  ),\n  orgs AS(\n    SELECT\n      pubs.id AS pub_id,\n      org\n    FROM `dimensions-ai.data_analytics.publications` pubs,\n      UNNEST(research_orgs) org\n    WHERE pubs.year = 2021\n  ), combined AS(\n  SELECT\n    rep.id,\n     CASE\n        WHEN REGEXP_CONTAINS(rep.kw, 'github') THEN 'github'\n        WHEN REGEXP_CONTAINS(rep.kw, 'osf') THEN 'osf'\n        WHEN REGEXP_CONTAINS(rep.kw, 'ncbi') THEN 'ncbi'\n        ELSE rep.kw\n      END AS kw,\n    f.funder_id AS funder,\n    o.org AS ro\n  FROM repositories rep\n  INNER JOIN funders f\n    ON rep.id = f.pub_id\n  INNER JOIN orgs o\n    ON rep.id = o.pub_id\n  )\n  SELECT\n  kw,\n  COUNT(DISTINCT funder) AS funders,\n  COUNT(DISTINCT ro) AS orgs,\n  COUNT(DISTINCT id) AS pubs\n  FROM combined\n  GROUP BY 1\n  ORDER BY pubs DESC\n  LIMIT 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> <pre><code>#see what we've got\nrepo_df\n</code></pre> kw funders orgs pubs 0 ncbi 1858 7378 16104 1 github 1680 6787 13160 2 geo 1456 5115 8802 3 zenodo 1175 4828 6757 4 gene 1429 5311 6441 5 bioproject 1136 4369 5925 6 sra 1172 4292 5817 7 genbank 1025 4123 5019 8 figshare 1164 4422 4683 9 impact 1176 4649 3659 <pre><code>#plot\nplot = px.scatter(\n    repo_df,\n    x = \"funders\",\n    y = \"orgs\",\n    hover_data = [\"kw\"]\n)\n\nplot.show()\n</code></pre>"},{"location":"tutorials/09-orcid/","title":"ORCiD example queries","text":"<p>The following examples explore how to use the openly available bigquery dataset available at: <code>ds-open-datasets.orcid.summaries_2024</code></p> <p>Further documentation on the orcid schema, along with how to get connected to bigquery can be found at:</p> <p>https://docs.dimensions.ai/bigquery/</p> <p>Prerequisites</p> <p>In order to run this tutorial, please ensure that you have</p> <ul> <li>Configured a Google Cloud project.</li> <li>Basic familiarity with Python and Jupyter notebooks.</li> </ul> <p>(This tutorial is based on a Jupyter notebook that is available directly via GitHub.)</p> <pre><code>from google.colab import auth\nauth.authenticate_user()\nprint('Authenticated')\n</code></pre> <pre><code>Authenticated\n</code></pre> <pre><code>from google.cloud import bigquery\n\nfrom google.cloud.bigquery import magics\n\nproject_id = input(\"Enter the name of a GBQ project to use when running the code in this notebook: \")\n\nmagics.context.project = project_id\n\nbq_params = {}\n\nclient = bigquery.Client(project=project_id)\n\n%load_ext bigquery_magics\n</code></pre> <pre><code>Enter the name of a GBQ project to use when running the code in this notebook: ds-ripeta-gbq\n</code></pre> <p>Before we go further, a quick warning.  In bigquery don't use \"select *\" to explore a dataset. It will be expensive. Use only the columns that you need</p>"},{"location":"tutorials/09-orcid/#exploring-the-orcid-dataset-on-google-bigquery","title":"Exploring the ORCiD dataset on Google Bigquery","text":"<p>In this notebook, we breakdown the orcid schema in bigquery, and demonstrate how to query each section</p> <ul> <li> <p>counting orcid identifiers</p> </li> <li> <p>querying person data</p> </li> <li>biography details</li> <li>emails</li> <li>addresses</li> <li>external identifiers</li> <li>activities</li> <li>education</li> <li>employments</li> <li>funding</li> <li>peer reviews</li> <li>works</li> <li>invited positions</li> <li>memberships</li> <li>qualifications</li> <li>services</li> <li>research resources</li> </ul>"},{"location":"tutorials/09-orcid/#querying-orcid-identifiers","title":"Querying orcid identifiers","text":"<p>How many active orcids do we have?</p> <pre><code>%%bigquery\n\nselect count(orcid_identifier)\nfrom ds-open-datasets.orcid.summaries_2024\n   where history.deactivation_date is null\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 21046010 <p>Example: How many orcid records have a publicly verified email?</p> <pre><code>%%bigquery\n\nselect count(orcid_identifier)\nfrom ds-open-datasets.orcid.summaries_2024\n   where history.deactivation_date is null\n   and history.verified_email is True\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 16623231 <p>How many orcids have been created by year?</p> <pre><code>%%bigquery\n\nselect\n  extract(YEAR FROM timestamp(history.submission_date)) AS year,\n  count(orcid_identifier)\nfrom ds-open-datasets.orcid.summaries_2024\n   where history.deactivation_date is null\n   group by 1 -- i.e. year\n   order by 1\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> year f0_ 0 2012 43695 1 2013 420762 2 2014 584072 3 2015 731309 4 2016 1021238 5 2017 1343595 6 2018 1546783 7 2019 1959241 8 2020 2598695 9 2021 2659905 10 2022 2748914 11 2023 2975162 12 2024 2412639 <p>How many orcids have been modified in 2023 (2024 is only up to the date of the snapshot) ?</p> <pre><code>%%bigquery\n\nselect  count(orcid_identifier)\nfrom ds-open-datasets.orcid.summaries_2024\n   where history.deactivation_date is null\n   and extract(YEAR FROM timestamp(history.last_modified_date)) = 2023\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 7471503"},{"location":"tutorials/09-orcid/#person","title":"person","text":"<p>How many orcid names are public?</p> <pre><code>%%bigquery\n\nselect\n  count(orcid_identifier)\nfrom ds-open-datasets.orcid.summaries_2024\nwhere person.name.visibility = 'public'\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 21023410 <p>How many orcid records have other names?</p> <pre><code>%%bigquery\n\n select count(orcid_identifier)\n from ds-open-datasets.orcid.summaries_2024\n where person.other_names is not null\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 742479 <p>How many orcids have a populated credit name?</p> <pre><code>%%bigquery\n\n select count(orcid_identifier)\n from\n ds-open-datasets.orcid.summaries_2024\n where person.name.credit_name is not null\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 811105"},{"location":"tutorials/09-orcid/#personbiography","title":"person.biography","text":"<p>Example: What are the dominant research url names?</p> <pre><code>%%bigquery\n\nselect\n  substr(url.url,1,25) url_beginning,\n  count(orcid_identifier) as orcid_count\nfrom ds-open-datasets.orcid.summaries_2024\n  cross join unnest(person.researcher_urls.urls) as url\ngroup by 1 -- url beginining\norder by 2 desc -- count of orcids\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> url_beginning orcid_count 0 https://www.linkedin.com/ 238827 1 https://www.researchgate. 150454 2 https://scholar.google.co 125628 3 https://www.facebook.com/ 40467 4 http://www.linkedin.com/i 35762 5 https://www.instagram.com 24594 6 https://sites.google.com/ 23693 7 https://publons.com/resea 13536 8 https://www.webofscience. 12911 9 https://scholar.google.es 10752 <p>Example: How many profiles have biographies?</p> <pre><code>%%bigquery\n\nselect count(orcid_identifier)\n from ds-open-datasets.orcid.summaries_2024\nwhere person.biography.content is not null\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 917857 <p>What are some of the most used words in biographies (other than stop words)?</p> <pre><code>%%bigquery\n\nwith\n  bio_tokens as (\n      select\n        orcid_identifier.path orcid,\n        split(person.biography.content,' ') as tokens\n      from ds-open-datasets.orcid.summaries_2024\n      where person.biography.content is not null\n    ),\n  tf_idf as (\n    SELECT\n      orcid,\n      TF_IDF(tokens, 10000, 20) OVER() AS results\n    FROM bio_tokens\n    ORDER BY orcid\n  )\n\nselect\n  token.index,\n  count(orcid) AS profiles\nfrom tf_idf\n  cross join unnest(results) as token\nwhere token.index is not null\n  and LENGTH(token.index) &gt; 2\n  and token.value &gt; .7\ngroup by 1 -- the word\norder by 2 desc -- number of profiles including the word\nlimit 200\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> index profiles 0 Estudiante 1149 1 Student 1087 2 Medical 886 3 Researcher 880 4 student 860 ... ... ... 195 Graduate 111 196 Cesar 110 197 Social 110 198 Applied 110 199 Finance 109 <p>200 rows \u00d7 2 columns</p>"},{"location":"tutorials/09-orcid/#personemails","title":"person.emails","text":"<p>How many public emails are available in ORCiD?</p> <pre><code>%%bigquery\n\nselect count(distinct email.email)\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(person.emails.emails) as email\nWHERE email.visibility = 'public'\n  AND email IS NOT NULL\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 729428"},{"location":"tutorials/09-orcid/#personaddresses","title":"person.addresses","text":"<p>The <code>person.addresses</code> field records the countries a person has worked in. How many people give publicly visible countries/regions in ORCiD?</p> <pre><code>%%bigquery\n\nselect count(distinct address.source.source_orcid.path)\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(person.addresses.addresses) as address\nWHERE address.visibility = 'public'\n  AND address IS NOT NULL\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ 0 2552260 <p>Which countries?</p> <pre><code>%%bigquery\n\nselect\n  address.country,\n  count(distinct address.source.source_orcid.path)\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(person.addresses.addresses) as address\nwhere address.visibility = 'public'\n  and address is not null\ngroup by address.country\norder by 2 DESC -- count of orcids\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> country f0_ 0 BR 272986 1 CN 231405 2 US 209177 3 IN 171638 4 ES 100541 ... ... ... 245 NU 6 246 CC 6 247 BV 5 248 TK 4 249 PM 4 <p>250 rows \u00d7 2 columns</p>"},{"location":"tutorials/09-orcid/#personkeywords","title":"person.keywords","text":"<p>What are the most frequent keywords in the ORCID dataset?</p> <pre><code>## Most frequent keywords?\n\n%%bigquery\n\nselect lower(keyword.content), count(orcid_identifier.path)\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(person.keywords.keywords) keyword\ngroup by lower(keyword.content)\norder by 2 desc -- i.e. count\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ f1_ 0 machine learning 19257 1 artificial intelligence 10382 2 deep learning 7789 3 bioinformatics 7404 4 education 6359 ... ... ... 1189703 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e-\u043f\u0441\u0438\u0445\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0432\u0438\u043a\u0442\u0438\u043c\u043e\u043b\u043e\u0433\u0438\u044f 1 1189704 analog circuits,oscillators, pll, adc, dac, lo... 1 1189705 high education, internationalization of educat... 1 1189706 structural mechanics, structural health monito... 1 1189707 ritual cognition 1 <p>1189708 rows \u00d7 2 columns</p>"},{"location":"tutorials/09-orcid/#personexternal_identifiers","title":"person.external_identifiers","text":"<p>What are the most common identifier types in ORCiD?</p> <pre><code>%%bigquery\n\nselect\n  lower(identifier.type),\n  count(orcid_identifier.path)\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(person.external_identifiers.identifiers) identifier\ngroup by 1 -- i.e. identifier type\norder by 2 desc -- i.e. count\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> f0_ f1_ 0 scopus author id 1791459 1 researcherid 700225 2 sciprofiles 325959 3 loop profile 261417 4 ci\u00eancia id 67820 5 researcher name resolver id 14228 6 gnd 9692 7 \u4e2d\u56fd\u79d1\u5b66\u5bb6\u5728\u7ebf 5457 8 isni 4227 9 pitt id 3400"},{"location":"tutorials/09-orcid/#activities","title":"activities","text":""},{"location":"tutorials/09-orcid/#activitieseducations","title":"activities.educations","text":"<p>What are the most common academic roles in ORCiD?</p> <pre><code>%%bigquery\n\nselect\n  record.role_title,\n  count(orcid_identifier.path) as orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(activities.educations.groups) grp,\n   unnest(grp.records) record\n   where start_date.year = \"2024\"\ngroup by record.role_title\norder by orcids desc\nlimit 40\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> role_title orcids 0 PhD 2118 1 None 1768 2 Mestrado 753 3 Doctor of Philosophy 666 4 Doutorado 583 5 Ph.D. 510 6 PhD Student 372 7 Master 317 8 PhD student 293 9 Master of Science 256 10 Ph.D 222 11 Mestranda 216 12 Completed the course and project 168 13 Mestrando 149 14 Doutoranda 145 15 Gradua\u00e7\u00e3o 137 16 PhD Candidate 136 17 Doctor 126 18 Doutorando 119 19 Mestre 112 20 Phd 97 21 Estudiante 94 22 PhD candidate 93 23 MD 91 24 Doctorate 89 25 Masters 84 26 MSc 78 27 Especializa\u00e7\u00e3o 75 28 Master's Degree 74 29 Doctor of Philosophy (PhD) 69 30 Doctor of Medicine 68 31 Ph.D. Student 63 32 Master's degree 58 33 ESTUDIANTE 58 34 MS 56 35 PHD 55 36 Doctoral Student 51 37 Bacharelado 49 38 Master's 48 39 Graduate Student 47"},{"location":"tutorials/09-orcid/#activitiesemployments","title":"activities.employments","text":"<p>How many employments have disambiguated addresses in 2024?</p> <pre><code>#How many employments have disambiguated addresses in 2024?\n\n%%bigquery\n\nselect organization.disambiguated_organization.source, count(orcid_identifier.path)\nfrom ds-open-datasets.orcid.summaries_2024,\n   unnest(activities.employments.groups) grp,\n   unnest(grp.records) record\n   where start_date.year = \"2024\"\ngroup by 1\norder by 2 desc\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> source f0_ 0 ROR 228862 1 None 30104 2 RINGGOLD 2865 3 FUNDREF 1904 4 GRID 576"},{"location":"tutorials/09-orcid/#activitiesfundings","title":"activities.fundings","text":"<p>What organizations have been tagged the most times in ORCID as having awarded funding?</p> <pre><code>%%bigquery\n\nselect\n  funding_record.organization.name,\n  count(*) AS mentioned_count\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.fundings.groups) as funding_groups,\n  unnest(funding_groups.records) as funding_record\nwhere funding_record.organization.name &lt;&gt; 'N/A'\ngroup by funding_record.organization.name\norder by mentioned_count desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> name mentioned_count 0 Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia 75382 1 European Commission 47984 2 Japan Society for the Promotion of Science 43370 3 National Natural Science Foundation of China 20965 4 Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia, I.P. 17160 5 Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N... 15851 6 Conselho Nacional de Desenvolvimento Cient\u00edfic... 15311 7 Canadian Institutes of Health Research 15123 8 Australian Research Council 14738 9 Swiss National Science Foundation 14689"},{"location":"tutorials/09-orcid/#activitiespeer_reviews","title":"activities.peer_reviews","text":"<p>What organizations make the most use of peer reviewers?</p> <pre><code>%%bigquery\n\nselect\n  record.reviewer_role,\n  record.review_type,\n  record.convening_organization.name,\n  count(orcid_identifier.path) AS orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n    unnest(activities.peer_reviews.groups) as grp,\n    unnest(grp.groups) as grpp,\n    unnest(grpp.records) as record\ngroup by record.reviewer_role, record.review_type, record.convening_organization.name\norder by orcids desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> reviewer_role review_type name orcids 0 reviewer review Clarivate PLC 3232053 1 reviewer review Publons 3117297 2 reviewer review Elsevier, Inc. 2660331 3 reviewer review Springer Nature 2261495 4 reviewer review American Chemical Society 1382847 5 reviewer review MDPI 694117 6 reviewer review SpringerNature 363646 7 reviewer review Wiley-VCH 300707 8 reviewer review Public Library of Science 166378 9 reviewer review BMJ Publishing Group 83693"},{"location":"tutorials/09-orcid/#activitiesworks","title":"activities.works","text":"<p>What are the most used identifier types for works claimed in ORCID?</p> <pre><code>%%bigquery\n\nselect\n  identifier.type,\n  count(identifier.value) AS instances\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.works.groups) as grp,\n  unnest(grp.external_ids.identifiers) as identifier\ngroup by identifier.type\norder by instances desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> type instances 0 doi 77820619 1 eid 40109303 2 wosuid 13082114 3 source-work-id 10850721 4 pmid 8329151 5 other-id 2997732 6 pmc 2545463 7 arxiv 2107965 8 handle 1552937 9 isbn 1054806"},{"location":"tutorials/09-orcid/#activitiesinvited_postition","title":"activities.invited_postition","text":"<p>What are the most common invited positions held by researchers with ORCIDs?</p> <pre><code>%%bigquery\n\nselect\n  record.role_title,\n  count(orcid_identifier.path) as orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.invited_positions.records) as rec,\n  unnest(rec.records) as record\nwhere record.role_title is not null\ngroup by record.role_title\norder by orcids desc\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> role_title orcids 0 Visiting Professor 8391 1 Visiting Scholar 4841 2 Member 3984 3 Visiting Researcher 3603 4 Reviewer 3197 ... ... ... 160075 Internal Medicine Chief Resident 1 160076 Leuven Institute for Advanced Studies Fellow 1 160077 Profesora invitada de Educaci\u00f3n Superior de Po... 1 160078 Director, Depression and Anxiety Disorders Pro... 1 160079 Representance de los centros de investigacion ... 1 <p>160080 rows \u00d7 2 columns</p>"},{"location":"tutorials/09-orcid/#activitiesmemberships","title":"activities.memberships","text":"<p>What are the most common membership positions held by researchers with ORCIDs?</p> <pre><code>%%bigquery\n\nselect\n  record.role_title,\n  count(orcid_identifier.path) as orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.memberships.groups) as grp,\n  unnest(grp.records) as record\nwhere record.role_title is not null\ngroup by record.role_title\norder by orcids desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> role_title orcids 0 Member 78249 1 Fellow 14622 2 Life Member 14297 3 member 11710 4 Student 10547 5 Student Member 6575 6 Miembro 5651 7 Associate Member 5036 8 Senior Member 4781 9 Life member 4575"},{"location":"tutorials/09-orcid/#activitiesqualifications","title":"activities.qualifications","text":"<p>What are the most common types of qualification recorded by researchers with ORCIDs?</p> <pre><code>%%bigquery\n\nselect\n  record.role_title,\n  count(orcid_identifier.path) as orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.qualifications.groups) as grp,\n  unnest(grp.records) as record\nwhere record.role_title is not null\n  and record.role_title &lt;&gt; 'Student'\ngroup by record.role_title\norder by orcids desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> role_title orcids 0 PhD 44330 1 Investigador RENACYT 10225 2 Ph.D. 8451 3 Mestrado 7355 4 Ph.D 6843 5 Master 5351 6 MSc 5329 7 Doutorado 5197 8 Mestre 5147 9 PhD student 5109"},{"location":"tutorials/09-orcid/#activitiesservices","title":"activities.services","text":"<p>What types of services do researchers with ORCIDs commonly say they have performed?</p> <pre><code>%%bigquery\n\nselect\n  record.role_title,\n  count(orcid_identifier.path) as orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.services.groups) as grp,\n  unnest(grp.records) as record\nwhere record.role_title is not null\ngroup by record.role_title\norder by orcids desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> role_title orcids 0 Member 11290 1 Reviewer 5011 2 President 4567 3 Associate Editor 2865 4 Chair 2463 5 Board Member 1897 6 Editorial Board Member 1882 7 Volunteer 1835 8 Director 1711 9 member 1653"},{"location":"tutorials/09-orcid/#activitiesresearch_resources","title":"activities.research_resources","text":"<p>What proposals for the use of specialist research facilities have involved the most researchers?</p> <pre><code>%%bigquery\n\nselect\n  record.proposal.title.title,\n  count(orcid_identifier.path) as orcids\nfrom ds-open-datasets.orcid.summaries_2024,\n  unnest(activities.research_resources.groups) as grp,\n  unnest(grp.records) as record\ngroup by record.proposal.title.title\norder by orcids desc\nlimit 10\n</code></pre> <pre><code>Query is running:   0%|          |\n\n\n\nDownloading:   0%|          |\n</code></pre> title orcids 0 Neutron Beam Award at Spallation Neutron Sourc... 858 1 Neutron Beam Award at High Flux Isotope Reacto... 338 2 Neutron Beam Award at High Flux Isotope Reacto... 199 3 Prediction of soil microbiome phenotypic respo... 16 4 Unraveling Redox Transformation Mechanisms of ... 13 5 The Transformation of Tetrahedral to Octahedra... 12 6 EVALUATION OF CEMENT COMPOSITES, ROCKS, AND OT... 10 7 Controls of bioorganic constituents of soils i... 7 8 In situ TEM study of hierarchical nanowires gr... 7 9 The effect of trace element content on the rat... 7"}]}