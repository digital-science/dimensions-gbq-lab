{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Dimensions BigQuery Lab! This site and its related Github repository contain a collection of tutorials and queries showing how to carry out common research data analytics tasks using the Dimensions on Google BigQuery database. Note The Dimensions on Google BigQuery product is subscription-only , so your Dimensions account needs to be activated for this service and is subject to restrictions on use. Please get in touch to discuss the best option for you to get access. For one-off academic research projects we can also grant free access to our APIs. Contact us How to use this Lab The learning materials in this Lab can be navigated sequentially, or, using the search box above, it is possible to enter a keyword and see matching documents. There are three main sections: The Tutorials section contain guides that focuses on specific topics or use cases e.g. how to deal with a specific data type, or how to use GBQ in combination with other technologies. The Collections section contains thematic groupings of queries based on application scenarios. The Queries section is an archive of reusable SQL queries together with an explanation of what they do. Tip You can try out Dimensions on Google BigQuery using the sandbox environment . This contains all Dimensions data related to COVID-19 and it is updated on a daily basis. Finding out more Find out more about Dimensions on BigQuery with the following resources: The Dimensions BigQuery homepage is the place to start from if you\u2019ve never heard about Dimensions on GBQ. The Dimensions on BigQuery official documentation contains detailed data model information, tutorials on how to sign up and how to access it using tools like Data Studio and Tableau. The GBQ Lab Github repository contains the source code for all the materials on this website, and more. Video tutorials The Dimensions YouTube channel contains various how-to videos like the one below: Changelog See the CHANGELOG.md file on Github.","title":"Home"},{"location":"#welcome-to-the-dimensions-bigquery-lab","text":"This site and its related Github repository contain a collection of tutorials and queries showing how to carry out common research data analytics tasks using the Dimensions on Google BigQuery database. Note The Dimensions on Google BigQuery product is subscription-only , so your Dimensions account needs to be activated for this service and is subject to restrictions on use. Please get in touch to discuss the best option for you to get access. For one-off academic research projects we can also grant free access to our APIs. Contact us","title":"Welcome to the Dimensions BigQuery Lab!"},{"location":"#how-to-use-this-lab","text":"The learning materials in this Lab can be navigated sequentially, or, using the search box above, it is possible to enter a keyword and see matching documents. There are three main sections: The Tutorials section contain guides that focuses on specific topics or use cases e.g. how to deal with a specific data type, or how to use GBQ in combination with other technologies. The Collections section contains thematic groupings of queries based on application scenarios. The Queries section is an archive of reusable SQL queries together with an explanation of what they do. Tip You can try out Dimensions on Google BigQuery using the sandbox environment . This contains all Dimensions data related to COVID-19 and it is updated on a daily basis.","title":"How to use this Lab"},{"location":"#finding-out-more","text":"Find out more about Dimensions on BigQuery with the following resources: The Dimensions BigQuery homepage is the place to start from if you\u2019ve never heard about Dimensions on GBQ. The Dimensions on BigQuery official documentation contains detailed data model information, tutorials on how to sign up and how to access it using tools like Data Studio and Tableau. The GBQ Lab Github repository contains the source code for all the materials on this website, and more.","title":"Finding out more"},{"location":"#video-tutorials","text":"The Dimensions YouTube channel contains various how-to videos like the one below:","title":"Video tutorials"},{"location":"#changelog","text":"See the CHANGELOG.md file on Github.","title":"Changelog"},{"location":"collections/","text":"About Collections The Collection section contains thematic groupings of queries based on common application scenarios. The source code of the queries and further explanations are available in the Queries section. Note New queries and collections are added frequently, so watch this space or keep an eye on the CHANGELOG file on Github!","title":"About Collections"},{"location":"collections/#about-collections","text":"The Collection section contains thematic groupings of queries based on common application scenarios. The source code of the queries and further explanations are available in the Queries section. Note New queries and collections are added frequently, so watch this space or keep an eye on the CHANGELOG file on Github!","title":"About Collections"},{"location":"collections/01-publications/","text":"Publications Number of publications added to Dimensions each month Number of Publications by Type Generate a list of publication categories by flattening/concatenating nested data Number of publications per SDG category Publications count per FoR category, total and percentage against total Finding articles matching a specific affiliation string Top publications by Altmetric score and research organization Select publications matching selected concepts Extracting complex publications records Finding Journals using string matching International collaboration of an organisation in a field International collaboration of a researcher with org and country context","title":"Publications"},{"location":"collections/01-publications/#publications","text":"Number of publications added to Dimensions each month Number of Publications by Type Generate a list of publication categories by flattening/concatenating nested data Number of publications per SDG category Publications count per FoR category, total and percentage against total Finding articles matching a specific affiliation string Top publications by Altmetric score and research organization Select publications matching selected concepts Extracting complex publications records Finding Journals using string matching International collaboration of an organisation in a field International collaboration of a researcher with org and country context","title":"Publications"},{"location":"collections/02-citations/","text":"Citations Top N publications by citations percentile One-degree citation network for a single publication Incoming citations by year Incoming citations by journal Citations by journal, for a specific publisher Outgoing citations from a journal Citing authors by country","title":"Citations"},{"location":"collections/02-citations/#citations","text":"Top N publications by citations percentile One-degree citation network for a single publication Incoming citations by year Incoming citations by journal Citations by journal, for a specific publisher Outgoing citations from a journal Citing authors by country","title":"Citations"},{"location":"collections/03-authors/","text":"Authors Generate a list of publication authors by flattening/concatenating nested data Count of corresponding authors by publisher Counting new vs recurring authors, for a specific journal International collaboration rate of individuals, with context","title":"Authors"},{"location":"collections/03-authors/#authors","text":"Generate a list of publication authors by flattening/concatenating nested data Count of corresponding authors by publisher Counting new vs recurring authors, for a specific journal International collaboration rate of individuals, with context","title":"Authors"},{"location":"collections/04-funding/","text":"Funding & Grants Grants of an organization Funding by journal","title":"Funding & Grants"},{"location":"collections/04-funding/#funding-grants","text":"Grants of an organization Funding by journal","title":"Funding &amp; Grants"},{"location":"queries/","text":"About Queries The Queries section is an archive of reusable SQL queries together with an explanation of how they work. You can navigate these contents sequentially, or simply use the search box at the top to find items of interest. Alternatively, you can find thematic groups of queries in the Collections section. Note The Google BigQuery official documentation website provides in-depth tutorials about SQL syntax.","title":"About Queries"},{"location":"queries/#about-queries","text":"The Queries section is an archive of reusable SQL queries together with an explanation of how they work. You can navigate these contents sequentially, or simply use the search box at the top to find items of interest. Alternatively, you can find thematic groups of queries in the Collections section. Note The Google BigQuery official documentation website provides in-depth tutorials about SQL syntax.","title":"About Queries"},{"location":"queries/01/","text":"1. Number of publications added to Dimensions each month Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description Counts the number of publications added to Dimensions each month, using the date_inserted field. Query SELECT DATE_TRUNC ( date_inserted , MONTH ) as date , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY date ORDER BY date DESC LIMIT 5 Breaking it down The DATETIME_TRUNC function is used here to \"round down\" the timestamps in the date_inserted field to the month level. Results [ { \"date\" : \"2021-04-01 00:00:00 UTC\" , \"countDim\" : \"458175\" }, { \"date\" : \"2021-03-01 00:00:00 UTC\" , \"countDim\" : \"746884\" }, { \"date\" : \"2021-02-01 00:00:00 UTC\" , \"countDim\" : \"661512\" }, { \"date\" : \"2021-01-01 00:00:00 UTC\" , \"countDim\" : \"687725\" }, { \"date\" : \"2020-12-01 00:00:00 UTC\" , \"countDim\" : \"828301\" } ]","title":"1. Number of publications added to Dimensions each month"},{"location":"queries/01/#1-number-of-publications-added-to-dimensions-each-month","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"1. Number of publications added to Dimensions each month"},{"location":"queries/01/#description","text":"Counts the number of publications added to Dimensions each month, using the date_inserted field.","title":"Description"},{"location":"queries/01/#query","text":"SELECT DATE_TRUNC ( date_inserted , MONTH ) as date , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY date ORDER BY date DESC LIMIT 5","title":"Query"},{"location":"queries/01/#breaking-it-down","text":"The DATETIME_TRUNC function is used here to \"round down\" the timestamps in the date_inserted field to the month level.","title":"Breaking it down"},{"location":"queries/01/#results","text":"[ { \"date\" : \"2021-04-01 00:00:00 UTC\" , \"countDim\" : \"458175\" }, { \"date\" : \"2021-03-01 00:00:00 UTC\" , \"countDim\" : \"746884\" }, { \"date\" : \"2021-02-01 00:00:00 UTC\" , \"countDim\" : \"661512\" }, { \"date\" : \"2021-01-01 00:00:00 UTC\" , \"countDim\" : \"687725\" }, { \"date\" : \"2020-12-01 00:00:00 UTC\" , \"countDim\" : \"828301\" } ]","title":"Results"},{"location":"queries/02/","text":"2. Number of Publications by Type Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description Each publication is classified into one of several \"types.\" This query counts how many have been classified as each type. Query SELECT COUNT ( id ) AS tot_articles , type FROM ` dimensions - ai . data_analytics . publications ` GROUP BY type ORDER BY tot_articles DESC Results [ { \"tot_articles\" : \"96627450\" , \"type\" : \"article\" }, { \"tot_articles\" : \"10781485\" , \"type\" : \"chapter\" }, { \"tot_articles\" : \"6527269\" , \"type\" : \"proceeding\" }, { \"tot_articles\" : \"2648537\" , \"type\" : \"preprint\" }, { \"tot_articles\" : \"795713\" , \"type\" : \"monograph\" }, { \"tot_articles\" : \"525722\" , \"type\" : \"book\" } ]","title":"2. Number of Publications by Type"},{"location":"queries/02/#2-number-of-publications-by-type","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"2. Number of Publications by Type"},{"location":"queries/02/#description","text":"Each publication is classified into one of several \"types.\" This query counts how many have been classified as each type.","title":"Description"},{"location":"queries/02/#query","text":"SELECT COUNT ( id ) AS tot_articles , type FROM ` dimensions - ai . data_analytics . publications ` GROUP BY type ORDER BY tot_articles DESC","title":"Query"},{"location":"queries/02/#results","text":"[ { \"tot_articles\" : \"96627450\" , \"type\" : \"article\" }, { \"tot_articles\" : \"10781485\" , \"type\" : \"chapter\" }, { \"tot_articles\" : \"6527269\" , \"type\" : \"proceeding\" }, { \"tot_articles\" : \"2648537\" , \"type\" : \"preprint\" }, { \"tot_articles\" : \"795713\" , \"type\" : \"monograph\" }, { \"tot_articles\" : \"525722\" , \"type\" : \"book\" } ]","title":"Results"},{"location":"queries/03/","text":"3. Generate a list of publication authors by flattening/concatenating nested data Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query returns a table with a single row. One field contains the Dimensions publication ID, and the other contains a string of all author names associated with the paper, separated by semicolons. For more details about working with nested fields, see the tutorial page on the topic . Example 3 in the tutorial deals with this query specifically. Query WITH author_array AS ( SELECT id , ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS author_names FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' ) SELECT id , ARRAY_TO_STRING ( author_names , '; ' ) AS authors_list FROM author_array Results [ { \"id\" : \"pub.1132070778\" , \"authors_list\" : \"O Gr\u00e5n\u00e4s; A Mocellin; E S Cardoso; F Burmeister; C Caleman; O Bj\u00f6rneholm; A Naves de Brito\" } ]","title":"3. Generate a list of publication authors by flattening/concatenating nested data"},{"location":"queries/03/#3-generate-a-list-of-publication-authors-by-flatteningconcatenating-nested-data","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"3. Generate a list of publication authors by flattening/concatenating nested data"},{"location":"queries/03/#description","text":"This query returns a table with a single row. One field contains the Dimensions publication ID, and the other contains a string of all author names associated with the paper, separated by semicolons. For more details about working with nested fields, see the tutorial page on the topic . Example 3 in the tutorial deals with this query specifically.","title":"Description"},{"location":"queries/03/#query","text":"WITH author_array AS ( SELECT id , ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS author_names FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' ) SELECT id , ARRAY_TO_STRING ( author_names , '; ' ) AS authors_list FROM author_array","title":"Query"},{"location":"queries/03/#results","text":"[ { \"id\" : \"pub.1132070778\" , \"authors_list\" : \"O Gr\u00e5n\u00e4s; A Mocellin; E S Cardoso; F Burmeister; C Caleman; O Bj\u00f6rneholm; A Naves de Brito\" } ]","title":"Results"},{"location":"queries/04/","text":"4. Generate a list of publication categories by flattening/concatenating nested data Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query returns a table with a single row. One field contains the Dimensions publication ID, and the other contains a string of all categories associated with the publication, separated by semicolons. It's very similar to the publication authors query . You may also be interested in the tutorial about working with nested fields . Query WITH categories AS ( SELECT id , ARRAY ( SELECT name FROM UNNEST ( category_for . first_level . FULL ) ) AS category_names FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' ) SELECT id , ARRAY_TO_STRING ( category_names , '; ' ) AS categories_list FROM categories Results [ { \"id\" : \"pub.1132070778\" , \"categories_list\" : \"Physical Sciences; Chemical Sciences\" } ]","title":"4. Generate a list of publication categories by flattening/concatenating nested data"},{"location":"queries/04/#4-generate-a-list-of-publication-categories-by-flatteningconcatenating-nested-data","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"4. Generate a list of publication categories by flattening/concatenating nested data"},{"location":"queries/04/#description","text":"This query returns a table with a single row. One field contains the Dimensions publication ID, and the other contains a string of all categories associated with the publication, separated by semicolons. It's very similar to the publication authors query . You may also be interested in the tutorial about working with nested fields .","title":"Description"},{"location":"queries/04/#query","text":"WITH categories AS ( SELECT id , ARRAY ( SELECT name FROM UNNEST ( category_for . first_level . FULL ) ) AS category_names FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' ) SELECT id , ARRAY_TO_STRING ( category_names , '; ' ) AS categories_list FROM categories","title":"Query"},{"location":"queries/04/#results","text":"[ { \"id\" : \"pub.1132070778\" , \"categories_list\" : \"Physical Sciences; Chemical Sciences\" } ]","title":"Results"},{"location":"queries/05/","text":"5. Number of publications per SDG category Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description Many publications in the Dimensions database are classified under Sustainable Development Goals (SDGs) . This query returns the top five most commonly applied SDG classifications and the total number of publications in each one. Query SELECT COUNT ( p . id ) AS tot , sdg . name FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( category_sdg . full ) sdg GROUP BY sdg . name ORDER BY tot DESC LIMIT 5 Breaking it down This query is short but wades through a complex data structure. While the author names query has to deal with an array of structs, this one has to parse a struct of arrays : Refer to the schema documentation for the details, but if we want to access the names of the SDG categories relevant to a single publication, we have to drill through multiple layers: The publications table uses a single row to represent a single publication. Publications with an SDG categorization have a struct in the category_sdg field with two keys: codes and full . We can access this by name, so category_sdg.full will give us the relevant entry for a publication. However, category_sdg.full is an array . Each entry in the category_sdg.full array is another struct, with three fields: code , id and name . We want the names. We can get through the first struct by getting the full field by name. From there, we need to do something more complex: CROSS JOIN UNNEST ( category_sdg . full ) sdg This line performs a cross join . The less-technical explanation for what happens here is that, since category_sdg.full is an array, using a cross join with the publications table creates a new field called sdg , and each row of the publications table is repeated, once for each value for sdg . A quick demonstration: WITHOUT UNNEST() : id category_sdg.full pub.123 sdg1, sdg2, sdg3 pub.987 sdg4, sdg1 WITH UNNEST() : id sdg pub.123 sdg1 pub.123 sdg2 pub.123 sdg3 pub.987 sdg4 pub.987 sdg1 So after this clause: CROSS JOIN UNNEST ( category_sdg . full ) sdg we have a new field, sdg , that has the nested fields associated with an individual SDG : code , id and name . Since we want name , that's what we refer to in the final query: SELECT COUNT ( p . id ) AS tot , sdg . name FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( category_sdg . full ) sdg GROUP BY sdg . name ORDER BY tot DESC LIMIT 5 We then count the number of IDs associated with each SDG name by using a GROUP BY clause. Results [ { \"tot\" : \"1577950\" , \"name\" : \"Affordable and Clean Energy\" }, { \"tot\" : \"1455575\" , \"name\" : \"Good Health and Well Being\" }, { \"tot\" : \"769875\" , \"name\" : \"Peace, Justice and Strong Institutions\" }, { \"tot\" : \"633369\" , \"name\" : \"Quality Education\" }, { \"tot\" : \"507003\" , \"name\" : \"Climate Action\" } ]","title":"5. Number of publications per SDG category"},{"location":"queries/05/#5-number-of-publications-per-sdg-category","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"5. Number of publications per SDG category"},{"location":"queries/05/#description","text":"Many publications in the Dimensions database are classified under Sustainable Development Goals (SDGs) . This query returns the top five most commonly applied SDG classifications and the total number of publications in each one.","title":"Description"},{"location":"queries/05/#query","text":"SELECT COUNT ( p . id ) AS tot , sdg . name FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( category_sdg . full ) sdg GROUP BY sdg . name ORDER BY tot DESC LIMIT 5","title":"Query"},{"location":"queries/05/#breaking-it-down","text":"This query is short but wades through a complex data structure. While the author names query has to deal with an array of structs, this one has to parse a struct of arrays : Refer to the schema documentation for the details, but if we want to access the names of the SDG categories relevant to a single publication, we have to drill through multiple layers: The publications table uses a single row to represent a single publication. Publications with an SDG categorization have a struct in the category_sdg field with two keys: codes and full . We can access this by name, so category_sdg.full will give us the relevant entry for a publication. However, category_sdg.full is an array . Each entry in the category_sdg.full array is another struct, with three fields: code , id and name . We want the names. We can get through the first struct by getting the full field by name. From there, we need to do something more complex: CROSS JOIN UNNEST ( category_sdg . full ) sdg This line performs a cross join . The less-technical explanation for what happens here is that, since category_sdg.full is an array, using a cross join with the publications table creates a new field called sdg , and each row of the publications table is repeated, once for each value for sdg . A quick demonstration: WITHOUT UNNEST() : id category_sdg.full pub.123 sdg1, sdg2, sdg3 pub.987 sdg4, sdg1 WITH UNNEST() : id sdg pub.123 sdg1 pub.123 sdg2 pub.123 sdg3 pub.987 sdg4 pub.987 sdg1 So after this clause: CROSS JOIN UNNEST ( category_sdg . full ) sdg we have a new field, sdg , that has the nested fields associated with an individual SDG : code , id and name . Since we want name , that's what we refer to in the final query: SELECT COUNT ( p . id ) AS tot , sdg . name FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( category_sdg . full ) sdg GROUP BY sdg . name ORDER BY tot DESC LIMIT 5 We then count the number of IDs associated with each SDG name by using a GROUP BY clause.","title":"Breaking it down"},{"location":"queries/05/#results","text":"[ { \"tot\" : \"1577950\" , \"name\" : \"Affordable and Clean Energy\" }, { \"tot\" : \"1455575\" , \"name\" : \"Good Health and Well Being\" }, { \"tot\" : \"769875\" , \"name\" : \"Peace, Justice and Strong Institutions\" }, { \"tot\" : \"633369\" , \"name\" : \"Quality Education\" }, { \"tot\" : \"507003\" , \"name\" : \"Climate Action\" } ]","title":"Results"},{"location":"queries/06/","text":"6. Publications count per FoR category, total and percentage against total Level: Advanced This query requires a good understanding of SQL and the Dimensions data model Description Many publications in the Dimensions database are classified under ANZSRC Field of Research codes . This query returns the total number of publications classified under each, plus the percentage of all publications in Dimensions with that classification. Query SELECT cat . name , COUNT ( p . id ) AS pubs_global , ROUND ( ( COUNT ( p . id ) * 100 / ( SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` ) ), 2 ) AS pubs_global_pc FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( category_for . first_level . full ) cat GROUP BY cat . name ORDER BY pubs_global_pc DESC , cat . name Breaking it down This query looks more complicated than it is. The main component could be summarized using this query: SELECT cat . name , COUNT ( p . id ) AS pubs_global FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( category_for . first_level . full ) cat GROUP BY cat . name This query works exactly like the \"publications per SDG\" query , and there's a full breakdown there explaining how we unnest structs full of arrays full of structs. The main take-away is that we extract the names of all the first-level FOR codes, then count the number of publications listed under each one. That leaves only one more SELECT statement: ROUND ( ( COUNT ( p . id ) * 100 / ( SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` ) ), 2 ) AS pubs_global_pc This piece of the query just counts how many total records are in the publications table: SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` And this next piece of the query takes the total number of publications in a single classification and divides it by that total. The * 100 piece converts the decimal into a percentage: COUNT ( p . id ) * 100 / ( SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` ) The outer-most call, to the ROUND() function, states that the function should return the percentage rounded to two decimal places. Results [ { \"name\" : \"Medical and Health Sciences\" , \"pubs_global\" : \"30145537\" , \"pubs_global_pc\" : \"25.49\" }, { \"name\" : \"Engineering\" , \"pubs_global\" : \"12388815\" , \"pubs_global_pc\" : \"10.48\" }, { \"name\" : \"Biological Sciences\" , \"pubs_global\" : \"9009961\" , \"pubs_global_pc\" : \"7.62\" }, { \"name\" : \"Chemical Sciences\" , \"pubs_global\" : \"7876669\" , \"pubs_global_pc\" : \"6.66\" }, { \"name\" : \"Physical Sciences\" , \"pubs_global\" : \"6149858\" , \"pubs_global_pc\" : \"5.2\" }, { \"name\" : \"Information and Computing Sciences\" , \"pubs_global\" : \"5236596\" , \"pubs_global_pc\" : \"4.43\" }, { \"name\" : \"Mathematical Sciences\" , \"pubs_global\" : \"5040655\" , \"pubs_global_pc\" : \"4.26\" }, { \"name\" : \"Psychology and Cognitive Sciences\" , \"pubs_global\" : \"3871992\" , \"pubs_global_pc\" : \"3.27\" }, { \"name\" : \"Studies in Human Society\" , \"pubs_global\" : \"3414299\" , \"pubs_global_pc\" : \"2.89\" }, { \"name\" : \"Language, Communication and Culture\" , \"pubs_global\" : \"2531296\" , \"pubs_global_pc\" : \"2.14\" }, { \"name\" : \"History and Archaeology\" , \"pubs_global\" : \"2357976\" , \"pubs_global_pc\" : \"1.99\" }, { \"name\" : \"Agricultural and Veterinary Sciences\" , \"pubs_global\" : \"2108659\" , \"pubs_global_pc\" : \"1.78\" }, { \"name\" : \"Earth Sciences\" , \"pubs_global\" : \"2059783\" , \"pubs_global_pc\" : \"1.74\" }, { \"name\" : \"Technology\" , \"pubs_global\" : \"1956256\" , \"pubs_global_pc\" : \"1.65\" }, { \"name\" : \"Commerce, Management, Tourism and Services\" , \"pubs_global\" : \"1830245\" , \"pubs_global_pc\" : \"1.55\" }, { \"name\" : \"Education\" , \"pubs_global\" : \"1838328\" , \"pubs_global_pc\" : \"1.55\" }, { \"name\" : \"Economics\" , \"pubs_global\" : \"1751713\" , \"pubs_global_pc\" : \"1.48\" }, { \"name\" : \"Philosophy and Religious Studies\" , \"pubs_global\" : \"1680088\" , \"pubs_global_pc\" : \"1.42\" }, { \"name\" : \"Environmental Sciences\" , \"pubs_global\" : \"1375226\" , \"pubs_global_pc\" : \"1.16\" }, { \"name\" : \"Law and Legal Studies\" , \"pubs_global\" : \"902366\" , \"pubs_global_pc\" : \"0.76\" }, { \"name\" : \"Studies in Creative Arts and Writing\" , \"pubs_global\" : \"644962\" , \"pubs_global_pc\" : \"0.55\" }, { \"name\" : \"Built Environment and Design\" , \"pubs_global\" : \"491404\" , \"pubs_global_pc\" : \"0.42\" } ]","title":"6. Publications count per FoR category, total and percentage against total"},{"location":"queries/06/#6-publications-count-per-for-category-total-and-percentage-against-total","text":"Level: Advanced This query requires a good understanding of SQL and the Dimensions data model","title":"6. Publications count per FoR category, total and percentage against total"},{"location":"queries/06/#description","text":"Many publications in the Dimensions database are classified under ANZSRC Field of Research codes . This query returns the total number of publications classified under each, plus the percentage of all publications in Dimensions with that classification.","title":"Description"},{"location":"queries/06/#query","text":"SELECT cat . name , COUNT ( p . id ) AS pubs_global , ROUND ( ( COUNT ( p . id ) * 100 / ( SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` ) ), 2 ) AS pubs_global_pc FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( category_for . first_level . full ) cat GROUP BY cat . name ORDER BY pubs_global_pc DESC , cat . name","title":"Query"},{"location":"queries/06/#breaking-it-down","text":"This query looks more complicated than it is. The main component could be summarized using this query: SELECT cat . name , COUNT ( p . id ) AS pubs_global FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( category_for . first_level . full ) cat GROUP BY cat . name This query works exactly like the \"publications per SDG\" query , and there's a full breakdown there explaining how we unnest structs full of arrays full of structs. The main take-away is that we extract the names of all the first-level FOR codes, then count the number of publications listed under each one. That leaves only one more SELECT statement: ROUND ( ( COUNT ( p . id ) * 100 / ( SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` ) ), 2 ) AS pubs_global_pc This piece of the query just counts how many total records are in the publications table: SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` And this next piece of the query takes the total number of publications in a single classification and divides it by that total. The * 100 piece converts the decimal into a percentage: COUNT ( p . id ) * 100 / ( SELECT COUNT ( * ) FROM ` dimensions - ai . data_analytics . publications ` ) The outer-most call, to the ROUND() function, states that the function should return the percentage rounded to two decimal places.","title":"Breaking it down"},{"location":"queries/06/#results","text":"[ { \"name\" : \"Medical and Health Sciences\" , \"pubs_global\" : \"30145537\" , \"pubs_global_pc\" : \"25.49\" }, { \"name\" : \"Engineering\" , \"pubs_global\" : \"12388815\" , \"pubs_global_pc\" : \"10.48\" }, { \"name\" : \"Biological Sciences\" , \"pubs_global\" : \"9009961\" , \"pubs_global_pc\" : \"7.62\" }, { \"name\" : \"Chemical Sciences\" , \"pubs_global\" : \"7876669\" , \"pubs_global_pc\" : \"6.66\" }, { \"name\" : \"Physical Sciences\" , \"pubs_global\" : \"6149858\" , \"pubs_global_pc\" : \"5.2\" }, { \"name\" : \"Information and Computing Sciences\" , \"pubs_global\" : \"5236596\" , \"pubs_global_pc\" : \"4.43\" }, { \"name\" : \"Mathematical Sciences\" , \"pubs_global\" : \"5040655\" , \"pubs_global_pc\" : \"4.26\" }, { \"name\" : \"Psychology and Cognitive Sciences\" , \"pubs_global\" : \"3871992\" , \"pubs_global_pc\" : \"3.27\" }, { \"name\" : \"Studies in Human Society\" , \"pubs_global\" : \"3414299\" , \"pubs_global_pc\" : \"2.89\" }, { \"name\" : \"Language, Communication and Culture\" , \"pubs_global\" : \"2531296\" , \"pubs_global_pc\" : \"2.14\" }, { \"name\" : \"History and Archaeology\" , \"pubs_global\" : \"2357976\" , \"pubs_global_pc\" : \"1.99\" }, { \"name\" : \"Agricultural and Veterinary Sciences\" , \"pubs_global\" : \"2108659\" , \"pubs_global_pc\" : \"1.78\" }, { \"name\" : \"Earth Sciences\" , \"pubs_global\" : \"2059783\" , \"pubs_global_pc\" : \"1.74\" }, { \"name\" : \"Technology\" , \"pubs_global\" : \"1956256\" , \"pubs_global_pc\" : \"1.65\" }, { \"name\" : \"Commerce, Management, Tourism and Services\" , \"pubs_global\" : \"1830245\" , \"pubs_global_pc\" : \"1.55\" }, { \"name\" : \"Education\" , \"pubs_global\" : \"1838328\" , \"pubs_global_pc\" : \"1.55\" }, { \"name\" : \"Economics\" , \"pubs_global\" : \"1751713\" , \"pubs_global_pc\" : \"1.48\" }, { \"name\" : \"Philosophy and Religious Studies\" , \"pubs_global\" : \"1680088\" , \"pubs_global_pc\" : \"1.42\" }, { \"name\" : \"Environmental Sciences\" , \"pubs_global\" : \"1375226\" , \"pubs_global_pc\" : \"1.16\" }, { \"name\" : \"Law and Legal Studies\" , \"pubs_global\" : \"902366\" , \"pubs_global_pc\" : \"0.76\" }, { \"name\" : \"Studies in Creative Arts and Writing\" , \"pubs_global\" : \"644962\" , \"pubs_global_pc\" : \"0.55\" }, { \"name\" : \"Built Environment and Design\" , \"pubs_global\" : \"491404\" , \"pubs_global_pc\" : \"0.42\" } ]","title":"Results"},{"location":"queries/07/","text":"7. Finding Journals using string matching Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description Returns data on how many publications have been published by journals that include a set of characters in the journal's title. Each row contains basic metadata about a single journal, plus a count of total publications indexed by Dimensions. Query SELECT COUNT ( id ) AS pubs , journal . id , journal . title , journal . issn , journal . eissn , publisher . name FROM ` dimensions - ai . data_analytics . publications ` WHERE LOWER ( journal . title ) LIKE '%medicine%' GROUP BY 2 , 3 , 4 , 5 , 6 ORDER BY pubs DESC LIMIT 20 Breaking it down WHERE LOWER(journal.title) LIKE '%medicine%' is a trick to do a case-insensitive substring search: LIKE '%medicine%' searches for any string that contains the characters medicine , in that order; the % characters indicate any other characters can show up on either side. Using LOWER(journal.title) here converts the entire journal title to lowercase before doing the string comparison , which means \"Medicine\" will be a match, as will \"MEDICINE,\" \"medicine,\" and so on. Results [ { \"pubs\" : \"168838\" , \"id\" : \"jour.1014075\" , \"title\" : \"New England Journal of Medicine\" , \"issn\" : \"0028-4793\" , \"eissn\" : \"1533-4406\" , \"name\" : \"Massachusetts Medical Society\" }, { \"pubs\" : \"84137\" , \"id\" : \"jour.1011551\" , \"title\" : \"Medicine & Science in Sports & Exercise\" , \"issn\" : \"0195-9131\" , \"eissn\" : \"1530-0315\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"58776\" , \"id\" : \"jour.1017222\" , \"title\" : \"Annals of Internal Medicine\" , \"issn\" : \"0003-4819\" , \"eissn\" : \"1539-3704\" , \"name\" : \"American College of Physicians\" }, { \"pubs\" : \"52827\" , \"id\" : \"jour.1312267\" , \"title\" : \"Journal of the Royal Society of Medicine\" , \"issn\" : \"0141-0768\" , \"eissn\" : \"1758-1095\" , \"name\" : \"SAGE Publications\" }, { \"pubs\" : \"52384\" , \"id\" : \"jour.1017256\" , \"title\" : \"JAMA Internal Medicine\" , \"issn\" : \"2168-6106\" , \"eissn\" : \"2168-6114\" , \"name\" : \"American Medical Association (AMA)\" }, { \"pubs\" : \"47157\" , \"id\" : \"jour.1027092\" , \"title\" : \"Experimental Biology and Medicine\" , \"issn\" : \"1535-3702\" , \"eissn\" : \"1535-3699\" , \"name\" : \"SAGE Publications\" }, { \"pubs\" : \"46459\" , \"id\" : \"jour.1016342\" , \"title\" : \"Critical Care Medicine\" , \"issn\" : \"0090-3493\" , \"eissn\" : \"1530-0293\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"37666\" , \"id\" : \"jour.1057918\" , \"title\" : \"Journal of Molecular Medicine\" , \"issn\" : \"0946-2716\" , \"eissn\" : \"1432-1440\" , \"name\" : \"Springer Nature\" }, { \"pubs\" : \"34891\" , \"id\" : \"jour.1017275\" , \"title\" : \"Arizona Medicine\" , \"issn\" : \"0093-0415\" , \"eissn\" : \"1476-2978\" , \"name\" : null }, { \"pubs\" : \"31166\" , \"id\" : \"jour.1014535\" , \"title\" : \"The American Journal of Medicine\" , \"issn\" : \"0002-9343\" , \"eissn\" : \"1555-7162\" , \"name\" : \"Elsevier\" }, { \"pubs\" : \"29793\" , \"id\" : \"jour.1017863\" , \"title\" : \"Oral Surgery Oral Medicine Oral Pathology and Oral Radiology\" , \"issn\" : \"2212-4403\" , \"eissn\" : \"2212-4411\" , \"name\" : \"Elsevier\" }, { \"pubs\" : \"28529\" , \"id\" : \"jour.1090935\" , \"title\" : \"Annals of Emergency Medicine\" , \"issn\" : \"0196-0644\" , \"eissn\" : \"1097-6760\" , \"name\" : \"Elsevier\" }, { \"pubs\" : \"27453\" , \"id\" : \"jour.1077253\" , \"title\" : \"Medicine\" , \"issn\" : \"0025-7974\" , \"eissn\" : \"1536-5964\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"25713\" , \"id\" : \"jour.1017316\" , \"title\" : \"Bulletin of Experimental Biology and Medicine\" , \"issn\" : \"0007-4888\" , \"eissn\" : \"1573-8221\" , \"name\" : \"Springer Nature\" }, { \"pubs\" : \"24861\" , \"id\" : \"jour.1077126\" , \"title\" : \"Journal of Experimental Medicine\" , \"issn\" : \"0022-1007\" , \"eissn\" : \"1540-9538\" , \"name\" : \"Rockefeller University Press\" }, { \"pubs\" : \"24370\" , \"id\" : \"jour.1319882\" , \"title\" : \"Journal of Internal Medicine\" , \"issn\" : \"0954-6820\" , \"eissn\" : \"1365-2796\" , \"name\" : \"Wiley\" }, { \"pubs\" : \"23679\" , \"id\" : \"jour.1017748\" , \"title\" : \"Academic Medicine\" , \"issn\" : \"1040-2446\" , \"eissn\" : \"1938-808X\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"22723\" , \"id\" : \"jour.1017031\" , \"title\" : \"American Journal of Respiratory and Critical Care Medicine\" , \"issn\" : \"1073-449X\" , \"eissn\" : \"1535-4970\" , \"name\" : \"American Thoracic Society\" }, { \"pubs\" : \"22340\" , \"id\" : \"jour.1036793\" , \"title\" : \"Military Medicine\" , \"issn\" : \"0026-4075\" , \"eissn\" : \"1930-613X\" , \"name\" : \"Oxford University Press (OUP)\" }, { \"pubs\" : \"21837\" , \"id\" : \"jour.1017021\" , \"title\" : \"American Journal of Tropical Medicine and Hygiene\" , \"issn\" : \"0002-9637\" , \"eissn\" : \"1476-1645\" , \"name\" : \"American Society of Tropical Medicine and Hygiene\" } ]","title":"7. Finding Journals using string matching"},{"location":"queries/07/#7-finding-journals-using-string-matching","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"7. Finding Journals using string matching"},{"location":"queries/07/#description","text":"Returns data on how many publications have been published by journals that include a set of characters in the journal's title. Each row contains basic metadata about a single journal, plus a count of total publications indexed by Dimensions.","title":"Description"},{"location":"queries/07/#query","text":"SELECT COUNT ( id ) AS pubs , journal . id , journal . title , journal . issn , journal . eissn , publisher . name FROM ` dimensions - ai . data_analytics . publications ` WHERE LOWER ( journal . title ) LIKE '%medicine%' GROUP BY 2 , 3 , 4 , 5 , 6 ORDER BY pubs DESC LIMIT 20","title":"Query"},{"location":"queries/07/#breaking-it-down","text":"WHERE LOWER(journal.title) LIKE '%medicine%' is a trick to do a case-insensitive substring search: LIKE '%medicine%' searches for any string that contains the characters medicine , in that order; the % characters indicate any other characters can show up on either side. Using LOWER(journal.title) here converts the entire journal title to lowercase before doing the string comparison , which means \"Medicine\" will be a match, as will \"MEDICINE,\" \"medicine,\" and so on.","title":"Breaking it down"},{"location":"queries/07/#results","text":"[ { \"pubs\" : \"168838\" , \"id\" : \"jour.1014075\" , \"title\" : \"New England Journal of Medicine\" , \"issn\" : \"0028-4793\" , \"eissn\" : \"1533-4406\" , \"name\" : \"Massachusetts Medical Society\" }, { \"pubs\" : \"84137\" , \"id\" : \"jour.1011551\" , \"title\" : \"Medicine & Science in Sports & Exercise\" , \"issn\" : \"0195-9131\" , \"eissn\" : \"1530-0315\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"58776\" , \"id\" : \"jour.1017222\" , \"title\" : \"Annals of Internal Medicine\" , \"issn\" : \"0003-4819\" , \"eissn\" : \"1539-3704\" , \"name\" : \"American College of Physicians\" }, { \"pubs\" : \"52827\" , \"id\" : \"jour.1312267\" , \"title\" : \"Journal of the Royal Society of Medicine\" , \"issn\" : \"0141-0768\" , \"eissn\" : \"1758-1095\" , \"name\" : \"SAGE Publications\" }, { \"pubs\" : \"52384\" , \"id\" : \"jour.1017256\" , \"title\" : \"JAMA Internal Medicine\" , \"issn\" : \"2168-6106\" , \"eissn\" : \"2168-6114\" , \"name\" : \"American Medical Association (AMA)\" }, { \"pubs\" : \"47157\" , \"id\" : \"jour.1027092\" , \"title\" : \"Experimental Biology and Medicine\" , \"issn\" : \"1535-3702\" , \"eissn\" : \"1535-3699\" , \"name\" : \"SAGE Publications\" }, { \"pubs\" : \"46459\" , \"id\" : \"jour.1016342\" , \"title\" : \"Critical Care Medicine\" , \"issn\" : \"0090-3493\" , \"eissn\" : \"1530-0293\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"37666\" , \"id\" : \"jour.1057918\" , \"title\" : \"Journal of Molecular Medicine\" , \"issn\" : \"0946-2716\" , \"eissn\" : \"1432-1440\" , \"name\" : \"Springer Nature\" }, { \"pubs\" : \"34891\" , \"id\" : \"jour.1017275\" , \"title\" : \"Arizona Medicine\" , \"issn\" : \"0093-0415\" , \"eissn\" : \"1476-2978\" , \"name\" : null }, { \"pubs\" : \"31166\" , \"id\" : \"jour.1014535\" , \"title\" : \"The American Journal of Medicine\" , \"issn\" : \"0002-9343\" , \"eissn\" : \"1555-7162\" , \"name\" : \"Elsevier\" }, { \"pubs\" : \"29793\" , \"id\" : \"jour.1017863\" , \"title\" : \"Oral Surgery Oral Medicine Oral Pathology and Oral Radiology\" , \"issn\" : \"2212-4403\" , \"eissn\" : \"2212-4411\" , \"name\" : \"Elsevier\" }, { \"pubs\" : \"28529\" , \"id\" : \"jour.1090935\" , \"title\" : \"Annals of Emergency Medicine\" , \"issn\" : \"0196-0644\" , \"eissn\" : \"1097-6760\" , \"name\" : \"Elsevier\" }, { \"pubs\" : \"27453\" , \"id\" : \"jour.1077253\" , \"title\" : \"Medicine\" , \"issn\" : \"0025-7974\" , \"eissn\" : \"1536-5964\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"25713\" , \"id\" : \"jour.1017316\" , \"title\" : \"Bulletin of Experimental Biology and Medicine\" , \"issn\" : \"0007-4888\" , \"eissn\" : \"1573-8221\" , \"name\" : \"Springer Nature\" }, { \"pubs\" : \"24861\" , \"id\" : \"jour.1077126\" , \"title\" : \"Journal of Experimental Medicine\" , \"issn\" : \"0022-1007\" , \"eissn\" : \"1540-9538\" , \"name\" : \"Rockefeller University Press\" }, { \"pubs\" : \"24370\" , \"id\" : \"jour.1319882\" , \"title\" : \"Journal of Internal Medicine\" , \"issn\" : \"0954-6820\" , \"eissn\" : \"1365-2796\" , \"name\" : \"Wiley\" }, { \"pubs\" : \"23679\" , \"id\" : \"jour.1017748\" , \"title\" : \"Academic Medicine\" , \"issn\" : \"1040-2446\" , \"eissn\" : \"1938-808X\" , \"name\" : \"Wolters Kluwer\" }, { \"pubs\" : \"22723\" , \"id\" : \"jour.1017031\" , \"title\" : \"American Journal of Respiratory and Critical Care Medicine\" , \"issn\" : \"1073-449X\" , \"eissn\" : \"1535-4970\" , \"name\" : \"American Thoracic Society\" }, { \"pubs\" : \"22340\" , \"id\" : \"jour.1036793\" , \"title\" : \"Military Medicine\" , \"issn\" : \"0026-4075\" , \"eissn\" : \"1930-613X\" , \"name\" : \"Oxford University Press (OUP)\" }, { \"pubs\" : \"21837\" , \"id\" : \"jour.1017021\" , \"title\" : \"American Journal of Tropical Medicine and Hygiene\" , \"issn\" : \"0002-9637\" , \"eissn\" : \"1476-1645\" , \"name\" : \"American Society of Tropical Medicine and Hygiene\" } ]","title":"Results"},{"location":"queries/08/","text":"8. Finding articles matching a specific affiliation string Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query returns a list of publications and affiliations associated with a single institution. Each line represents a single affiliation string, and includes the publication ID, the institution ID, and the affiliation string as reported by the journal. Results are filtered to include only a single institution, specified by GRID ID ( grid.69566.3a ), and affiliation strings that include the phrase \"school of medicine\" . For more details about working with nested fields, see the tutorial page on the topic . Query SELECT p . id , aff . grid_id , aff . raw_affiliation FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( authors ) auth CROSS JOIN UNNEST ( auth . affiliations_address ) aff WHERE year = 2020 AND aff . grid_id = \"grid.69566.3a\" -- Sendai, Japan AND LOWER ( aff . raw_affiliation ) LIKE \"%school of medicine%\" Results [ { \"id\" : \"pub.1120198400\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Department of Neurosurgery, Tohoku University Graduate School of Medicine, Sendai, Miyagi, Japan\" }, { \"id\" : \"pub.1117164397\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Division of Cardiovascular Surgery, Tohoku University Graduate School of Medicine, 1-1 Seiryo-machi, Aoba-ku, Sendai, Miyagi, Japan.\" }, { \"id\" : \"pub.1120113207\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Division of Internal Medicine and Hypertension Unit Division of Cardiology, Department of Medical Sciences, University of Torino, Torino Division of Internal Medicine, Department of Medicine, University of Udine, Udine, Italy Division of Clinical Hypertension, Endocrinology and Metabolism, Tohoku University Graduate School of Medicine, Sendai, Japan.\" }, { \"id\" : \"pub.1119863526\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Division of Emergency and Critical Care Medicine, Tohoku University Graduate School of Medicine, Japan.\" }, // ma n y t housa n ds more records... ] 8.1 Variant: get unique publication records with affiliation count SELECT COUNT ( aff ) AS matching_affiliations , id , title . preferred AS title FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( authors ) auth , UNNEST ( auth . affiliations_address ) AS aff WHERE year = 2020 AND aff . grid_id = \"grid.69566.3a\" AND LOWER ( aff . raw_affiliation ) LIKE \"%school of medicine%\" GROUP BY id , title Results from variant [ { \"matching_affiliations\" : \"3\" , \"id\" : \"pub.1123153684\" , \"title\" : \"Management following endoscopic resection in elderly patients with early\u2010stage upper gastrointestinal neoplasia\" }, { \"matching_affiliations\" : \"1\" , \"id\" : \"pub.1124283456\" , \"title\" : \"Unique Sex Steroid Profiles in Estrogen-Producing Adrenocortical Adenoma Associated with Bilateral Hyperaldosteronism\" }, { \"matching_affiliations\" : \"5\" , \"id\" : \"pub.1124295695\" , \"title\" : \"Clinical implication of myocardial FDG uptake pattern in oncologic PET: retrospective comparison study with stress myocardial perfusion imaging as the reference standard\" }, { \"matching_affiliations\" : \"7\" , \"id\" : \"pub.1124238412\" , \"title\" : \"Keap1 deletion accelerates mutant K-ras/p53-driven cholangiocarcinoma\" }, // ma n y more resul ts ... ]","title":"8. Finding articles matching a specific affiliation string"},{"location":"queries/08/#8-finding-articles-matching-a-specific-affiliation-string","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"8. Finding articles matching a specific affiliation string"},{"location":"queries/08/#description","text":"This query returns a list of publications and affiliations associated with a single institution. Each line represents a single affiliation string, and includes the publication ID, the institution ID, and the affiliation string as reported by the journal. Results are filtered to include only a single institution, specified by GRID ID ( grid.69566.3a ), and affiliation strings that include the phrase \"school of medicine\" . For more details about working with nested fields, see the tutorial page on the topic .","title":"Description"},{"location":"queries/08/#query","text":"SELECT p . id , aff . grid_id , aff . raw_affiliation FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( authors ) auth CROSS JOIN UNNEST ( auth . affiliations_address ) aff WHERE year = 2020 AND aff . grid_id = \"grid.69566.3a\" -- Sendai, Japan AND LOWER ( aff . raw_affiliation ) LIKE \"%school of medicine%\"","title":"Query"},{"location":"queries/08/#results","text":"[ { \"id\" : \"pub.1120198400\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Department of Neurosurgery, Tohoku University Graduate School of Medicine, Sendai, Miyagi, Japan\" }, { \"id\" : \"pub.1117164397\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Division of Cardiovascular Surgery, Tohoku University Graduate School of Medicine, 1-1 Seiryo-machi, Aoba-ku, Sendai, Miyagi, Japan.\" }, { \"id\" : \"pub.1120113207\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Division of Internal Medicine and Hypertension Unit Division of Cardiology, Department of Medical Sciences, University of Torino, Torino Division of Internal Medicine, Department of Medicine, University of Udine, Udine, Italy Division of Clinical Hypertension, Endocrinology and Metabolism, Tohoku University Graduate School of Medicine, Sendai, Japan.\" }, { \"id\" : \"pub.1119863526\" , \"grid_id\" : \"grid.69566.3a\" , \"raw_affiliation\" : \"Division of Emergency and Critical Care Medicine, Tohoku University Graduate School of Medicine, Japan.\" }, // ma n y t housa n ds more records... ]","title":"Results"},{"location":"queries/08/#81-variant-get-unique-publication-records-with-affiliation-count","text":"SELECT COUNT ( aff ) AS matching_affiliations , id , title . preferred AS title FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( authors ) auth , UNNEST ( auth . affiliations_address ) AS aff WHERE year = 2020 AND aff . grid_id = \"grid.69566.3a\" AND LOWER ( aff . raw_affiliation ) LIKE \"%school of medicine%\" GROUP BY id , title","title":"8.1 Variant: get unique publication records with affiliation count"},{"location":"queries/08/#results-from-variant","text":"[ { \"matching_affiliations\" : \"3\" , \"id\" : \"pub.1123153684\" , \"title\" : \"Management following endoscopic resection in elderly patients with early\u2010stage upper gastrointestinal neoplasia\" }, { \"matching_affiliations\" : \"1\" , \"id\" : \"pub.1124283456\" , \"title\" : \"Unique Sex Steroid Profiles in Estrogen-Producing Adrenocortical Adenoma Associated with Bilateral Hyperaldosteronism\" }, { \"matching_affiliations\" : \"5\" , \"id\" : \"pub.1124295695\" , \"title\" : \"Clinical implication of myocardial FDG uptake pattern in oncologic PET: retrospective comparison study with stress myocardial perfusion imaging as the reference standard\" }, { \"matching_affiliations\" : \"7\" , \"id\" : \"pub.1124238412\" , \"title\" : \"Keap1 deletion accelerates mutant K-ras/p53-driven cholangiocarcinoma\" }, // ma n y more resul ts ... ]","title":"Results from variant"},{"location":"queries/09/","text":"9. Top publications by Altmetric score and research organization Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description Retrieve publications for a selected research organization (using its GRID identifier ) and sort them using their Altmetric Attention Score. Query SELECT id , title . preferred as title , ARRAY_LENGTH ( authors ) as authors , -- include number of authors altmetrics . score as altmetrics_score FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND 'grid.4991.5' in UNNEST ( research_orgs ) -- a sample grid ID ORDER BY altmetrics . score DESC LIMIT 5 -- Get top 5 only Results [ { \"id\" : \"pub.1130340155\" , \"title\" : \"Two metres or one: what is the evidence for physical distancing in covid-19?\" , \"authors\" : \"6\" , \"altmetrics_score\" : \"15978\" }, { \"id\" : \"pub.1129493369\" , \"title\" : \"Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial\" , \"authors\" : \"366\" , \"altmetrics_score\" : \"15612\" }, { \"id\" : \"pub.1133359801\" , \"title\" : \"Safety and efficacy of the ChAdOx1 nCoV-19 vaccine (AZD1222) against SARS-CoV-2: an interim analysis of four randomised controlled trials in Brazil, South Africa, and the UK\" , \"authors\" : \"766\" , \"altmetrics_score\" : \"12292\" }, { \"id\" : \"pub.1127239818\" , \"title\" : \"Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial\" , \"authors\" : \"46\" , \"altmetrics_score\" : \"12036\" }, { \"id\" : \"pub.1131721397\" , \"title\" : \"Scientific consensus on the COVID-19 pandemic: we need to act now\" , \"authors\" : \"31\" , \"altmetrics_score\" : \"10534\" } ]","title":"9. Top publications by Altmetric score and research organization"},{"location":"queries/09/#9-top-publications-by-altmetric-score-and-research-organization","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"9. Top publications by Altmetric score and research organization"},{"location":"queries/09/#description","text":"Retrieve publications for a selected research organization (using its GRID identifier ) and sort them using their Altmetric Attention Score.","title":"Description"},{"location":"queries/09/#query","text":"SELECT id , title . preferred as title , ARRAY_LENGTH ( authors ) as authors , -- include number of authors altmetrics . score as altmetrics_score FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND 'grid.4991.5' in UNNEST ( research_orgs ) -- a sample grid ID ORDER BY altmetrics . score DESC LIMIT 5 -- Get top 5 only","title":"Query"},{"location":"queries/09/#results","text":"[ { \"id\" : \"pub.1130340155\" , \"title\" : \"Two metres or one: what is the evidence for physical distancing in covid-19?\" , \"authors\" : \"6\" , \"altmetrics_score\" : \"15978\" }, { \"id\" : \"pub.1129493369\" , \"title\" : \"Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial\" , \"authors\" : \"366\" , \"altmetrics_score\" : \"15612\" }, { \"id\" : \"pub.1133359801\" , \"title\" : \"Safety and efficacy of the ChAdOx1 nCoV-19 vaccine (AZD1222) against SARS-CoV-2: an interim analysis of four randomised controlled trials in Brazil, South Africa, and the UK\" , \"authors\" : \"766\" , \"altmetrics_score\" : \"12292\" }, { \"id\" : \"pub.1127239818\" , \"title\" : \"Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial\" , \"authors\" : \"46\" , \"altmetrics_score\" : \"12036\" }, { \"id\" : \"pub.1131721397\" , \"title\" : \"Scientific consensus on the COVID-19 pandemic: we need to act now\" , \"authors\" : \"31\" , \"altmetrics_score\" : \"10534\" } ]","title":"Results"},{"location":"queries/10/","text":"10. Select publications matching selected concepts Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query counts the number of publications about a set of tropical diseases. It filters publications by selecting only those that either refer to those diseases in the title, or that have been tagged with those diseases as concepts within the Dimensions database. Once publications have been found, the counts are broken down both by year and by publisher; the final list shows the top 10 publisher-years in which the most papers were published about those diseases. Query SELECT publisher . NAME AS publisher , year , COUNT ( * ) AS num_pub FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( concepts ) c WHERE ( LOWER ( c . concept ) IN UNNEST ([ \"buruli ulcer\" , \"mycobacterium\" , \"mycolactone\" , \"bairnsdale ulcer\" ]) OR REGEXP_CONTAINS ( title . preferred , r \"(?i)/buruli ulcer|mycobacterium|mycolactone|bairnsdale ulcer/\" )) AND year >= 2010 AND publisher IS NOT NULL GROUP BY publisher , year ORDER BY num_pub DESC , year , publisher LIMIT 10 Results [ { \"publisher\" : \"Elsevier\" , \"year\" : \"2020\" , \"num_pub\" : \"31602\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2018\" , \"num_pub\" : \"29639\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2019\" , \"num_pub\" : \"28941\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2017\" , \"num_pub\" : \"28415\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2015\" , \"num_pub\" : \"27299\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2011\" , \"num_pub\" : \"25757\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2016\" , \"num_pub\" : \"25149\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2013\" , \"num_pub\" : \"23205\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2014\" , \"num_pub\" : \"22952\" }, { \"publisher\" : \"Springer Nature\" , \"year\" : \"2019\" , \"num_pub\" : \"22072\" } ]","title":"10. Select publications matching selected concepts"},{"location":"queries/10/#10-select-publications-matching-selected-concepts","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"10. Select publications matching selected concepts"},{"location":"queries/10/#description","text":"This query counts the number of publications about a set of tropical diseases. It filters publications by selecting only those that either refer to those diseases in the title, or that have been tagged with those diseases as concepts within the Dimensions database. Once publications have been found, the counts are broken down both by year and by publisher; the final list shows the top 10 publisher-years in which the most papers were published about those diseases.","title":"Description"},{"location":"queries/10/#query","text":"SELECT publisher . NAME AS publisher , year , COUNT ( * ) AS num_pub FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( concepts ) c WHERE ( LOWER ( c . concept ) IN UNNEST ([ \"buruli ulcer\" , \"mycobacterium\" , \"mycolactone\" , \"bairnsdale ulcer\" ]) OR REGEXP_CONTAINS ( title . preferred , r \"(?i)/buruli ulcer|mycobacterium|mycolactone|bairnsdale ulcer/\" )) AND year >= 2010 AND publisher IS NOT NULL GROUP BY publisher , year ORDER BY num_pub DESC , year , publisher LIMIT 10","title":"Query"},{"location":"queries/10/#results","text":"[ { \"publisher\" : \"Elsevier\" , \"year\" : \"2020\" , \"num_pub\" : \"31602\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2018\" , \"num_pub\" : \"29639\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2019\" , \"num_pub\" : \"28941\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2017\" , \"num_pub\" : \"28415\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2015\" , \"num_pub\" : \"27299\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2011\" , \"num_pub\" : \"25757\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2016\" , \"num_pub\" : \"25149\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2013\" , \"num_pub\" : \"23205\" }, { \"publisher\" : \"Elsevier\" , \"year\" : \"2014\" , \"num_pub\" : \"22952\" }, { \"publisher\" : \"Springer Nature\" , \"year\" : \"2019\" , \"num_pub\" : \"22072\" } ]","title":"Results"},{"location":"queries/11/","text":"11. Count of corresponding authors by publisher Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description This query counts the number of unique corresponding authors that appear on publications from each publisher. The results list publishers with the count of unique researcher IDs . Query SELECT COUNT ( DISTINCT researcher_id ) AS tot , publisher . name FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( authors ) aff WHERE aff . corresponding IS TRUE AND publisher . name IS NOT NULL AND year >= 2010 GROUP BY publisher . name ORDER BY tot DESC LIMIT 10 Results [ { \"tot\" : \"1717859\" , \"name\" : \"Springer Nature\" }, { \"tot\" : \"1716636\" , \"name\" : \"Elsevier\" }, { \"tot\" : \"303497\" , \"name\" : \"Institute of Electrical and Electronics Engineers (IEEE)\" }, { \"tot\" : \"287259\" , \"name\" : \"SAGE Publications\" }, { \"tot\" : \"262973\" , \"name\" : \"MDPI\" }, { \"tot\" : \"141491\" , \"name\" : \"Hindawi\" }, { \"tot\" : \"122421\" , \"name\" : \"Public Library of Science (PLoS)\" }, { \"tot\" : \"84153\" , \"name\" : \"Cold Spring Harbor Laboratory\" }, { \"tot\" : \"75136\" , \"name\" : \"Frontiers\" }, { \"tot\" : \"68176\" , \"name\" : \"Pleiades Publishing\" } ]","title":"11. Count of corresponding authors by publisher"},{"location":"queries/11/#11-count-of-corresponding-authors-by-publisher","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"11. Count of corresponding authors by publisher"},{"location":"queries/11/#description","text":"This query counts the number of unique corresponding authors that appear on publications from each publisher. The results list publishers with the count of unique researcher IDs .","title":"Description"},{"location":"queries/11/#query","text":"SELECT COUNT ( DISTINCT researcher_id ) AS tot , publisher . name FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( authors ) aff WHERE aff . corresponding IS TRUE AND publisher . name IS NOT NULL AND year >= 2010 GROUP BY publisher . name ORDER BY tot DESC LIMIT 10","title":"Query"},{"location":"queries/11/#results","text":"[ { \"tot\" : \"1717859\" , \"name\" : \"Springer Nature\" }, { \"tot\" : \"1716636\" , \"name\" : \"Elsevier\" }, { \"tot\" : \"303497\" , \"name\" : \"Institute of Electrical and Electronics Engineers (IEEE)\" }, { \"tot\" : \"287259\" , \"name\" : \"SAGE Publications\" }, { \"tot\" : \"262973\" , \"name\" : \"MDPI\" }, { \"tot\" : \"141491\" , \"name\" : \"Hindawi\" }, { \"tot\" : \"122421\" , \"name\" : \"Public Library of Science (PLoS)\" }, { \"tot\" : \"84153\" , \"name\" : \"Cold Spring Harbor Laboratory\" }, { \"tot\" : \"75136\" , \"name\" : \"Frontiers\" }, { \"tot\" : \"68176\" , \"name\" : \"Pleiades Publishing\" } ]","title":"Results"},{"location":"queries/12/","text":"12. Counting new vs recurring authors, for a specific journal Level: Advanced This query requires a good understanding of SQL and the Dimensions data model Description This query evaluates a single journal (specified by journal ID - e.g. jour.1115214 ) and counts the number of authors per year that it has published. Authors are split into two categories: new authors who have never appeared in the journal before, and \"recurring\" authors who have already written at least one previous article in the journal. Note Because the query starts in 2011, some \"recurring\" authors are never counted as new authors : If an author publishes one paper each in, say, 2007 and 2014, they will appear as a recurring author in 2014, but the year in which they would be a \"new\" author is not displayed. Query WITH authoryear AS ( -- how many papers has each individual researcher published in -- the specified journal? Broken down by year. SELECT pubs . year , author . researcher_id , COUNT ( pubs . id ) AS numpubs FROM ` dimensions - ai . data_analytics . publications ` AS pubs CROSS JOIN UNNEST ( pubs . authors ) AS author WHERE author . researcher_id IS NOT NULL AND journal . id = \"jour.1115214\" -- Nature BioTechnology GROUP BY author . researcher_id , pubs . year ), authorfirst AS ( -- For each author, what year is their FIRST publication in -- the specified journal? SELECT researcher_id , MIN ( year ) AS minyear FROM authoryear GROUP BY researcher_id ), authorsummary AS ( -- Modify the author-level list of publications per -- year by adding a new field, \"firstyear\", that indicates -- whether this is the year in which they are \"new.\" SELECT ay . * , IF ( ay . year = af . minyear , TRUE , FALSE ) AS firstyear FROM authoryear ay INNER JOIN authorfirst af ON af . researcher_id = ay . researcher_id ), numauthors AS ( -- For each year, total up the new and recurring authors SELECT year , firstyear , COUNT ( DISTINCT researcher_id ) AS numresearchers FROM authorsummary WHERE year > 2010 GROUP BY year , firstyear ) -- Finally, we rearrange the \"numauthors\" subquery so -- each year in the specified range only has a SINGLE ROW, -- indicating both the new and recurring authors. SELECT year , SUM ( CASE WHEN firstyear THEN numresearchers ELSE 0 END ) AS num_first , SUM ( CASE WHEN NOT firstyear THEN numresearchers ELSE 0 END ) AS num_recurring FROM numauthors GROUP BY year ORDER BY year Results [ { \"year\" : \"2011\" , \"num_first\" : \"1041\" , \"num_recurring\" : \"352\" }, { \"year\" : \"2012\" , \"num_first\" : \"859\" , \"num_recurring\" : \"374\" }, { \"year\" : \"2013\" , \"num_first\" : \"927\" , \"num_recurring\" : \"347\" }, { \"year\" : \"2014\" , \"num_first\" : \"1088\" , \"num_recurring\" : \"338\" }, { \"year\" : \"2015\" , \"num_first\" : \"1044\" , \"num_recurring\" : \"392\" }, { \"year\" : \"2016\" , \"num_first\" : \"1319\" , \"num_recurring\" : \"350\" }, { \"year\" : \"2017\" , \"num_first\" : \"1074\" , \"num_recurring\" : \"404\" }, { \"year\" : \"2018\" , \"num_first\" : \"1111\" , \"num_recurring\" : \"419\" }, { \"year\" : \"2019\" , \"num_first\" : \"1219\" , \"num_recurring\" : \"447\" }, { \"year\" : \"2020\" , \"num_first\" : \"1611\" , \"num_recurring\" : \"570\" }, { \"year\" : \"2021\" , \"num_first\" : \"411\" , \"num_recurring\" : \"189\" } ]","title":"12. Counting new vs recurring authors, for a specific journal"},{"location":"queries/12/#12-counting-new-vs-recurring-authors-for-a-specific-journal","text":"Level: Advanced This query requires a good understanding of SQL and the Dimensions data model","title":"12. Counting new vs recurring authors, for a specific journal"},{"location":"queries/12/#description","text":"This query evaluates a single journal (specified by journal ID - e.g. jour.1115214 ) and counts the number of authors per year that it has published. Authors are split into two categories: new authors who have never appeared in the journal before, and \"recurring\" authors who have already written at least one previous article in the journal. Note Because the query starts in 2011, some \"recurring\" authors are never counted as new authors : If an author publishes one paper each in, say, 2007 and 2014, they will appear as a recurring author in 2014, but the year in which they would be a \"new\" author is not displayed.","title":"Description"},{"location":"queries/12/#query","text":"WITH authoryear AS ( -- how many papers has each individual researcher published in -- the specified journal? Broken down by year. SELECT pubs . year , author . researcher_id , COUNT ( pubs . id ) AS numpubs FROM ` dimensions - ai . data_analytics . publications ` AS pubs CROSS JOIN UNNEST ( pubs . authors ) AS author WHERE author . researcher_id IS NOT NULL AND journal . id = \"jour.1115214\" -- Nature BioTechnology GROUP BY author . researcher_id , pubs . year ), authorfirst AS ( -- For each author, what year is their FIRST publication in -- the specified journal? SELECT researcher_id , MIN ( year ) AS minyear FROM authoryear GROUP BY researcher_id ), authorsummary AS ( -- Modify the author-level list of publications per -- year by adding a new field, \"firstyear\", that indicates -- whether this is the year in which they are \"new.\" SELECT ay . * , IF ( ay . year = af . minyear , TRUE , FALSE ) AS firstyear FROM authoryear ay INNER JOIN authorfirst af ON af . researcher_id = ay . researcher_id ), numauthors AS ( -- For each year, total up the new and recurring authors SELECT year , firstyear , COUNT ( DISTINCT researcher_id ) AS numresearchers FROM authorsummary WHERE year > 2010 GROUP BY year , firstyear ) -- Finally, we rearrange the \"numauthors\" subquery so -- each year in the specified range only has a SINGLE ROW, -- indicating both the new and recurring authors. SELECT year , SUM ( CASE WHEN firstyear THEN numresearchers ELSE 0 END ) AS num_first , SUM ( CASE WHEN NOT firstyear THEN numresearchers ELSE 0 END ) AS num_recurring FROM numauthors GROUP BY year ORDER BY year","title":"Query"},{"location":"queries/12/#results","text":"[ { \"year\" : \"2011\" , \"num_first\" : \"1041\" , \"num_recurring\" : \"352\" }, { \"year\" : \"2012\" , \"num_first\" : \"859\" , \"num_recurring\" : \"374\" }, { \"year\" : \"2013\" , \"num_first\" : \"927\" , \"num_recurring\" : \"347\" }, { \"year\" : \"2014\" , \"num_first\" : \"1088\" , \"num_recurring\" : \"338\" }, { \"year\" : \"2015\" , \"num_first\" : \"1044\" , \"num_recurring\" : \"392\" }, { \"year\" : \"2016\" , \"num_first\" : \"1319\" , \"num_recurring\" : \"350\" }, { \"year\" : \"2017\" , \"num_first\" : \"1074\" , \"num_recurring\" : \"404\" }, { \"year\" : \"2018\" , \"num_first\" : \"1111\" , \"num_recurring\" : \"419\" }, { \"year\" : \"2019\" , \"num_first\" : \"1219\" , \"num_recurring\" : \"447\" }, { \"year\" : \"2020\" , \"num_first\" : \"1611\" , \"num_recurring\" : \"570\" }, { \"year\" : \"2021\" , \"num_first\" : \"411\" , \"num_recurring\" : \"189\" } ]","title":"Results"},{"location":"queries/13/","text":"13. Funding by journal Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query looks at all publications from a single specified journal and counts the total number of grants associated with those publications. The results list each funding agency with a count of how many papers it has been linked to, combined with the number of grants from that agency that were referenced by those papers. Query WITH funding AS ( SELECT funding . grid_id AS funders , COUNT ( id ) AS pubs , COUNT ( funding . grant_id ) AS grants FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( funding_details ) AS funding WHERE journal . id = \"jour.1113716\" -- nature medicine GROUP BY funders ) SELECT funding . * , grid . name FROM funding INNER JOIN ` dimensions - ai . data_analytics . grid ` grid ON funding . funders = grid . id ORDER BY pubs DESC , grants DESC LIMIT 10 Results [ { \"funders\" : \"grid.48336.3a\" , \"pubs\" : \"2727\" , \"grants\" : \"2510\" , \"name\" : \"National Cancer Institute\" }, { \"funders\" : \"grid.419681.3\" , \"pubs\" : \"2043\" , \"grants\" : \"1910\" , \"name\" : \"National Institute of Allergy and Infectious Diseases\" }, { \"funders\" : \"grid.419635.c\" , \"pubs\" : \"1642\" , \"grants\" : \"1584\" , \"name\" : \"National Institute of Diabetes and Digestive and Kidney Diseases\" }, { \"funders\" : \"grid.279885.9\" , \"pubs\" : \"1641\" , \"grants\" : \"1554\" , \"name\" : \"National Heart Lung and Blood Institute\" }, { \"funders\" : \"grid.416870.c\" , \"pubs\" : \"717\" , \"grants\" : \"673\" , \"name\" : \"National Institute of Neurological Disorders and Stroke\" }, { \"funders\" : \"grid.419475.a\" , \"pubs\" : \"585\" , \"grants\" : \"553\" , \"name\" : \"National Institute on Aging\" }, { \"funders\" : \"grid.14105.31\" , \"pubs\" : \"547\" , \"grants\" : \"431\" , \"name\" : \"Medical Research Council\" }, { \"funders\" : \"grid.54432.34\" , \"pubs\" : \"512\" , \"grants\" : \"447\" , \"name\" : \"Japan Society for the Promotion of Science\" }, { \"funders\" : \"grid.280785.0\" , \"pubs\" : \"465\" , \"grants\" : \"446\" , \"name\" : \"National Institute of General Medical Sciences\" }, { \"funders\" : \"grid.270680.b\" , \"pubs\" : \"410\" , \"grants\" : \"187\" , \"name\" : \"European Commission\" } ]","title":"13. Funding by journal"},{"location":"queries/13/#13-funding-by-journal","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"13. Funding by journal"},{"location":"queries/13/#description","text":"This query looks at all publications from a single specified journal and counts the total number of grants associated with those publications. The results list each funding agency with a count of how many papers it has been linked to, combined with the number of grants from that agency that were referenced by those papers.","title":"Description"},{"location":"queries/13/#query","text":"WITH funding AS ( SELECT funding . grid_id AS funders , COUNT ( id ) AS pubs , COUNT ( funding . grant_id ) AS grants FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( funding_details ) AS funding WHERE journal . id = \"jour.1113716\" -- nature medicine GROUP BY funders ) SELECT funding . * , grid . name FROM funding INNER JOIN ` dimensions - ai . data_analytics . grid ` grid ON funding . funders = grid . id ORDER BY pubs DESC , grants DESC LIMIT 10","title":"Query"},{"location":"queries/13/#results","text":"[ { \"funders\" : \"grid.48336.3a\" , \"pubs\" : \"2727\" , \"grants\" : \"2510\" , \"name\" : \"National Cancer Institute\" }, { \"funders\" : \"grid.419681.3\" , \"pubs\" : \"2043\" , \"grants\" : \"1910\" , \"name\" : \"National Institute of Allergy and Infectious Diseases\" }, { \"funders\" : \"grid.419635.c\" , \"pubs\" : \"1642\" , \"grants\" : \"1584\" , \"name\" : \"National Institute of Diabetes and Digestive and Kidney Diseases\" }, { \"funders\" : \"grid.279885.9\" , \"pubs\" : \"1641\" , \"grants\" : \"1554\" , \"name\" : \"National Heart Lung and Blood Institute\" }, { \"funders\" : \"grid.416870.c\" , \"pubs\" : \"717\" , \"grants\" : \"673\" , \"name\" : \"National Institute of Neurological Disorders and Stroke\" }, { \"funders\" : \"grid.419475.a\" , \"pubs\" : \"585\" , \"grants\" : \"553\" , \"name\" : \"National Institute on Aging\" }, { \"funders\" : \"grid.14105.31\" , \"pubs\" : \"547\" , \"grants\" : \"431\" , \"name\" : \"Medical Research Council\" }, { \"funders\" : \"grid.54432.34\" , \"pubs\" : \"512\" , \"grants\" : \"447\" , \"name\" : \"Japan Society for the Promotion of Science\" }, { \"funders\" : \"grid.280785.0\" , \"pubs\" : \"465\" , \"grants\" : \"446\" , \"name\" : \"National Institute of General Medical Sciences\" }, { \"funders\" : \"grid.270680.b\" , \"pubs\" : \"410\" , \"grants\" : \"187\" , \"name\" : \"European Commission\" } ]","title":"Results"},{"location":"queries/14/","text":"14. Extracting complex publications records Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description The query below combines various techniques in order to extract full publication metadata records that include both single-value metadata and unpacked lists. Note: we use LEFT JOIN clauses in order to ensure we obtain all records, not just the ones that have non-null values in the nested objects. Query SELECT p . id , p . title . preferred AS title , p . doi , p . year , COALESCE ( p . journal . title , p . proceedings_title . preferred , p . book_title . preferred , p . book_series_title . preferred ) AS venue , p . type , p . date AS date_publication , p . date_inserted , p . altmetrics . score AS altmetrics_score , p . metrics . times_cited , grid . id AS gridid , grid . name AS gridname , grid . address . country AS gridcountry , grid . address . city AS gridcity , open_access_categories , cat_for . name AS category_for , FROM ` dimensions - ai . data_analytics . publications ` p LEFT JOIN UNNEST ( research_orgs ) AS research_orgs_grids LEFT JOIN ` dimensions - ai . data_analytics . grid ` grid ON grid . id = research_orgs_grids LEFT JOIN UNNEST ( p . open_access_categories_v2 ) AS open_access_categories LEFT JOIN UNNEST ( p . category_for . first_level . full ) AS cat_for WHERE EXTRACT ( YEAR FROM date_inserted ) >= 2020 Results [ { \"id\" : \"pub.1124854415\" , \"title\" : \"Gabinetto armonico pieno d'istromenti sonori\" , \"doi\" : \"10.5479/sil.744616.39088011251444\" , \"year\" : \"1722\" , \"venue\" : null , \"type\" : \"monograph\" , \"date_publication\" : \"1722\" , \"date_inserted\" : \"2020-02-15 01:10:52 UTC\" , \"altmetrics_score\" : null , \"times_cited\" : \"3\" , \"gridid\" : null , \"gridname\" : null , \"gridcountry\" : null , \"gridcity\" : null , \"open_access_categories\" : \"oa_all\" , \"category_for\" : null }, { \"id\" : \"pub.1124854415\" , \"title\" : \"Gabinetto armonico pieno d'istromenti sonori\" , \"doi\" : \"10.5479/sil.744616.39088011251444\" , \"year\" : \"1722\" , \"venue\" : null , \"type\" : \"monograph\" , \"date_publication\" : \"1722\" , \"date_inserted\" : \"2020-02-15 01:10:52 UTC\" , \"altmetrics_score\" : null , \"times_cited\" : \"3\" , \"gridid\" : null , \"gridname\" : null , \"gridcountry\" : null , \"gridcity\" : null , \"open_access_categories\" : \"bronze\" , \"category_for\" : null }, // ma n y more e ntr ies here... ]","title":"14. Extracting complex publications records"},{"location":"queries/14/#14-extracting-complex-publications-records","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"14. Extracting complex publications records"},{"location":"queries/14/#description","text":"The query below combines various techniques in order to extract full publication metadata records that include both single-value metadata and unpacked lists. Note: we use LEFT JOIN clauses in order to ensure we obtain all records, not just the ones that have non-null values in the nested objects.","title":"Description"},{"location":"queries/14/#query","text":"SELECT p . id , p . title . preferred AS title , p . doi , p . year , COALESCE ( p . journal . title , p . proceedings_title . preferred , p . book_title . preferred , p . book_series_title . preferred ) AS venue , p . type , p . date AS date_publication , p . date_inserted , p . altmetrics . score AS altmetrics_score , p . metrics . times_cited , grid . id AS gridid , grid . name AS gridname , grid . address . country AS gridcountry , grid . address . city AS gridcity , open_access_categories , cat_for . name AS category_for , FROM ` dimensions - ai . data_analytics . publications ` p LEFT JOIN UNNEST ( research_orgs ) AS research_orgs_grids LEFT JOIN ` dimensions - ai . data_analytics . grid ` grid ON grid . id = research_orgs_grids LEFT JOIN UNNEST ( p . open_access_categories_v2 ) AS open_access_categories LEFT JOIN UNNEST ( p . category_for . first_level . full ) AS cat_for WHERE EXTRACT ( YEAR FROM date_inserted ) >= 2020","title":"Query"},{"location":"queries/14/#results","text":"[ { \"id\" : \"pub.1124854415\" , \"title\" : \"Gabinetto armonico pieno d'istromenti sonori\" , \"doi\" : \"10.5479/sil.744616.39088011251444\" , \"year\" : \"1722\" , \"venue\" : null , \"type\" : \"monograph\" , \"date_publication\" : \"1722\" , \"date_inserted\" : \"2020-02-15 01:10:52 UTC\" , \"altmetrics_score\" : null , \"times_cited\" : \"3\" , \"gridid\" : null , \"gridname\" : null , \"gridcountry\" : null , \"gridcity\" : null , \"open_access_categories\" : \"oa_all\" , \"category_for\" : null }, { \"id\" : \"pub.1124854415\" , \"title\" : \"Gabinetto armonico pieno d'istromenti sonori\" , \"doi\" : \"10.5479/sil.744616.39088011251444\" , \"year\" : \"1722\" , \"venue\" : null , \"type\" : \"monograph\" , \"date_publication\" : \"1722\" , \"date_inserted\" : \"2020-02-15 01:10:52 UTC\" , \"altmetrics_score\" : null , \"times_cited\" : \"3\" , \"gridid\" : null , \"gridname\" : null , \"gridcountry\" : null , \"gridcity\" : null , \"open_access_categories\" : \"bronze\" , \"category_for\" : null }, // ma n y more e ntr ies here... ]","title":"Results"},{"location":"queries/15/","text":"15. Top N publications by citations percentile Level: Advanced This query requires a good understanding of SQL and the Dimensions data model Description This query sorts all engineering publications from 2020 by their total citations and returns those in the top 1%. Engineering publications are determined by evaluating Field of Research classifications that are applied to the publications. \"09\" is the top-level code assigned to \"Engineering.\" Query WITH pubs AS ( SELECT p . id as id , p . title . preferred as title , p . citations_count as citations , FROM ` dimensions - ai . data_analytics . publications ` p WHERE year = 2020 AND \"09\" IN UNNEST ( category_for . first_level . codes ) ), ranked_pubs AS ( SELECT p . * , PERCENT_RANK () OVER ( ORDER BY p . citations DESC ) citation_percentile FROM pubs p ) SELECT * FROM ranked_pubs WHERE citation_percentile <= 0 . 01 ORDER BY citation_percentile ASC Results [ { \"id\" : \"pub.1129408972\" , \"title\" : \"Estimation of total flavonoid content in propolis by two complementary colometric methods\" , \"citations\" : \"1014\" , \"citation_percentile\" : \"0.0\" }, { \"id\" : \"pub.1122861707\" , \"title\" : \"Mercury 4.0: from visualization to analysis, design and prediction\" , \"citations\" : \"517\" , \"citation_percentile\" : \"1.4502085399880502E-6\" }, { \"id\" : \"pub.1126110231\" , \"title\" : \"Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks\" , \"citations\" : \"373\" , \"citation_percentile\" : \"2.9004170799761005E-6\" }, { \"id\" : \"pub.1125814051\" , \"title\" : \"Analysis and forecast of COVID-19 spreading in China, Italy and France\" , \"citations\" : \"348\" , \"citation_percentile\" : \"4.350625619964151E-6\" }, { \"id\" : \"pub.1121839330\" , \"title\" : \"A Vision of 6G Wireless Systems: Applications, Trends, Technologies, and Open Research Problems\" , \"citations\" : \"327\" , \"citation_percentile\" : \"5.800834159952201E-6\" }, { \"id\" : \"pub.1125821215\" , \"title\" : \"The Role of Telehealth in Reducing the Mental Health Burden from COVID-19\" , \"citations\" : \"307\" , \"citation_percentile\" : \"7.251042699940251E-6\" }, // ma n y more e ntr ies here... ]","title":"15. Top N publications by citations percentile"},{"location":"queries/15/#15-top-n-publications-by-citations-percentile","text":"Level: Advanced This query requires a good understanding of SQL and the Dimensions data model","title":"15. Top N publications by citations percentile"},{"location":"queries/15/#description","text":"This query sorts all engineering publications from 2020 by their total citations and returns those in the top 1%. Engineering publications are determined by evaluating Field of Research classifications that are applied to the publications. \"09\" is the top-level code assigned to \"Engineering.\"","title":"Description"},{"location":"queries/15/#query","text":"WITH pubs AS ( SELECT p . id as id , p . title . preferred as title , p . citations_count as citations , FROM ` dimensions - ai . data_analytics . publications ` p WHERE year = 2020 AND \"09\" IN UNNEST ( category_for . first_level . codes ) ), ranked_pubs AS ( SELECT p . * , PERCENT_RANK () OVER ( ORDER BY p . citations DESC ) citation_percentile FROM pubs p ) SELECT * FROM ranked_pubs WHERE citation_percentile <= 0 . 01 ORDER BY citation_percentile ASC","title":"Query"},{"location":"queries/15/#results","text":"[ { \"id\" : \"pub.1129408972\" , \"title\" : \"Estimation of total flavonoid content in propolis by two complementary colometric methods\" , \"citations\" : \"1014\" , \"citation_percentile\" : \"0.0\" }, { \"id\" : \"pub.1122861707\" , \"title\" : \"Mercury 4.0: from visualization to analysis, design and prediction\" , \"citations\" : \"517\" , \"citation_percentile\" : \"1.4502085399880502E-6\" }, { \"id\" : \"pub.1126110231\" , \"title\" : \"Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks\" , \"citations\" : \"373\" , \"citation_percentile\" : \"2.9004170799761005E-6\" }, { \"id\" : \"pub.1125814051\" , \"title\" : \"Analysis and forecast of COVID-19 spreading in China, Italy and France\" , \"citations\" : \"348\" , \"citation_percentile\" : \"4.350625619964151E-6\" }, { \"id\" : \"pub.1121839330\" , \"title\" : \"A Vision of 6G Wireless Systems: Applications, Trends, Technologies, and Open Research Problems\" , \"citations\" : \"327\" , \"citation_percentile\" : \"5.800834159952201E-6\" }, { \"id\" : \"pub.1125821215\" , \"title\" : \"The Role of Telehealth in Reducing the Mental Health Burden from COVID-19\" , \"citations\" : \"307\" , \"citation_percentile\" : \"7.251042699940251E-6\" }, // ma n y more e ntr ies here... ]","title":"Results"},{"location":"queries/16/","text":"16. Citations by journal, for a specific publisher Level: Advanced This query requires a good understanding of SQL and the Dimensions data model Description This query returns a list of journals that have cited a publisher's articles in 2020, ordered by how many citations appeared in each journal. Query WITH publisher_pubs AS ( -- get a list of all publication IDs associated with a single publisher SELECT id FROM ` dimensions - ai . data_analytics . publications ` WHERE publisher . id = \"pblshr.1000340\" -- Public Library of Science (PLoS) AND type = \"article\" ) -- then find all publications that CITE that publisher's papers SELECT COUNT ( p . id ) as tot , p . journal . title as journal FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( p . reference_ids ) r WHERE p . year = 2020 AND p . type = \"article\" -- restrict to articles with a published year of 2020 AND p . publisher . id <> \"pblshr.1000340\" -- where the publisher is not the same as the pusblisher above AND r IN ( SELECT id FROM publisher_pubs ) -- the publication must reference a publishers publication GROUP BY journal ORDER BY tot DESC LIMIT 10 Results [ { \"tot\" : \"26309\" , \"journal\" : \"Scientific Reports\" }, { \"tot\" : \"18911\" , \"journal\" : \"International Journal of Molecular Sciences\" }, { \"tot\" : \"8533\" , \"journal\" : \"Frontiers in Microbiology\" }, { \"tot\" : \"7787\" , \"journal\" : \"Frontiers in Immunology\" }, { \"tot\" : \"6999\" , \"journal\" : \"International Journal of Environmental Research and Public Health\" }, { \"tot\" : \"6446\" , \"journal\" : \"Nature Communications\" }, { \"tot\" : \"6199\" , \"journal\" : \"Cells\" }, { \"tot\" : \"5706\" , \"journal\" : \"Cancers\" }, { \"tot\" : \"5036\" , \"journal\" : \"Microorganisms\" }, { \"tot\" : \"5019\" , \"journal\" : \"Nutrients\" } ]","title":"16. Citations by journal, for a specific publisher"},{"location":"queries/16/#16-citations-by-journal-for-a-specific-publisher","text":"Level: Advanced This query requires a good understanding of SQL and the Dimensions data model","title":"16. Citations by journal, for a specific publisher"},{"location":"queries/16/#description","text":"This query returns a list of journals that have cited a publisher's articles in 2020, ordered by how many citations appeared in each journal.","title":"Description"},{"location":"queries/16/#query","text":"WITH publisher_pubs AS ( -- get a list of all publication IDs associated with a single publisher SELECT id FROM ` dimensions - ai . data_analytics . publications ` WHERE publisher . id = \"pblshr.1000340\" -- Public Library of Science (PLoS) AND type = \"article\" ) -- then find all publications that CITE that publisher's papers SELECT COUNT ( p . id ) as tot , p . journal . title as journal FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( p . reference_ids ) r WHERE p . year = 2020 AND p . type = \"article\" -- restrict to articles with a published year of 2020 AND p . publisher . id <> \"pblshr.1000340\" -- where the publisher is not the same as the pusblisher above AND r IN ( SELECT id FROM publisher_pubs ) -- the publication must reference a publishers publication GROUP BY journal ORDER BY tot DESC LIMIT 10","title":"Query"},{"location":"queries/16/#results","text":"[ { \"tot\" : \"26309\" , \"journal\" : \"Scientific Reports\" }, { \"tot\" : \"18911\" , \"journal\" : \"International Journal of Molecular Sciences\" }, { \"tot\" : \"8533\" , \"journal\" : \"Frontiers in Microbiology\" }, { \"tot\" : \"7787\" , \"journal\" : \"Frontiers in Immunology\" }, { \"tot\" : \"6999\" , \"journal\" : \"International Journal of Environmental Research and Public Health\" }, { \"tot\" : \"6446\" , \"journal\" : \"Nature Communications\" }, { \"tot\" : \"6199\" , \"journal\" : \"Cells\" }, { \"tot\" : \"5706\" , \"journal\" : \"Cancers\" }, { \"tot\" : \"5036\" , \"journal\" : \"Microorganisms\" }, { \"tot\" : \"5019\" , \"journal\" : \"Nutrients\" } ]","title":"Results"},{"location":"queries/17/","text":"17. One-degree citation network for a single publication Level: Advanced This query requires a good understanding of SQL and the Dimensions data model Description This query generates a basic (incoming) citation network for a single publication (the \"root node\"). \"Level 1\" publications in the query are publications that are citing the root publication. \"Level 2\" publications are citing Level 1 publications. Each row describes a vertex in the network and what year the citation occurred. Query WITH level1 AS ( SELECT \"pub.1099396382\" as citation_from , citations . id AS citation_to , 1 AS level , citations . year as citation_year FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) AS citations WHERE p . id = \"pub.1099396382\" -- starting node defined here ), level2 AS ( SELECT l . citation_to AS citation_from , citations . id AS citation_to , 2 AS level , citations . year AS citation_year FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) as citations , level1 l where p . id = l . citation_to ) SELECT * from level1 UNION ALL SELECT * from level2 Results [ { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1135701707\" , \"level\" : \"2\" , \"citation_year\" : \"2021\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1126671825\" , \"level\" : \"2\" , \"citation_year\" : \"2020\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1101037901\" , \"level\" : \"2\" , \"citation_year\" : \"2018\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1120764290\" , \"level\" : \"2\" , \"citation_year\" : \"2019\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1103943561\" , \"level\" : \"2\" , \"citation_year\" : \"2018\" }, // ma n y more e ntr ies here... ] 17.1 Variant: one-degree references network for a single publication We could use the same approach in order to build a references network (=outgoing citations). This can be achieved via the publications field references_ids . WITH level1 AS ( SELECT \"pub.1099396382\" as references_from , reference AS reference_to , 1 AS level FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( reference_ids ) AS reference WHERE p . id = \"pub.1099396382\" -- starting node defined here ), level2 AS ( SELECT l . reference_to AS reference_from , reference AS reference_to , 2 AS level FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( reference_ids ) as reference , level1 l where p . id = l . reference_to ) SELECT * from level1 UNION ALL SELECT * from level2","title":"17. One-degree citation network for a single publication"},{"location":"queries/17/#17-one-degree-citation-network-for-a-single-publication","text":"Level: Advanced This query requires a good understanding of SQL and the Dimensions data model","title":"17. One-degree citation network for a single publication"},{"location":"queries/17/#description","text":"This query generates a basic (incoming) citation network for a single publication (the \"root node\"). \"Level 1\" publications in the query are publications that are citing the root publication. \"Level 2\" publications are citing Level 1 publications. Each row describes a vertex in the network and what year the citation occurred.","title":"Description"},{"location":"queries/17/#query","text":"WITH level1 AS ( SELECT \"pub.1099396382\" as citation_from , citations . id AS citation_to , 1 AS level , citations . year as citation_year FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) AS citations WHERE p . id = \"pub.1099396382\" -- starting node defined here ), level2 AS ( SELECT l . citation_to AS citation_from , citations . id AS citation_to , 2 AS level , citations . year AS citation_year FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) as citations , level1 l where p . id = l . citation_to ) SELECT * from level1 UNION ALL SELECT * from level2","title":"Query"},{"location":"queries/17/#results","text":"[ { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1135701707\" , \"level\" : \"2\" , \"citation_year\" : \"2021\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1126671825\" , \"level\" : \"2\" , \"citation_year\" : \"2020\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1101037901\" , \"level\" : \"2\" , \"citation_year\" : \"2018\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1120764290\" , \"level\" : \"2\" , \"citation_year\" : \"2019\" }, { \"citation_from\" : \"pub.1084215961\" , \"citation_to\" : \"pub.1103943561\" , \"level\" : \"2\" , \"citation_year\" : \"2018\" }, // ma n y more e ntr ies here... ]","title":"Results"},{"location":"queries/17/#171-variant-one-degree-references-network-for-a-single-publication","text":"We could use the same approach in order to build a references network (=outgoing citations). This can be achieved via the publications field references_ids . WITH level1 AS ( SELECT \"pub.1099396382\" as references_from , reference AS reference_to , 1 AS level FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( reference_ids ) AS reference WHERE p . id = \"pub.1099396382\" -- starting node defined here ), level2 AS ( SELECT l . reference_to AS reference_from , reference AS reference_to , 2 AS level FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( reference_ids ) as reference , level1 l where p . id = l . reference_to ) SELECT * from level1 UNION ALL SELECT * from level2","title":"17.1 Variant: one-degree references network for a single publication"},{"location":"queries/18/","text":"18. Incoming citations for a journal Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query lists incoming citations per year for a single journal. Results are split out by year and by the type of publication that cited the journal (article, book, etc). Note There is an important clause in the SELECT statement that changes the behavior of this query: If you use COUNT(DISTINCT id) , the query counts unique publications that cited the selected journal. If you use COUNT(id) instead, this counts citations : If one publication cites multiple papers from a single journal, the latter query will count each citation separately. Query SELECT COUNT ( DISTINCT id ) AS totcount , year , type FROM ` dimensions - ai . data_analytics . publications ` WHERE id IN ( SELECT citing_pubs . id FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( citations ) AS citing_pubs WHERE journal . id = \"jour.1115214\" -- Nature Biotechnology ) AND year >= 2005 GROUP BY year , type ORDER BY year , type Results [ { \"totcount\" : \"13064\" , \"year\" : \"2005\" , \"type\" : \"article\" }, { \"totcount\" : \"12\" , \"year\" : \"2005\" , \"type\" : \"book\" }, { \"totcount\" : \"1492\" , \"year\" : \"2005\" , \"type\" : \"chapter\" }, { \"totcount\" : \"23\" , \"year\" : \"2005\" , \"type\" : \"monograph\" }, { \"totcount\" : \"192\" , \"year\" : \"2005\" , \"type\" : \"proceeding\" }, // more e ntr ies here... ]","title":"18. Incoming citations for a journal"},{"location":"queries/18/#18-incoming-citations-for-a-journal","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"18. Incoming citations for a journal"},{"location":"queries/18/#description","text":"This query lists incoming citations per year for a single journal. Results are split out by year and by the type of publication that cited the journal (article, book, etc). Note There is an important clause in the SELECT statement that changes the behavior of this query: If you use COUNT(DISTINCT id) , the query counts unique publications that cited the selected journal. If you use COUNT(id) instead, this counts citations : If one publication cites multiple papers from a single journal, the latter query will count each citation separately.","title":"Description"},{"location":"queries/18/#query","text":"SELECT COUNT ( DISTINCT id ) AS totcount , year , type FROM ` dimensions - ai . data_analytics . publications ` WHERE id IN ( SELECT citing_pubs . id FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( citations ) AS citing_pubs WHERE journal . id = \"jour.1115214\" -- Nature Biotechnology ) AND year >= 2005 GROUP BY year , type ORDER BY year , type","title":"Query"},{"location":"queries/18/#results","text":"[ { \"totcount\" : \"13064\" , \"year\" : \"2005\" , \"type\" : \"article\" }, { \"totcount\" : \"12\" , \"year\" : \"2005\" , \"type\" : \"book\" }, { \"totcount\" : \"1492\" , \"year\" : \"2005\" , \"type\" : \"chapter\" }, { \"totcount\" : \"23\" , \"year\" : \"2005\" , \"type\" : \"monograph\" }, { \"totcount\" : \"192\" , \"year\" : \"2005\" , \"type\" : \"proceeding\" }, // more e ntr ies here... ]","title":"Results"},{"location":"queries/19/","text":"19. Outgoing citations from a journal Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query counts outgoing citations per year from a single journal. Results are broken down by year and by the type of publication being cited (article, book, etc). Note There is an important clause in the SELECT statement that changes the behavior of this query: If you use COUNT(DISTINCT id) , the query counts unique publications that have been cited in the selected journal. If you use COUNT(id) instead, this counts citations : If one publication it cited by multiple papers in a single journal, the latter query will count each citation separately. Query SELECT COUNT ( DISTINCT id ) AS totcount , year , type FROM ` dimensions - ai . data_analytics . publications ` WHERE id IN ( SELECT DISTINCT reference_pubs FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( reference_ids ) AS reference_pubs WHERE journal . id = \"jour.1115214\" -- Nature Biotechnology ) AND year >= 2005 GROUP BY year , type ORDER BY year , type Results [ { \"totcount\" : \"3757\" , \"year\" : \"2005\" , \"type\" : \"article\" }, { \"totcount\" : \"12\" , \"year\" : \"2005\" , \"type\" : \"book\" }, { \"totcount\" : \"60\" , \"year\" : \"2005\" , \"type\" : \"chapter\" }, { \"totcount\" : \"9\" , \"year\" : \"2005\" , \"type\" : \"monograph\" }, { \"totcount\" : \"8\" , \"year\" : \"2005\" , \"type\" : \"proceeding\" }, // more e ntr ies here... ]","title":"19. Outgoing citations from a journal"},{"location":"queries/19/#19-outgoing-citations-from-a-journal","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"19. Outgoing citations from a journal"},{"location":"queries/19/#description","text":"This query counts outgoing citations per year from a single journal. Results are broken down by year and by the type of publication being cited (article, book, etc). Note There is an important clause in the SELECT statement that changes the behavior of this query: If you use COUNT(DISTINCT id) , the query counts unique publications that have been cited in the selected journal. If you use COUNT(id) instead, this counts citations : If one publication it cited by multiple papers in a single journal, the latter query will count each citation separately.","title":"Description"},{"location":"queries/19/#query","text":"SELECT COUNT ( DISTINCT id ) AS totcount , year , type FROM ` dimensions - ai . data_analytics . publications ` WHERE id IN ( SELECT DISTINCT reference_pubs FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( reference_ids ) AS reference_pubs WHERE journal . id = \"jour.1115214\" -- Nature Biotechnology ) AND year >= 2005 GROUP BY year , type ORDER BY year , type","title":"Query"},{"location":"queries/19/#results","text":"[ { \"totcount\" : \"3757\" , \"year\" : \"2005\" , \"type\" : \"article\" }, { \"totcount\" : \"12\" , \"year\" : \"2005\" , \"type\" : \"book\" }, { \"totcount\" : \"60\" , \"year\" : \"2005\" , \"type\" : \"chapter\" }, { \"totcount\" : \"9\" , \"year\" : \"2005\" , \"type\" : \"monograph\" }, { \"totcount\" : \"8\" , \"year\" : \"2005\" , \"type\" : \"proceeding\" }, // more e ntr ies here... ]","title":"Results"},{"location":"queries/20/","text":"20. International collaboration of an organisation in a field Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query looks at international collaborations by year, with additional filters for institution and Field of Research . The pubcounts subquery counts the total number of relevant papers that have authors from multiple countries, then the final query divides this number by the total number of relevant papers in that category. Query WITH pubcounts AS ( SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND \"0601\" in UNNEST ( category_for . second_level . codes ) -- field defined here AND \"grid.4991.5\" in UNNEST ( research_orgs ) -- org defined here GROUP BY year ORDER BY year DESC ) SELECT pubcounts . year , pubcounts . intl , pubcounts . domestic , ROUND ( ( pubcounts . intl * 100 ) / ( pubcounts . domestic + pubcounts . intl ) , 1 ) AS percentagecollab FROM pubcounts ORDER BY year DESC Results [ { \"year\" : \"2021\" , \"intl\" : \"184\" , \"domestic\" : \"92\" , \"percentagecollab\" : \"66.7\" }, { \"year\" : \"2020\" , \"intl\" : \"606\" , \"domestic\" : \"307\" , \"percentagecollab\" : \"66.4\" }, { \"year\" : \"2019\" , \"intl\" : \"534\" , \"domestic\" : \"262\" , \"percentagecollab\" : \"67.1\" }, { \"year\" : \"2018\" , \"intl\" : \"471\" , \"domestic\" : \"246\" , \"percentagecollab\" : \"65.7\" }, { \"year\" : \"2017\" , \"intl\" : \"460\" , \"domestic\" : \"277\" , \"percentagecollab\" : \"62.4\" }, { \"year\" : \"2016\" , \"intl\" : \"422\" , \"domestic\" : \"235\" , \"percentagecollab\" : \"64.2\" }, { \"year\" : \"2015\" , \"intl\" : \"369\" , \"domestic\" : \"268\" , \"percentagecollab\" : \"57.9\" } ]","title":"20. International collaboration of an organisation in a field"},{"location":"queries/20/#20-international-collaboration-of-an-organisation-in-a-field","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"20. International collaboration of an organisation in a field"},{"location":"queries/20/#description","text":"This query looks at international collaborations by year, with additional filters for institution and Field of Research . The pubcounts subquery counts the total number of relevant papers that have authors from multiple countries, then the final query divides this number by the total number of relevant papers in that category.","title":"Description"},{"location":"queries/20/#query","text":"WITH pubcounts AS ( SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND \"0601\" in UNNEST ( category_for . second_level . codes ) -- field defined here AND \"grid.4991.5\" in UNNEST ( research_orgs ) -- org defined here GROUP BY year ORDER BY year DESC ) SELECT pubcounts . year , pubcounts . intl , pubcounts . domestic , ROUND ( ( pubcounts . intl * 100 ) / ( pubcounts . domestic + pubcounts . intl ) , 1 ) AS percentagecollab FROM pubcounts ORDER BY year DESC","title":"Query"},{"location":"queries/20/#results","text":"[ { \"year\" : \"2021\" , \"intl\" : \"184\" , \"domestic\" : \"92\" , \"percentagecollab\" : \"66.7\" }, { \"year\" : \"2020\" , \"intl\" : \"606\" , \"domestic\" : \"307\" , \"percentagecollab\" : \"66.4\" }, { \"year\" : \"2019\" , \"intl\" : \"534\" , \"domestic\" : \"262\" , \"percentagecollab\" : \"67.1\" }, { \"year\" : \"2018\" , \"intl\" : \"471\" , \"domestic\" : \"246\" , \"percentagecollab\" : \"65.7\" }, { \"year\" : \"2017\" , \"intl\" : \"460\" , \"domestic\" : \"277\" , \"percentagecollab\" : \"62.4\" }, { \"year\" : \"2016\" , \"intl\" : \"422\" , \"domestic\" : \"235\" , \"percentagecollab\" : \"64.2\" }, { \"year\" : \"2015\" , \"intl\" : \"369\" , \"domestic\" : \"268\" , \"percentagecollab\" : \"57.9\" } ]","title":"Results"},{"location":"queries/21/","text":"21. International collaboration rate of individuals, with context Level: Advanced This query requires a good understanding of SQL and the Dimensions data model Description This query determines the yearly proportion of publications from a single author that include international collaborators. It also calculates the same rate for the author's current institution (for papers in the same field), and the author's current country. A few highlights: We can simplify the query by collecting all the author-specific data up front, in the researcher_details and researcher_field subqueries, and referring to it later simply as something like (SELECT org FROM researcher_details) . When we calculate the percentage of papers that are international collaborations, it would be much simpler to simply write (intl*100) / (intl + domestic) . However, in situations where there are zero papers returned for that particular category, this will return an error because the query would call for dividing by zero. We can avoid this by using the COALESCE function . The COUNTIF function is used multiple times here\u2014it can be helpful in situations where you want to maintain separate counts for different conditionals without using COUNT and lots of separate subqueries. Query WITH researcher_details AS ( -- grab the basic metadata about the selected researcher SELECT r . id , r . current_research_org AS org , grid . address . country FROM ` dimensions - ai . data_analytics . researchers ` r INNER JOIN ` dimensions - ai . data_analytics . grid ` grid ON r . current_research_org = grid . id WHERE r . id = \"ur.0761121015.96\" -- researcher defined here ), researcher_field AS ( -- determines the field of research code in which -- the researcher has most frequently authored papers SELECT for2 , COUNT ( DISTINCT p . id ) FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( category_for . second_level . codes ) for2 WHERE ( SELECT id FROM researcher_details ) IN UNNEST ( researcher_ids ) GROUP BY 1 ORDER BY 2 DESC LIMIT 1 ), counts_researcher AS ( -- count how many publications from the selected researcher -- include authors from multiple countries SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND ( SELECT id FROM researcher_details ) IN UNNEST ( researcher_ids ) GROUP BY year ), counts_org AS ( -- Count how many publications from the selected researcher's -- CURRENT ORGANIZATION that include authors from multiple countries. -- We count only publications in the author's primary field of -- research, and EXCLUDE papers they co-authored. SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND ( SELECT org FROM researcher_details ) IN UNNEST ( research_orgs ) AND ( SELECT for2 FROM researcher_field ) IN UNNEST ( category_for . second_level . codes ) AND ( SELECT id FROM researcher_details ) NOT IN UNNEST ( researcher_ids ) GROUP BY year ), counts_country AS ( -- Count how many publications from the selected researcher's -- current COUNTRY that include authors from multiple countries. -- We count only publications in the author's primary field of -- research, and EXCLUDE papers they co-authored. SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND ( SELECT country FROM researcher_details ) IN UNNEST ( research_org_country_names ) AND ( SELECT for2 FROM researcher_field ) IN UNNEST ( category_for . second_level . codes ) AND ( SELECT id FROM researcher_details ) NOT IN UNNEST ( researcher_ids ) GROUP BY year ), raw_percents AS ( -- Divide international collabs by total collabs in each category SELECT researcher . year , ( researcher . intl * 100 ) / COALESCE ( researcher . intl + researcher . domestic , 1 ) AS intl_researcher , ( org . intl * 100 ) / COALESCE ( org . intl + org . domestic , 1 ) AS intl_org , ( country . intl * 100 ) / COALESCE ( country . intl + country . domestic , 1 ) AS intl_country , FROM counts_researcher researcher LEFT JOIN counts_org org ON researcher . year = org . year LEFT JOIN counts_country country ON researcher . year = country . year ORDER BY researcher . year DESC ) -- Pull the percentages from the raw_percents table and round them SELECT year , ROUND ( intl_researcher , 1 ) AS intl_researcher , ROUND ( intl_org , 1 ) AS intl_org , ROUND ( intl_country , 1 ) AS intl_country FROM raw_percents Results [ { \"year\" : \"2021\" , \"intl_researcher\" : \"44.4\" , \"intl_org\" : \"53.8\" , \"intl_country\" : \"59.0\" }, { \"year\" : \"2020\" , \"intl_researcher\" : \"50.0\" , \"intl_org\" : \"54.7\" , \"intl_country\" : \"48.7\" }, { \"year\" : \"2019\" , \"intl_researcher\" : \"53.6\" , \"intl_org\" : \"48.9\" , \"intl_country\" : \"45.8\" }, { \"year\" : \"2018\" , \"intl_researcher\" : \"57.6\" , \"intl_org\" : \"49.5\" , \"intl_country\" : \"42.5\" }, { \"year\" : \"2017\" , \"intl_researcher\" : \"53.3\" , \"intl_org\" : \"42.9\" , \"intl_country\" : \"43.4\" }, { \"year\" : \"2016\" , \"intl_researcher\" : \"40.6\" , \"intl_org\" : \"37.4\" , \"intl_country\" : \"39.4\" }, { \"year\" : \"2015\" , \"intl_researcher\" : \"38.5\" , \"intl_org\" : \"41.5\" , \"intl_country\" : \"40.2\" } ]","title":"21. International collaboration rate of individuals, with context"},{"location":"queries/21/#21-international-collaboration-rate-of-individuals-with-context","text":"Level: Advanced This query requires a good understanding of SQL and the Dimensions data model","title":"21. International collaboration rate of individuals, with context"},{"location":"queries/21/#description","text":"This query determines the yearly proportion of publications from a single author that include international collaborators. It also calculates the same rate for the author's current institution (for papers in the same field), and the author's current country. A few highlights: We can simplify the query by collecting all the author-specific data up front, in the researcher_details and researcher_field subqueries, and referring to it later simply as something like (SELECT org FROM researcher_details) . When we calculate the percentage of papers that are international collaborations, it would be much simpler to simply write (intl*100) / (intl + domestic) . However, in situations where there are zero papers returned for that particular category, this will return an error because the query would call for dividing by zero. We can avoid this by using the COALESCE function . The COUNTIF function is used multiple times here\u2014it can be helpful in situations where you want to maintain separate counts for different conditionals without using COUNT and lots of separate subqueries.","title":"Description"},{"location":"queries/21/#query","text":"WITH researcher_details AS ( -- grab the basic metadata about the selected researcher SELECT r . id , r . current_research_org AS org , grid . address . country FROM ` dimensions - ai . data_analytics . researchers ` r INNER JOIN ` dimensions - ai . data_analytics . grid ` grid ON r . current_research_org = grid . id WHERE r . id = \"ur.0761121015.96\" -- researcher defined here ), researcher_field AS ( -- determines the field of research code in which -- the researcher has most frequently authored papers SELECT for2 , COUNT ( DISTINCT p . id ) FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( category_for . second_level . codes ) for2 WHERE ( SELECT id FROM researcher_details ) IN UNNEST ( researcher_ids ) GROUP BY 1 ORDER BY 2 DESC LIMIT 1 ), counts_researcher AS ( -- count how many publications from the selected researcher -- include authors from multiple countries SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND ( SELECT id FROM researcher_details ) IN UNNEST ( researcher_ids ) GROUP BY year ), counts_org AS ( -- Count how many publications from the selected researcher's -- CURRENT ORGANIZATION that include authors from multiple countries. -- We count only publications in the author's primary field of -- research, and EXCLUDE papers they co-authored. SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND ( SELECT org FROM researcher_details ) IN UNNEST ( research_orgs ) AND ( SELECT for2 FROM researcher_field ) IN UNNEST ( category_for . second_level . codes ) AND ( SELECT id FROM researcher_details ) NOT IN UNNEST ( researcher_ids ) GROUP BY year ), counts_country AS ( -- Count how many publications from the selected researcher's -- current COUNTRY that include authors from multiple countries. -- We count only publications in the author's primary field of -- research, and EXCLUDE papers they co-authored. SELECT year , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) > 1 ) AS intl , COUNTIF ( ARRAY_LENGTH ( p . research_org_countries ) <= 1 ) AS domestic FROM ` dimensions - ai . data_analytics . publications ` p WHERE year >= 2015 AND ( SELECT country FROM researcher_details ) IN UNNEST ( research_org_country_names ) AND ( SELECT for2 FROM researcher_field ) IN UNNEST ( category_for . second_level . codes ) AND ( SELECT id FROM researcher_details ) NOT IN UNNEST ( researcher_ids ) GROUP BY year ), raw_percents AS ( -- Divide international collabs by total collabs in each category SELECT researcher . year , ( researcher . intl * 100 ) / COALESCE ( researcher . intl + researcher . domestic , 1 ) AS intl_researcher , ( org . intl * 100 ) / COALESCE ( org . intl + org . domestic , 1 ) AS intl_org , ( country . intl * 100 ) / COALESCE ( country . intl + country . domestic , 1 ) AS intl_country , FROM counts_researcher researcher LEFT JOIN counts_org org ON researcher . year = org . year LEFT JOIN counts_country country ON researcher . year = country . year ORDER BY researcher . year DESC ) -- Pull the percentages from the raw_percents table and round them SELECT year , ROUND ( intl_researcher , 1 ) AS intl_researcher , ROUND ( intl_org , 1 ) AS intl_org , ROUND ( intl_country , 1 ) AS intl_country FROM raw_percents","title":"Query"},{"location":"queries/21/#results","text":"[ { \"year\" : \"2021\" , \"intl_researcher\" : \"44.4\" , \"intl_org\" : \"53.8\" , \"intl_country\" : \"59.0\" }, { \"year\" : \"2020\" , \"intl_researcher\" : \"50.0\" , \"intl_org\" : \"54.7\" , \"intl_country\" : \"48.7\" }, { \"year\" : \"2019\" , \"intl_researcher\" : \"53.6\" , \"intl_org\" : \"48.9\" , \"intl_country\" : \"45.8\" }, { \"year\" : \"2018\" , \"intl_researcher\" : \"57.6\" , \"intl_org\" : \"49.5\" , \"intl_country\" : \"42.5\" }, { \"year\" : \"2017\" , \"intl_researcher\" : \"53.3\" , \"intl_org\" : \"42.9\" , \"intl_country\" : \"43.4\" }, { \"year\" : \"2016\" , \"intl_researcher\" : \"40.6\" , \"intl_org\" : \"37.4\" , \"intl_country\" : \"39.4\" }, { \"year\" : \"2015\" , \"intl_researcher\" : \"38.5\" , \"intl_org\" : \"41.5\" , \"intl_country\" : \"40.2\" } ]","title":"Results"},{"location":"queries/22/","text":"22. Incoming citations for a single publication, by journal Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query counts incoming citations for a single publication. Results are grouped by journal, and include the publisher of each journal in the list. The publications table has a citations field that includes the publication IDs of all incoming citations for a given paper. We fetch that list in the citing subquery below, then query the publications table for information about all the IDs in that list. The COALESCE function is used here to minimize the number of null fields in the final results\u2014if an incoming citation is published in a book rather than a journal, for example, then the journal.title field will be NULL , and the boook_title.preferred field is likely to have the value we want. Occasionally, the publisher field is unavailable, so we use COALESCE(p.publisher.name, \"(unknown)\") to make sure there aren't any blank fields. Note The list of citing publications is determined by the clause WHERE p.id='pub.1113640622' . This can be changed to be as broad or narrow as you wish\u2014changing it to something like WHERE journal.title='eLife' , for example, would return incoming citations to an entire journal rather than a single paper. Query WITH citing AS ( SELECT citing_pubs . id FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) AS citing_pubs WHERE p . id = 'pub.1113640622' -- publication of interest here ) SELECT COALESCE ( p . journal . title , CONCAT ( p . book_title . preferred , ' (book)' ), p . proceedings_title . preferred , CONCAT ( p . title . preferred , ' (book)' ) -- some books have this field instead of book_title ) AS journal , COALESCE ( p . publisher . name , \"(unknown)\" ) AS publisher , p . type AS pubtype , COUNT ( p . id ) AS citations FROM ` dimensions - ai . data_analytics . publications ` p WHERE p . id IN ( SELECT id FROM citing ) GROUP BY 1 , 2 , 3 ORDER BY 4 DESC Results [ { \"journal\" : \"bioRxiv\" , \"publisher\" : \"Cold Spring Harbor Laboratory\" , \"pubtype\" : \"preprint\" , \"citations\" : \"15\" }, { \"journal\" : \"PLOS Biology\" , \"publisher\" : \"Public Library of Science (PLoS)\" , \"pubtype\" : \"article\" , \"citations\" : \"5\" }, { \"journal\" : \"eLife\" , \"publisher\" : \"eLife\" , \"pubtype\" : \"article\" , \"citations\" : \"3\" }, { \"journal\" : \"medRxiv\" , \"publisher\" : \"Cold Spring Harbor Laboratory\" , \"pubtype\" : \"preprint\" , \"citations\" : \"3\" }, { \"journal\" : \"Scientometrics\" , \"publisher\" : \"Springer Nature\" , \"pubtype\" : \"article\" , \"citations\" : \"3\" }, // more e ntr ies here... ]","title":"22. Incoming citations for a single publication, by journal"},{"location":"queries/22/#22-incoming-citations-for-a-single-publication-by-journal","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"22. Incoming citations for a single publication, by journal"},{"location":"queries/22/#description","text":"This query counts incoming citations for a single publication. Results are grouped by journal, and include the publisher of each journal in the list. The publications table has a citations field that includes the publication IDs of all incoming citations for a given paper. We fetch that list in the citing subquery below, then query the publications table for information about all the IDs in that list. The COALESCE function is used here to minimize the number of null fields in the final results\u2014if an incoming citation is published in a book rather than a journal, for example, then the journal.title field will be NULL , and the boook_title.preferred field is likely to have the value we want. Occasionally, the publisher field is unavailable, so we use COALESCE(p.publisher.name, \"(unknown)\") to make sure there aren't any blank fields. Note The list of citing publications is determined by the clause WHERE p.id='pub.1113640622' . This can be changed to be as broad or narrow as you wish\u2014changing it to something like WHERE journal.title='eLife' , for example, would return incoming citations to an entire journal rather than a single paper.","title":"Description"},{"location":"queries/22/#query","text":"WITH citing AS ( SELECT citing_pubs . id FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) AS citing_pubs WHERE p . id = 'pub.1113640622' -- publication of interest here ) SELECT COALESCE ( p . journal . title , CONCAT ( p . book_title . preferred , ' (book)' ), p . proceedings_title . preferred , CONCAT ( p . title . preferred , ' (book)' ) -- some books have this field instead of book_title ) AS journal , COALESCE ( p . publisher . name , \"(unknown)\" ) AS publisher , p . type AS pubtype , COUNT ( p . id ) AS citations FROM ` dimensions - ai . data_analytics . publications ` p WHERE p . id IN ( SELECT id FROM citing ) GROUP BY 1 , 2 , 3 ORDER BY 4 DESC","title":"Query"},{"location":"queries/22/#results","text":"[ { \"journal\" : \"bioRxiv\" , \"publisher\" : \"Cold Spring Harbor Laboratory\" , \"pubtype\" : \"preprint\" , \"citations\" : \"15\" }, { \"journal\" : \"PLOS Biology\" , \"publisher\" : \"Public Library of Science (PLoS)\" , \"pubtype\" : \"article\" , \"citations\" : \"5\" }, { \"journal\" : \"eLife\" , \"publisher\" : \"eLife\" , \"pubtype\" : \"article\" , \"citations\" : \"3\" }, { \"journal\" : \"medRxiv\" , \"publisher\" : \"Cold Spring Harbor Laboratory\" , \"pubtype\" : \"preprint\" , \"citations\" : \"3\" }, { \"journal\" : \"Scientometrics\" , \"publisher\" : \"Springer Nature\" , \"pubtype\" : \"article\" , \"citations\" : \"3\" }, // more e ntr ies here... ]","title":"Results"},{"location":"queries/23/","text":"23. Citing authors by country Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query counts incoming citations for a single publication. The final results count authors , rather than publications, and group counts by the country of each author's current affiliation. The strategy is similar to query 22 , which groups citations by journal: the publications table has a citations field that includes the publication IDs of all incoming citations for a given paper. We fetch that list in the citing subquery below, then query the publications table to get a list of authors for all the publications citing the paper of interest. We join the researchers table to get each author's current affiliation. In the final query, we use the grid table to associate each author affiliation to a single country , then group all results by those countries. Note The list of citing publications is determined by the clause WHERE p.id='pub.1113640622' . This can be changed to be as broad or narrow as you wish\u2014changing it to something like WHERE journal.title='eLife' , for example, would return incoming citations to an entire journal rather than a single paper. Query WITH citing AS ( SELECT citing_pubs . id FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) AS citing_pubs WHERE p . id = 'pub.1113640622' -- publication of interest ), people_and_grids as ( SELECT COUNT ( DISTINCT auth . researcher_id ) AS people , res . current_research_org AS gridid FROM ` dimensions - ai . data_analytics . publications ` pubs CROSS JOIN UNNEST ( authors ) as auth INNER JOIN ` dimensions - ai . data_analytics . researchers ` res ON res . id = auth . researcher_id WHERE pubs . id IN ( SELECT id FROM citing ) GROUP BY gridid ) SELECT people , address . country FROM people_and_grids INNER JOIN ` dimensions - ai . data_analytics . grid ` gridinfo ON gridinfo . id = people_and_grids . gridid ORDER BY people DESC Results [ { \"people\" : \"10\" , \"country\" : \"Brazil\" }, { \"people\" : \"5\" , \"country\" : \"United States\" }, { \"people\" : \"5\" , \"country\" : \"United States\" }, { \"people\" : \"5\" , \"country\" : \"United States\" }, { \"people\" : \"4\" , \"country\" : \"Croatia\" }, // more resul ts here... ]","title":"23. Citing authors by country"},{"location":"queries/23/#23-citing-authors-by-country","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"23. Citing authors by country"},{"location":"queries/23/#description","text":"This query counts incoming citations for a single publication. The final results count authors , rather than publications, and group counts by the country of each author's current affiliation. The strategy is similar to query 22 , which groups citations by journal: the publications table has a citations field that includes the publication IDs of all incoming citations for a given paper. We fetch that list in the citing subquery below, then query the publications table to get a list of authors for all the publications citing the paper of interest. We join the researchers table to get each author's current affiliation. In the final query, we use the grid table to associate each author affiliation to a single country , then group all results by those countries. Note The list of citing publications is determined by the clause WHERE p.id='pub.1113640622' . This can be changed to be as broad or narrow as you wish\u2014changing it to something like WHERE journal.title='eLife' , for example, would return incoming citations to an entire journal rather than a single paper.","title":"Description"},{"location":"queries/23/#query","text":"WITH citing AS ( SELECT citing_pubs . id FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( citations ) AS citing_pubs WHERE p . id = 'pub.1113640622' -- publication of interest ), people_and_grids as ( SELECT COUNT ( DISTINCT auth . researcher_id ) AS people , res . current_research_org AS gridid FROM ` dimensions - ai . data_analytics . publications ` pubs CROSS JOIN UNNEST ( authors ) as auth INNER JOIN ` dimensions - ai . data_analytics . researchers ` res ON res . id = auth . researcher_id WHERE pubs . id IN ( SELECT id FROM citing ) GROUP BY gridid ) SELECT people , address . country FROM people_and_grids INNER JOIN ` dimensions - ai . data_analytics . grid ` gridinfo ON gridinfo . id = people_and_grids . gridid ORDER BY people DESC","title":"Query"},{"location":"queries/23/#results","text":"[ { \"people\" : \"10\" , \"country\" : \"Brazil\" }, { \"people\" : \"5\" , \"country\" : \"United States\" }, { \"people\" : \"5\" , \"country\" : \"United States\" }, { \"people\" : \"5\" , \"country\" : \"United States\" }, { \"people\" : \"4\" , \"country\" : \"Croatia\" }, // more resul ts here... ]","title":"Results"},{"location":"queries/24/","text":"24. Organizations and sub-organizations Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query shows how to extract information about a GRID organization together with all of its sub-organizations. Many organizations in the GRID database include parent-child relationships. For example, grid.495456.f (the United States Department of the Air Force page) has both parent and children institutions, so one can use the hierarchy when querying related data e.g. the total number of publications for each of these organizations. The query below shows how to leverage the organization_recursive_child_ids field in the grid table in order to achieve that. This field is prepopulated with all children institutions GRID IDs (recursively), hence it makes it easier to run this type of analyses. Query WITH hierarchy AS ( SELECT g . id AS parent , g . name AS parent_name , children , g2 . name AS children_name , FROM ` dimensions - ai . data_analytics . grid ` g CROSS JOIN UNNEST ( organization_recursive_child_ids ) AS children INNER JOIN ` dimensions - ai . data_analytics . grid ` g2 ON g2 . id = children WHERE g . id = \"grid.495456.f\" -- United States Department of the Air Force ) SELECT hierarchy . * , COUNT ( DISTINCT p . id ) as pubs FROM hierarchy INNER JOIN ` dimensions - ai . data_analytics . publications ` p on hierarchy . children in UNNEST ( p . research_orgs ) GROUP by 1 , 2 , 3 , 4 Breaking it down The key part of the query uses a CROSS JOIN on the organization_recursive_child_ids field to retrieve all the descendants of the chosen organization: SELECT g . id AS parent , children , FROM ` dimensions - ai . data_analytics . grid ` g CROSS JOIN UNNEST ( organization_recursive_child_ids ) AS children WHERE g . id = \"grid.495456.f\" Furthermore, in order to get more organization metadata e.g. the name, an inner self-join is introduced: SELECT g . id AS parent , g . name AS parent_name , children , g2 . name AS children_name , FROM ` dimensions - ai . data_analytics . grid ` g CROSS JOIN UNNEST ( organization_recursive_child_ids ) AS children INNER JOIN ` dimensions - ai . data_analytics . grid ` g2 ON g2 . id = children WHERE g . id = \"grid.495456.f\" The final step is to join also the publications table, so to get the total publications count for each organization. WITH hierarchy AS ( -- the grid query above ) SELECT hierarchy . * , COUNT ( DISTINCT p . id ) as pubs FROM hierarchy INNER JOIN ` dimensions - ai . data_analytics . publications ` p on hierarchy . children in UNNEST ( p . research_orgs ) GROUP by 1 , 2 , 3 , 4 Results [ { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.494596.3\" , \"children_name\" : \"Edwards Air Force Base\" , \"pubs\" : \"467\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.461685.8\" , \"children_name\" : \"Joint Base San Antonio\" , \"pubs\" : \"1031\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.265457.7\" , \"children_name\" : \"United States Air Force Academy\" , \"pubs\" : \"3137\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.431316.2\" , \"children_name\" : \"Grand Forks Air Force Base\" , \"pubs\" : \"4\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.499282.c\" , \"children_name\" : \"Maxwell Air Force Base\" , \"pubs\" : \"201\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.417730.6\" , \"children_name\" : \"United States Air Force Research Laboratory\" , \"pubs\" : \"24279\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.453002.0\" , \"children_name\" : \"United States Air Force\" , \"pubs\" : \"2545\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.461677.5\" , \"children_name\" : \"Eglin Air Force Base\" , \"pubs\" : \"575\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.507554.6\" , \"children_name\" : \"United States Air Force Office of Scientific Research\" , \"pubs\" : \"543\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.465246.7\" , \"children_name\" : \"Air University\" , \"pubs\" : \"47\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.472535.2\" , \"children_name\" : \"Kirtland Air Force Base\" , \"pubs\" : \"801\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.461680.d\" , \"children_name\" : \"Hanscom Air Force Base\" , \"pubs\" : \"400\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.427848.5\" , \"children_name\" : \"Air Force Institute of Technology\" , \"pubs\" : \"4028\" } // more resul ts here... ]","title":"24. Organizations and sub-organizations"},{"location":"queries/24/#24-organizations-and-sub-organizations","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"24. Organizations and sub-organizations"},{"location":"queries/24/#description","text":"This query shows how to extract information about a GRID organization together with all of its sub-organizations. Many organizations in the GRID database include parent-child relationships. For example, grid.495456.f (the United States Department of the Air Force page) has both parent and children institutions, so one can use the hierarchy when querying related data e.g. the total number of publications for each of these organizations. The query below shows how to leverage the organization_recursive_child_ids field in the grid table in order to achieve that. This field is prepopulated with all children institutions GRID IDs (recursively), hence it makes it easier to run this type of analyses.","title":"Description"},{"location":"queries/24/#query","text":"WITH hierarchy AS ( SELECT g . id AS parent , g . name AS parent_name , children , g2 . name AS children_name , FROM ` dimensions - ai . data_analytics . grid ` g CROSS JOIN UNNEST ( organization_recursive_child_ids ) AS children INNER JOIN ` dimensions - ai . data_analytics . grid ` g2 ON g2 . id = children WHERE g . id = \"grid.495456.f\" -- United States Department of the Air Force ) SELECT hierarchy . * , COUNT ( DISTINCT p . id ) as pubs FROM hierarchy INNER JOIN ` dimensions - ai . data_analytics . publications ` p on hierarchy . children in UNNEST ( p . research_orgs ) GROUP by 1 , 2 , 3 , 4","title":"Query"},{"location":"queries/24/#breaking-it-down","text":"The key part of the query uses a CROSS JOIN on the organization_recursive_child_ids field to retrieve all the descendants of the chosen organization: SELECT g . id AS parent , children , FROM ` dimensions - ai . data_analytics . grid ` g CROSS JOIN UNNEST ( organization_recursive_child_ids ) AS children WHERE g . id = \"grid.495456.f\" Furthermore, in order to get more organization metadata e.g. the name, an inner self-join is introduced: SELECT g . id AS parent , g . name AS parent_name , children , g2 . name AS children_name , FROM ` dimensions - ai . data_analytics . grid ` g CROSS JOIN UNNEST ( organization_recursive_child_ids ) AS children INNER JOIN ` dimensions - ai . data_analytics . grid ` g2 ON g2 . id = children WHERE g . id = \"grid.495456.f\" The final step is to join also the publications table, so to get the total publications count for each organization. WITH hierarchy AS ( -- the grid query above ) SELECT hierarchy . * , COUNT ( DISTINCT p . id ) as pubs FROM hierarchy INNER JOIN ` dimensions - ai . data_analytics . publications ` p on hierarchy . children in UNNEST ( p . research_orgs ) GROUP by 1 , 2 , 3 , 4","title":"Breaking it down"},{"location":"queries/24/#results","text":"[ { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.494596.3\" , \"children_name\" : \"Edwards Air Force Base\" , \"pubs\" : \"467\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.461685.8\" , \"children_name\" : \"Joint Base San Antonio\" , \"pubs\" : \"1031\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.265457.7\" , \"children_name\" : \"United States Air Force Academy\" , \"pubs\" : \"3137\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.431316.2\" , \"children_name\" : \"Grand Forks Air Force Base\" , \"pubs\" : \"4\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.499282.c\" , \"children_name\" : \"Maxwell Air Force Base\" , \"pubs\" : \"201\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.417730.6\" , \"children_name\" : \"United States Air Force Research Laboratory\" , \"pubs\" : \"24279\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.453002.0\" , \"children_name\" : \"United States Air Force\" , \"pubs\" : \"2545\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.461677.5\" , \"children_name\" : \"Eglin Air Force Base\" , \"pubs\" : \"575\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.507554.6\" , \"children_name\" : \"United States Air Force Office of Scientific Research\" , \"pubs\" : \"543\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.465246.7\" , \"children_name\" : \"Air University\" , \"pubs\" : \"47\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.472535.2\" , \"children_name\" : \"Kirtland Air Force Base\" , \"pubs\" : \"801\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.461680.d\" , \"children_name\" : \"Hanscom Air Force Base\" , \"pubs\" : \"400\" }, { \"parent\" : \"grid.495456.f\" , \"parent_name\" : \"United States Department of the Air Force\" , \"children\" : \"grid.427848.5\" , \"children_name\" : \"Air Force Institute of Technology\" , \"pubs\" : \"4028\" } // more resul ts here... ]","title":"Results"},{"location":"queries/25/","text":"25. Grants for an organization Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description This query counts all grants received by a selected GRID organization and sums up the total amount of funding received, based on the grants start_year . Query SELECT COUNT ( * ) AS total_grants , SUM ( funding_usd ) AS total_grants_amount_usd FROM ` dimensions - ai . data_analytics . grants ` WHERE \"grid.10837.3d\" IN UNNEST ( research_orgs ) AND ( start_year >= 2009 AND start_year <= 2020 ) Results [ { \"total_grants\" : \"731\" , \"total_grants_amount_usd\" : \"8.4692966E8\" } ]","title":"25. Grants for an organization"},{"location":"queries/25/#25-grants-for-an-organization","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"25. Grants for an organization"},{"location":"queries/25/#description","text":"This query counts all grants received by a selected GRID organization and sums up the total amount of funding received, based on the grants start_year .","title":"Description"},{"location":"queries/25/#query","text":"SELECT COUNT ( * ) AS total_grants , SUM ( funding_usd ) AS total_grants_amount_usd FROM ` dimensions - ai . data_analytics . grants ` WHERE \"grid.10837.3d\" IN UNNEST ( research_orgs ) AND ( start_year >= 2009 AND start_year <= 2020 )","title":"Query"},{"location":"queries/25/#results","text":"[ { \"total_grants\" : \"731\" , \"total_grants_amount_usd\" : \"8.4692966E8\" } ]","title":"Results"},{"location":"queries/26/","text":"26. Field Citation Ratio (FCR) median average Level: Medium This query requires basic knowledge of SQL and the Dimensions data model Description This query calculates the Field Citation Ratio (FCR) mean per year for publications from a chosen country (eg 'CA' ). FCR Mean is the average Field Citation Ratio (FCR), which indicates the relative citation performance of an article, when compared to similarly-aged articles in its Fields of Research (FoR) category. To calculate the geometric mean, we use the following approach, as documented in Thelwall & Fairclough (2015) : For a set of documents, all citation counts are incremented by 1. We calculate the natural log of the citations counts. We add these values together, and divide by the number of documents. We calculate the exponential of this value (reversing the log effect). We reduce the final value by 1. For more background, see this article: What is the FCR? How is it calculated? . Note The equivalent Dimensions API query is: search publications where research_org_countries = \"CA\" return year aggregate fcr_gavg sort by fcr_gavg Query SELECT year , COUNT ( * ) AS pub_count , ( EXP ( AVG ( LN ( metrics . field_citation_ratio + 1 ))) - 1 ) AS fcr_gavg FROM ` dimensions - ai . data_analytics . publications ` AS p WHERE 'CA' IN UNNEST ( research_org_countries ) AND year >= 2000 AND year <= 2019 GROUP BY year ORDER BY year DESC Results [ { \"year\" : \"2019\" , \"pub_count\" : \"126595\" , \"fcr_gavg\" : \"1.895202772875376\" }, { \"year\" : \"2018\" , \"pub_count\" : \"123072\" , \"fcr_gavg\" : \"2.063853669997279\" }, { \"year\" : \"2017\" , \"pub_count\" : \"116099\" , \"fcr_gavg\" : \"2.173716507769935\" }, { \"year\" : \"2016\" , \"pub_count\" : \"110619\" , \"fcr_gavg\" : \"2.2466482612818988\" }, { \"year\" : \"2015\" , \"pub_count\" : \"105632\" , \"fcr_gavg\" : \"2.3125283272558983\" }, { \"year\" : \"2014\" , \"pub_count\" : \"103433\" , \"fcr_gavg\" : \"2.3687156780522907\" }, { \"year\" : \"2013\" , \"pub_count\" : \"98650\" , \"fcr_gavg\" : \"2.467124347845802\" }, { \"year\" : \"2012\" , \"pub_count\" : \"94561\" , \"fcr_gavg\" : \"2.544932622059771\" }, { \"year\" : \"2011\" , \"pub_count\" : \"90036\" , \"fcr_gavg\" : \"2.5889138063645207\" }, { \"year\" : \"2010\" , \"pub_count\" : \"86903\" , \"fcr_gavg\" : \"2.6670704464809525\" }, { \"year\" : \"2009\" , \"pub_count\" : \"83592\" , \"fcr_gavg\" : \"2.686271282515282\" }, { \"year\" : \"2008\" , \"pub_count\" : \"79437\" , \"fcr_gavg\" : \"2.622504341868477\" }, { \"year\" : \"2007\" , \"pub_count\" : \"72631\" , \"fcr_gavg\" : \"2.6134419550710426\" }, { \"year\" : \"2006\" , \"pub_count\" : \"70521\" , \"fcr_gavg\" : \"2.5591136625055593\" }, { \"year\" : \"2005\" , \"pub_count\" : \"63057\" , \"fcr_gavg\" : \"2.6001407619739907\" }, { \"year\" : \"2004\" , \"pub_count\" : \"55570\" , \"fcr_gavg\" : \"2.6729901969921546\" }, { \"year\" : \"2003\" , \"pub_count\" : \"50762\" , \"fcr_gavg\" : \"2.6458690167701433\" }, { \"year\" : \"2002\" , \"pub_count\" : \"46301\" , \"fcr_gavg\" : \"2.5733709228189774\" }, { \"year\" : \"2001\" , \"pub_count\" : \"42605\" , \"fcr_gavg\" : \"2.7140322564560413\" }, { \"year\" : \"2000\" , \"pub_count\" : \"41724\" , \"fcr_gavg\" : \"2.6536040582685607\" } ]","title":"26. Field Citation Ratio (FCR) median average"},{"location":"queries/26/#26-field-citation-ratio-fcr-median-average","text":"Level: Medium This query requires basic knowledge of SQL and the Dimensions data model","title":"26. Field Citation Ratio (FCR) median average"},{"location":"queries/26/#description","text":"This query calculates the Field Citation Ratio (FCR) mean per year for publications from a chosen country (eg 'CA' ). FCR Mean is the average Field Citation Ratio (FCR), which indicates the relative citation performance of an article, when compared to similarly-aged articles in its Fields of Research (FoR) category. To calculate the geometric mean, we use the following approach, as documented in Thelwall & Fairclough (2015) : For a set of documents, all citation counts are incremented by 1. We calculate the natural log of the citations counts. We add these values together, and divide by the number of documents. We calculate the exponential of this value (reversing the log effect). We reduce the final value by 1. For more background, see this article: What is the FCR? How is it calculated? . Note The equivalent Dimensions API query is: search publications where research_org_countries = \"CA\" return year aggregate fcr_gavg sort by fcr_gavg","title":"Description"},{"location":"queries/26/#query","text":"SELECT year , COUNT ( * ) AS pub_count , ( EXP ( AVG ( LN ( metrics . field_citation_ratio + 1 ))) - 1 ) AS fcr_gavg FROM ` dimensions - ai . data_analytics . publications ` AS p WHERE 'CA' IN UNNEST ( research_org_countries ) AND year >= 2000 AND year <= 2019 GROUP BY year ORDER BY year DESC","title":"Query"},{"location":"queries/26/#results","text":"[ { \"year\" : \"2019\" , \"pub_count\" : \"126595\" , \"fcr_gavg\" : \"1.895202772875376\" }, { \"year\" : \"2018\" , \"pub_count\" : \"123072\" , \"fcr_gavg\" : \"2.063853669997279\" }, { \"year\" : \"2017\" , \"pub_count\" : \"116099\" , \"fcr_gavg\" : \"2.173716507769935\" }, { \"year\" : \"2016\" , \"pub_count\" : \"110619\" , \"fcr_gavg\" : \"2.2466482612818988\" }, { \"year\" : \"2015\" , \"pub_count\" : \"105632\" , \"fcr_gavg\" : \"2.3125283272558983\" }, { \"year\" : \"2014\" , \"pub_count\" : \"103433\" , \"fcr_gavg\" : \"2.3687156780522907\" }, { \"year\" : \"2013\" , \"pub_count\" : \"98650\" , \"fcr_gavg\" : \"2.467124347845802\" }, { \"year\" : \"2012\" , \"pub_count\" : \"94561\" , \"fcr_gavg\" : \"2.544932622059771\" }, { \"year\" : \"2011\" , \"pub_count\" : \"90036\" , \"fcr_gavg\" : \"2.5889138063645207\" }, { \"year\" : \"2010\" , \"pub_count\" : \"86903\" , \"fcr_gavg\" : \"2.6670704464809525\" }, { \"year\" : \"2009\" , \"pub_count\" : \"83592\" , \"fcr_gavg\" : \"2.686271282515282\" }, { \"year\" : \"2008\" , \"pub_count\" : \"79437\" , \"fcr_gavg\" : \"2.622504341868477\" }, { \"year\" : \"2007\" , \"pub_count\" : \"72631\" , \"fcr_gavg\" : \"2.6134419550710426\" }, { \"year\" : \"2006\" , \"pub_count\" : \"70521\" , \"fcr_gavg\" : \"2.5591136625055593\" }, { \"year\" : \"2005\" , \"pub_count\" : \"63057\" , \"fcr_gavg\" : \"2.6001407619739907\" }, { \"year\" : \"2004\" , \"pub_count\" : \"55570\" , \"fcr_gavg\" : \"2.6729901969921546\" }, { \"year\" : \"2003\" , \"pub_count\" : \"50762\" , \"fcr_gavg\" : \"2.6458690167701433\" }, { \"year\" : \"2002\" , \"pub_count\" : \"46301\" , \"fcr_gavg\" : \"2.5733709228189774\" }, { \"year\" : \"2001\" , \"pub_count\" : \"42605\" , \"fcr_gavg\" : \"2.7140322564560413\" }, { \"year\" : \"2000\" , \"pub_count\" : \"41724\" , \"fcr_gavg\" : \"2.6536040582685607\" } ]","title":"Results"},{"location":"queries/27/","text":"27. List of corresponding authors Level: Easy This query is suitable for new users of Dimensions on Google BigQuery Description Extract corresponding authors for the Nature Medicine journal. Query SELECT id , doi , title . preferred , a . first_name , a . last_name , a . corresponding , journal . title AS journal_title FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( authors ) AS a WHERE journal . id = \"jour.1113716\" AND a . corresponding = TRUE LIMIT 10 Results [ { \"id\" : \"pub.1000366392\" , \"doi\" : \"10.1038/74704\" , \"preferred\" : \"Inhibitory Fc receptors modulate in vivo cytoxicity against tumor targets\" , \"first_name\" : \"Jeffrey V.\" , \"last_name\" : \"Ravetch\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1002332484\" , \"doi\" : \"10.1038/74689\" , \"preferred\" : \"The tyrosine kinase p56lck is essential in coxsackievirus B3-mediated heart disease\" , \"first_name\" : \"Josef M.\" , \"last_name\" : \"Penninger\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1002696559\" , \"doi\" : \"10.1038/75068\" , \"preferred\" : \"Blockade of interleukin 6 trans signaling suppresses T-cell resistance against apoptosis in chronic intestinal inflammation: Evidence in Crohn disease and experimental colitis in vivo\" , \"first_name\" : \"M.F.\" , \"last_name\" : \"Neurath\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1001207508\" , \"doi\" : \"10.1038/74918\" , \"preferred\" : \"Human neural progenitor cells: better blue than green?\" , \"first_name\" : \"Alberto\" , \"last_name\" : \"Mart\u00ednez-Serrano\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000887404\" , \"doi\" : \"10.1038/73213\" , \"preferred\" : \"Transdermal monitoring of glucose and other analytes using ultrasound\" , \"first_name\" : \"Joseph\" , \"last_name\" : \"Kost\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000887404\" , \"doi\" : \"10.1038/73213\" , \"preferred\" : \"Transdermal monitoring of glucose and other analytes using ultrasound\" , \"first_name\" : \"Robert\" , \"last_name\" : \"Langer\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000556369\" , \"doi\" : \"10.1038/71527\" , \"preferred\" : \"PR39, a peptide regulator of angiogenesis\" , \"first_name\" : \"Michael\" , \"last_name\" : \"Simons\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1001854662\" , \"doi\" : \"10.1038/72262\" , \"preferred\" : \"Protection from septic shock by neutralization of macrophage migration inhibitory factor\" , \"first_name\" : \"Thierry\" , \"last_name\" : \"Calandra\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000791818\" , \"doi\" : \"10.1038/76267\" , \"preferred\" : \"Immunologic \u2018ignorance\u2019 of vascularized organ transplants in the absence of secondary lymphoid tissue\" , \"first_name\" : \"Fadi G.\" , \"last_name\" : \"Lakkis\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1002229158\" , \"doi\" : \"10.1038/72329\" , \"preferred\" : \"Molecular mimicry mediated by MHC class Ib molecules after infection with Gram-negative pathogens\" , \"first_name\" : \"Mark J.\" , \"last_name\" : \"Soloski\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" } ]","title":"27. List of corresponding authors"},{"location":"queries/27/#27-list-of-corresponding-authors","text":"Level: Easy This query is suitable for new users of Dimensions on Google BigQuery","title":"27. List of corresponding authors"},{"location":"queries/27/#description","text":"Extract corresponding authors for the Nature Medicine journal.","title":"Description"},{"location":"queries/27/#query","text":"SELECT id , doi , title . preferred , a . first_name , a . last_name , a . corresponding , journal . title AS journal_title FROM ` dimensions - ai . data_analytics . publications ` , UNNEST ( authors ) AS a WHERE journal . id = \"jour.1113716\" AND a . corresponding = TRUE LIMIT 10","title":"Query"},{"location":"queries/27/#results","text":"[ { \"id\" : \"pub.1000366392\" , \"doi\" : \"10.1038/74704\" , \"preferred\" : \"Inhibitory Fc receptors modulate in vivo cytoxicity against tumor targets\" , \"first_name\" : \"Jeffrey V.\" , \"last_name\" : \"Ravetch\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1002332484\" , \"doi\" : \"10.1038/74689\" , \"preferred\" : \"The tyrosine kinase p56lck is essential in coxsackievirus B3-mediated heart disease\" , \"first_name\" : \"Josef M.\" , \"last_name\" : \"Penninger\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1002696559\" , \"doi\" : \"10.1038/75068\" , \"preferred\" : \"Blockade of interleukin 6 trans signaling suppresses T-cell resistance against apoptosis in chronic intestinal inflammation: Evidence in Crohn disease and experimental colitis in vivo\" , \"first_name\" : \"M.F.\" , \"last_name\" : \"Neurath\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1001207508\" , \"doi\" : \"10.1038/74918\" , \"preferred\" : \"Human neural progenitor cells: better blue than green?\" , \"first_name\" : \"Alberto\" , \"last_name\" : \"Mart\u00ednez-Serrano\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000887404\" , \"doi\" : \"10.1038/73213\" , \"preferred\" : \"Transdermal monitoring of glucose and other analytes using ultrasound\" , \"first_name\" : \"Joseph\" , \"last_name\" : \"Kost\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000887404\" , \"doi\" : \"10.1038/73213\" , \"preferred\" : \"Transdermal monitoring of glucose and other analytes using ultrasound\" , \"first_name\" : \"Robert\" , \"last_name\" : \"Langer\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000556369\" , \"doi\" : \"10.1038/71527\" , \"preferred\" : \"PR39, a peptide regulator of angiogenesis\" , \"first_name\" : \"Michael\" , \"last_name\" : \"Simons\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1001854662\" , \"doi\" : \"10.1038/72262\" , \"preferred\" : \"Protection from septic shock by neutralization of macrophage migration inhibitory factor\" , \"first_name\" : \"Thierry\" , \"last_name\" : \"Calandra\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1000791818\" , \"doi\" : \"10.1038/76267\" , \"preferred\" : \"Immunologic \u2018ignorance\u2019 of vascularized organ transplants in the absence of secondary lymphoid tissue\" , \"first_name\" : \"Fadi G.\" , \"last_name\" : \"Lakkis\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" }, { \"id\" : \"pub.1002229158\" , \"doi\" : \"10.1038/72329\" , \"preferred\" : \"Molecular mimicry mediated by MHC class Ib molecules after infection with Gram-negative pathogens\" , \"first_name\" : \"Mark J.\" , \"last_name\" : \"Soloski\" , \"corresponding\" : true , \"journal_title\" : \"Nature Medicine\" } ]","title":"Results"},{"location":"tutorials/","text":"About Tutorials The Tutorials section contain guides that focuses on specific topics or use cases e.g. how to deal with a specific data type, or how to use GBQ in combination with other technologies. Note See also the Dimensions on BigQuery official documentation for more information about how to access the data.","title":"About Tutorials"},{"location":"tutorials/#about-tutorials","text":"The Tutorials section contain guides that focuses on specific topics or use cases e.g. how to deal with a specific data type, or how to use GBQ in combination with other technologies. Note See also the Dimensions on BigQuery official documentation for more information about how to access the data.","title":"About Tutorials"},{"location":"tutorials/01-connection/","text":"Verifying your connection In this tutorial we will show how to connect to the Dimensions on Google BigQuery using Python, so that we can then run a few sample queries. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . You have some basic familiarity with Python and Jupyter notebooks . Connection methods There are a few options available: Use Google Colaboratory and your personal credentials. This option is the simplest of all, as it doesn't require you to install anything on your computer. It is normally ok for small to mid-sized projects that can live in the cloud. Use a local Jupyter environment and your personal credentials. This option requires you to install the Google Cloud SDK in order to authenticate. It is the best option if you want to work locally and/or have other Python libraries or services that you need to access. Use a local Jupyter environment and a service account. This option is really a variance on the option 2, for those users that must use a service account. NOTE All of these options require you to first set up a GCP project (as you haven't done it already) and provide your project ID. E.g.: MY_PROJECT_ID = \"my-cool-gbq-project\" Option 1: using Google Colaboratory and your personal credentials Google Colaboratory is a free cloud-based Jupyter environment from Google. This option provides an easy service allowing you to get started with notebooks. Using your Google Account you can create notebooks, execute BigQuery queries and share these with other Google Accounts quickly and easily. # authentication happens via your browser from google.colab import auth auth . authenticate_user () print ( 'Authenticated' ) MY_PROJECT_ID = \"my-cool-gbq-project\" from google.cloud import bigquery client = bigquery . Client ( project = MY_PROJECT_ID ) Option 2: using a local Jupyter and your personal credentials A Google Account represents a developer, an administrator, or any other person who interacts with Google Cloud. This is normally the Google account one has used to get access to the Dimensions on BigQuery product. In order to configure programmatic access for local development, the easiest way is to authenticate using the Google Cloud SDK . $ gcloud auth application-default login Note: the command above should be run from a Terminal or console. This will generate a JSON file that is used as the default application credentials for the account that was selected in the above login process. When using the default Client for each Google provided package (such as BigQuery) they should automatically authenticate using these default credentials. # install python client library ! pip install google - cloud - bigquery - U -- quiet from google.cloud import bigquery MY_PROJECT_ID = \"my-cool-gbq-project\" client = bigquery . Client ( project = MY_PROJECT_ID ) Option 3: using a local Jupyter and a service account A service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person. Each service account is associated with two sets of public/private RSA key pairs that are used to authenticate to Google: Google-managed keys, and user-managed keys. When using a service account you'd just have to point your client object to the a key file. from google.cloud import bigquery credentials_file = 'my-awesome-gbq-project-47616836.json' MY_PROJECT_ID = \"my-cool-gbq-project\" # Explicitly use service account credentials by specifying the private key file client = bigquery . Client . from_service_account_json ( credentials_file ) Running queries Once the connection is set up, all you have to do is to type in a SQL query and run it using the client object. # Query: Top publications from Oxford univ. by Altmetric Score in 2020 query_1 = \"\"\" SELECT id, title.preferred as title, ARRAY_LENGTH(authors) as authors_count, CAST(altmetrics.score as INT64) as altmetric_score FROM `dimensions-ai.data_analytics.publications` WHERE year = 2020 AND 'grid.4991.5' in UNNEST(research_orgs) ORDER BY altmetric_score DESC LIMIT 5\"\"\" # 1 - main syntax query_job = client . query ( query_1 ) results = query_job . result () # Waits for job to complete. for row in results : print ( \"> {} : {} \\n\\t Authors: {} \\n\\t Altmetric Score: {} \" . format ( row . id , row . title , row . authors_count , row . altmetric_score )) > pub.1129493369 : Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial Authors: 366 Altmetric Score: 15451 > pub.1130340155 : Two metres or one: what is the evidence for physical distancing in covid-19? Authors: 6 Altmetric Score: 15125 > pub.1127239818 : Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial Authors: 46 Altmetric Score: 12675 > pub.1131721397 : Scientific consensus on the COVID-19 pandemic: we need to act now Authors: 31 Altmetric Score: 10192 > pub.1126016857 : Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing Authors: 9 Altmetric Score: 8320 An slighly alternative syntax is also possible # 2 - omit calling result() query_job = client . query ( query_1 ) for row in query_job : print ( row ) Row(('pub.1129493369', 'Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial', 366, 15451), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1130340155', 'Two metres or one: what is the evidence for physical distancing in covid-19?', 6, 15125), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1127239818', 'Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial', 46, 12675), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1131721397', 'Scientific consensus on the COVID-19 pandemic: we need to act now', 31, 10192), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1126016857', 'Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing', 9, 8320), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Another quite handy feature is to transform data direclty into Pandas dataframes # 3 - return a dataframe query_job = client . query ( query_1 ) . to_dataframe () query_job | Row | id | title | authors_count | altmetric_score | | --- | ----- | ----- | ------------- | --------------- | | 1 | pub.1129493369 | Safety and immunogenicity of the ChAdOx1 nCoV-... | 366 | 15451 | | 2 | pub.1130340155 | Two metres or one: what is the evidence for ph... | 6 | 15125 | | 3 | pub.1127239818 | Remdesivir in adults with severe COVID-19: a r... | 46 | 12675 | | 4 | pub.1131721397 | Scientific consensus on the COVID-19 pandemic:... | 31 | 10192 | | 5 | pub.1126016857 | Quantifying SARS-CoV-2 transmission suggests e... | 9 | 8320 | Advanced: BigQuery magic command and dynamic parameters The GBQ library comes with a magic command that is essentially a nice shortcut method for running queries. This extensions needs to be loaded sepately e.g.: % load_ext google . cloud . bigquery We can then set up a couple of query parameters for the query itself, as well as the usual project ID value. project_id = MY_PROJECT_ID bq_params = {} bq_params [ \"journal_id\" ] = \"jour.1115214\" Finally we can query by starting a cell with the command %%bigquery ... : %% bigquery -- params $ bq_params -- project $ project_id # Publications per year for Nature Biotechnology SELECT count ( * ) as pubs , year , journal . title FROM ` dimensions - ai . data_analytics . publications ` WHERE year >= 2010 AND journal . id = @journal_id GROUP BY year , journal . title ORDER BY year DESC Query complete after 0.02s: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 699.28query/s] Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02<00:00, 4.31rows/s] Row pubs year title 1 438 2020 Nature Biotechnology 2 386 2019 Nature Biotechnology 3 374 2018 Nature Biotechnology 4 380 2017 Nature Biotechnology 5 436 2016 Nature Biotechnology 6 467 2015 Nature Biotechnology 7 475 2014 Nature Biotechnology 8 462 2013 Nature Biotechnology 9 507 2012 Nature Biotechnology 10 459 2011 Nature Biotechnology 11 486 2010 Nature Biotechnology Troubleshooting Query fails wit to_dataframe() ArrowNotImplementedError Try reinstalling pyarrow ie pip install pyarrow -U Query fails with AttributeError: 'NoneType' object has no attribute 'transport' Try pip install google-cloud-bigquery-storage -U and restarting the notebook","title":"Verifying your connection"},{"location":"tutorials/01-connection/#verifying-your-connection","text":"In this tutorial we will show how to connect to the Dimensions on Google BigQuery using Python, so that we can then run a few sample queries. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . You have some basic familiarity with Python and Jupyter notebooks .","title":"Verifying your connection"},{"location":"tutorials/01-connection/#connection-methods","text":"There are a few options available: Use Google Colaboratory and your personal credentials. This option is the simplest of all, as it doesn't require you to install anything on your computer. It is normally ok for small to mid-sized projects that can live in the cloud. Use a local Jupyter environment and your personal credentials. This option requires you to install the Google Cloud SDK in order to authenticate. It is the best option if you want to work locally and/or have other Python libraries or services that you need to access. Use a local Jupyter environment and a service account. This option is really a variance on the option 2, for those users that must use a service account. NOTE All of these options require you to first set up a GCP project (as you haven't done it already) and provide your project ID. E.g.: MY_PROJECT_ID = \"my-cool-gbq-project\"","title":"Connection methods"},{"location":"tutorials/01-connection/#option-1-using-google-colaboratory-and-your-personal-credentials","text":"Google Colaboratory is a free cloud-based Jupyter environment from Google. This option provides an easy service allowing you to get started with notebooks. Using your Google Account you can create notebooks, execute BigQuery queries and share these with other Google Accounts quickly and easily. # authentication happens via your browser from google.colab import auth auth . authenticate_user () print ( 'Authenticated' ) MY_PROJECT_ID = \"my-cool-gbq-project\" from google.cloud import bigquery client = bigquery . Client ( project = MY_PROJECT_ID )","title":"Option 1: using Google Colaboratory and your personal credentials"},{"location":"tutorials/01-connection/#option-2-using-a-local-jupyter-and-your-personal-credentials","text":"A Google Account represents a developer, an administrator, or any other person who interacts with Google Cloud. This is normally the Google account one has used to get access to the Dimensions on BigQuery product. In order to configure programmatic access for local development, the easiest way is to authenticate using the Google Cloud SDK . $ gcloud auth application-default login Note: the command above should be run from a Terminal or console. This will generate a JSON file that is used as the default application credentials for the account that was selected in the above login process. When using the default Client for each Google provided package (such as BigQuery) they should automatically authenticate using these default credentials. # install python client library ! pip install google - cloud - bigquery - U -- quiet from google.cloud import bigquery MY_PROJECT_ID = \"my-cool-gbq-project\" client = bigquery . Client ( project = MY_PROJECT_ID )","title":"Option 2: using a local Jupyter and your personal credentials"},{"location":"tutorials/01-connection/#option-3-using-a-local-jupyter-and-a-service-account","text":"A service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person. Each service account is associated with two sets of public/private RSA key pairs that are used to authenticate to Google: Google-managed keys, and user-managed keys. When using a service account you'd just have to point your client object to the a key file. from google.cloud import bigquery credentials_file = 'my-awesome-gbq-project-47616836.json' MY_PROJECT_ID = \"my-cool-gbq-project\" # Explicitly use service account credentials by specifying the private key file client = bigquery . Client . from_service_account_json ( credentials_file )","title":"Option 3: using a local Jupyter and a service account"},{"location":"tutorials/01-connection/#running-queries","text":"Once the connection is set up, all you have to do is to type in a SQL query and run it using the client object. # Query: Top publications from Oxford univ. by Altmetric Score in 2020 query_1 = \"\"\" SELECT id, title.preferred as title, ARRAY_LENGTH(authors) as authors_count, CAST(altmetrics.score as INT64) as altmetric_score FROM `dimensions-ai.data_analytics.publications` WHERE year = 2020 AND 'grid.4991.5' in UNNEST(research_orgs) ORDER BY altmetric_score DESC LIMIT 5\"\"\" # 1 - main syntax query_job = client . query ( query_1 ) results = query_job . result () # Waits for job to complete. for row in results : print ( \"> {} : {} \\n\\t Authors: {} \\n\\t Altmetric Score: {} \" . format ( row . id , row . title , row . authors_count , row . altmetric_score )) > pub.1129493369 : Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial Authors: 366 Altmetric Score: 15451 > pub.1130340155 : Two metres or one: what is the evidence for physical distancing in covid-19? Authors: 6 Altmetric Score: 15125 > pub.1127239818 : Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial Authors: 46 Altmetric Score: 12675 > pub.1131721397 : Scientific consensus on the COVID-19 pandemic: we need to act now Authors: 31 Altmetric Score: 10192 > pub.1126016857 : Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing Authors: 9 Altmetric Score: 8320 An slighly alternative syntax is also possible # 2 - omit calling result() query_job = client . query ( query_1 ) for row in query_job : print ( row ) Row(('pub.1129493369', 'Safety and immunogenicity of the ChAdOx1 nCoV-19 vaccine against SARS-CoV-2: a preliminary report of a phase 1/2, single-blind, randomised controlled trial', 366, 15451), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1130340155', 'Two metres or one: what is the evidence for physical distancing in covid-19?', 6, 15125), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1127239818', 'Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial', 46, 12675), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1131721397', 'Scientific consensus on the COVID-19 pandemic: we need to act now', 31, 10192), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Row(('pub.1126016857', 'Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing', 9, 8320), {'id': 0, 'title': 1, 'authors_count': 2, 'altmetric_score': 3}) Another quite handy feature is to transform data direclty into Pandas dataframes # 3 - return a dataframe query_job = client . query ( query_1 ) . to_dataframe () query_job | Row | id | title | authors_count | altmetric_score | | --- | ----- | ----- | ------------- | --------------- | | 1 | pub.1129493369 | Safety and immunogenicity of the ChAdOx1 nCoV-... | 366 | 15451 | | 2 | pub.1130340155 | Two metres or one: what is the evidence for ph... | 6 | 15125 | | 3 | pub.1127239818 | Remdesivir in adults with severe COVID-19: a r... | 46 | 12675 | | 4 | pub.1131721397 | Scientific consensus on the COVID-19 pandemic:... | 31 | 10192 | | 5 | pub.1126016857 | Quantifying SARS-CoV-2 transmission suggests e... | 9 | 8320 |","title":"Running queries"},{"location":"tutorials/01-connection/#advanced-bigquery-magic-command-and-dynamic-parameters","text":"The GBQ library comes with a magic command that is essentially a nice shortcut method for running queries. This extensions needs to be loaded sepately e.g.: % load_ext google . cloud . bigquery We can then set up a couple of query parameters for the query itself, as well as the usual project ID value. project_id = MY_PROJECT_ID bq_params = {} bq_params [ \"journal_id\" ] = \"jour.1115214\" Finally we can query by starting a cell with the command %%bigquery ... : %% bigquery -- params $ bq_params -- project $ project_id # Publications per year for Nature Biotechnology SELECT count ( * ) as pubs , year , journal . title FROM ` dimensions - ai . data_analytics . publications ` WHERE year >= 2010 AND journal . id = @journal_id GROUP BY year , journal . title ORDER BY year DESC Query complete after 0.02s: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 699.28query/s] Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02<00:00, 4.31rows/s] Row pubs year title 1 438 2020 Nature Biotechnology 2 386 2019 Nature Biotechnology 3 374 2018 Nature Biotechnology 4 380 2017 Nature Biotechnology 5 436 2016 Nature Biotechnology 6 467 2015 Nature Biotechnology 7 475 2014 Nature Biotechnology 8 462 2013 Nature Biotechnology 9 507 2012 Nature Biotechnology 10 459 2011 Nature Biotechnology 11 486 2010 Nature Biotechnology","title":"Advanced: BigQuery magic command and dynamic parameters"},{"location":"tutorials/01-connection/#troubleshooting","text":"Query fails wit to_dataframe() ArrowNotImplementedError Try reinstalling pyarrow ie pip install pyarrow -U Query fails with AttributeError: 'NoneType' object has no attribute 'transport' Try pip install google-cloud-bigquery-storage -U and restarting the notebook","title":"Troubleshooting"},{"location":"tutorials/02-dsl/","text":"From the DSL API to Google BigQuery This tutorial demonstrates how to perform a full-text search in Dimensions using the Analytics API and then export the data to Google BigQuery for further analysis. This technique allows to take advantage of the strengths of each of these data products: The Analytics API allows to run full-text searches over the tens of millions documents stored in the Dimensions database. This makes it an ideal tool for identifying a corpus of documents using collections of keywords and/or other filters (note: this is the same functionality available when you search on app.dimensions.ai) The Dimensions on Google BigQuery database allows to run SQL queries of any complexity using a cloud-based environment containing all of the metadata available in Dimensions, thus removing the need to download/analyse the data offline first. This makes is the perfect solution for advanced analytics tasks such as benchmarking, metrics calculations or impact analyses. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . You have a valid Dimensions API account . You have some basic familiarity with Python and Jupyter notebooks . Example: profiling researchers linked to a topic The concrete usecase we'll be looking at involves running a full-text search for \"moon landing\" publications using the DSL API, then creating a corpus in GBQ based on this search (eg see this search ). Once we have the publication corpus available in GBQ, we will extract all associated researchers (=authors). At the same time, we are going to use SQL in order to enrich the results using other metrics (eg more researchers metadata including citations & altmetric). Getting started The following code will load the Python BigQuery library and authenticate you as a valid user. ! pip install google - cloud - bigquery - U -- quiet % load_ext google . cloud . bigquery import sys print ( \"== \\n Authenticating...\" ) if 'google.colab' in sys . modules : from google.colab import auth auth . authenticate_user () print ( '..done (method: Colab)' ) else : from google.cloud import bigquery print ( '..done (method: local credentials)' ) # # PLEASE UPDATE USING YOUR CLOUD PROJECT ID (= the 'billing' account) # MY_PROJECT_ID = \"ds-data-solutions-gbq\" print ( \"== \\n Testing connection..\" ) client = bigquery . Client ( project = MY_PROJECT_ID ) test = client . query ( \"\"\" SELECT COUNT(*) as pubs from `dimensions-ai.data_analytics.publications` \"\"\" ) rows = [ x for x in test . result ()] print ( \"...success!\" ) print ( \"Total publications in Dimensions: \" , rows [ 0 ][ 'pubs' ]) The google.cloud.bigquery extension is already loaded. To reload it, use: %reload_ext google.cloud.bigquery == Authenticating... ..done (method: local credentials) == Testing connection.. ...success! Total publications in Dimensions: 115963650 1. Connecting to the DSL API For more background on the Analytics API and how to work with it, see this tutorial ! pip install dimcli -- quiet import dimcli from dimcli.utils import * import json import sys import pandas as pd # ENDPOINT = \"https://app.dimensions.ai\" USERNAME , PASSWORD = \"\" , \"\" dimcli . login ( USERNAME , PASSWORD , ENDPOINT ) dsl = dimcli . Dsl () Let's try running a sample query. TIP Review the full text search syntax of the Dimensions Search Language. %% dsldf search publications for \" \\\" moon landing \\\" AND Moon AND \\\" lunar surface \\\" \" return publications limit 10 Returned Publications: 10 (total = 11305) \u001b[2mTime: 0.72s\u001b[0m Row type volume pages id year author_affiliations title journal.id journal.title issue 0 article 122 100692 pub.1134954138 2021 `[[{'raw_affiliation': ['Department of Aerospac...` Review of space habitat designs for long term ... jour.1139377 Progress in Aerospace Sciences NaN 1 article 181 167-189 pub.1130384298 2021 `[[{'raw_affiliation': ['U.S. Naval War College...` Joseph G. Gavin, Jr. and MIT\u2019s contribution to... jour.1134138 Acta Astronautica NaN 2 article 180 650-678 pub.1134475636 2021 `[[{'raw_affiliation': ['Skolkovo Institute of ...` Regolith-based additive manufacturing for sust... jour.1134138 Acta Astronautica NaN 3 article NaN 1-13 pub.1135101079 2021 `[[{'raw_affiliation': ['Centre for Teaching an...` Looking at Gail Jones\u2019s \u201cThe Man in the Moon\u201d ... jour.1137860 Journal of Australian Studies NaN 4 article 21 959 pub.1135057882 2021 `[[{'raw_affiliation': ['School of Artificial I...` Three-Dimensional Model of the Moon with Seman... jour.1033312 Sensors 3 2. Exporting DSL results to GBQ First off, we want to run the full-text search so to extract all relevant publications IDs. Second, we will export the publications IDs to GBQ. NOTE: Pandas provides a handy command to move data to GBQ: DataFrame.to_gbq . %% dslloopdf search publications for \" \\\" moon landing \\\" AND Moon AND \\\" lunar surface \\\" \" return publications [ id ] Starting iteration with limit=1000 skip=0 ... 0-1000 / 11201 (0.43s) 1000-2000 / 11201 (1.01s) 2000-3000 / 11201 (0.69s) 3000-4000 / 11201 (1.03s) 4000-5000 / 11201 (1.30s) 5000-6000 / 11201 (1.58s) 6000-7000 / 11201 (0.33s) 7000-8000 / 11201 (0.25s) 8000-9000 / 11201 (0.27s) 9000-10000 / 11201 (0.64s) 10000-11000 / 11201 (1.05s) 11000-11201 / 11201 (1.80s) === Records extracted: 11201 Row id 0 pub.1128771471 1 pub.1130814402 2 pub.1131658726 3 pub.1124123379 4 pub.1131232278 ... ... 11196 pub.1061739351 11197 pub.1025947790 11198 pub.1091822752 11199 pub.1025757974 11200 pub.1023928923 11201 rows \u00d7 1 columns df = dsl_last_results The command below will add a new table moonlanding to the demo_dsl dataset in GQB. That destination table is entirely up to you of course, so you need to make sure you have write access to the database. DATASET = \"demo_dsl\" table_id = DATASET + \".moonlanding\" df . to_gbq ( table_id , project_id = PROJECTID , if_exists = \"replace\" ) 1it [00:05, 5.05s/it] That's it - you should now be able to go to the online GBQ console and see the new demo_dsl.moonlanding dataset. 3. Querying your new dataset using a JOIN on Dimensions We can now use the publications IDs we imported in order to create a JOIN query on the main Dimensions dataset. This is a bit like creating a 'view' of Dimensions corresponding to the full-text search we have done above. GOAL: Roughly, the results should be the same as the 'publication year' facet in the webapp %% bigquery --project $PROJECTID WITH mypubs AS ( SELECT dim_pubs . * FROM ` dimensions - ai . data_analytics . publications ` dim_pubs JOIN ` ds - data - solutions - gbq . demo_dsl . moonlanding ` dslexport ON dim_pubs . id = dslexport . id ) SELECT COUNT ( id ) as tot , year FROM mypubs GROUP BY year ORDER BY tot DESC Row tot year 0 10052 2003 1 133 2020 2 97 2019 3 70 2015 4 66 2017 5 65 2018 6 63 2009 7 59 2013 rows truncated for display 4. Using GBQ to generate researcher statistics The goal is to generate a table just like the one in the 'researchers' analytical view in the webapp . For each researcher we want to display some extra information: the total number of publications the citations count the total Altmetric Attention Score %% bigquery --project $PROJECTID WITH mypubs AS ( SELECT dim_pubs . * FROM ` dimensions - ai . data_analytics . publications ` dim_pubs JOIN ` ds - data - solutions - gbq . demo_dsl . moonlanding ` dslexport ON dim_pubs . id = dslexport . id ), researchers_metrics AS ( SELECT researcher_id , COUNT ( id ) as publications_count , SUM ( citations_count ) as citations_count , SUM ( altmetrics . score ) as altmetric_sum FROM mypubs , UNNEST ( researcher_ids ) as researcher_id GROUP BY researcher_id ) SELECT * FROM researchers_metrics ORDER BY publications_count DESC Row researcher_id publications_count citations_count altmetric_sum 0 ur.01056354465.10 11 21.0 49.0 1 ur.014402173273.44 6 42.0 10.0 2 ur.012373502003.54 4 63.0 NaN 3 ur.010534421371.14 4 63.0 NaN 4 ur.015145367415.34 4 44.0 1.0 ... ... ... ... ... 1080 ur.0767272510.86 1 5.0 NaN 1081 ur.07637166751.28 1 3.0 NaN 1082 ur.012762707227.21 1 3.0 NaN 1083 ur.010101533313.52 1 NaN 1.0 1084 ur.016406136233.64 1 NaN 16.0 Final step : let's add researchers names and current organization details by joining up data from the GRID table . %% bigquery --project $PROJECTID WITH mypubs AS ( SELECT dim_pubs . * FROM ` dimensions - ai . data_analytics . publications ` dim_pubs JOIN ` ds - data - solutions - gbq . demo_dsl . moonlanding ` dslexport ON dim_pubs . id = dslexport . id ), researchers_metrics AS ( SELECT researcher_id , COUNT ( id ) as publications_count , SUM ( citations_count ) as citations_count , SUM ( altmetrics . score ) as altmetric_sum FROM mypubs , UNNEST ( researcher_ids ) as researcher_id GROUP BY researcher_id ), researchers_full AS ( SELECT researchers_metrics . * , r . first_name , r . last_name , r . total_grants , grid . id as grid_id , grid . name as grid_name , grid . address . city as grid_city , grid . address . country as grid_country FROM researchers_metrics JOIN ` dimensions - ai . data_analytics . researchers ` r ON researchers_metrics . researcher_id = r . id JOIN ` dimensions - ai . data_analytics . grid ` grid ON grid . id = r . current_research_org ) SELECT * FROM researchers_full ORDER BY publications_count DESC Row researcher_id publications_count citations_count altmetric_sum first_name last_name total_grants grid_id grid_name grid_city grid_country 0 ur.01056354465.10 11 21.0 49.0 Roger D Launius 4 grid.1214.6 Smithsonian Institution Washington D.C. United States 1 ur.014402173273.44 6 42.0 10.0 Joseph N Pelton 0 grid.33224.34 International Space University Illkirch-Graffenstaden France 2 ur.010243405673.63 4 14.0 NaN Sachiko Wakabayashi 1 grid.62167.34 Japan Aerospace Exploration Agency Tokyo Japan 3 ur.012503545245.69 4 12.0 1.0 Stephan Theil 2 grid.7551.6 German Aerospace Center Cologne Germany 4 ur.0720745255.73 4 8.0 28.0 Chun-Lai Li 3 grid.450302.0 National Astronomical Observatories Beijing China ... ... ... ... ... ... ... ... ... ... ... ... 886 ur.011430662526.22 1 10.0 NaN Gang Lei 0 grid.458502.e Technical Institute of Physics and Chemistry Beijing China 887 ur.015477605337.38 1 1.0 NaN Olivier Dubois-Matra 0 grid.424669.b European Space Research and Technology Centre Noordwijk-Binnen Netherlands 888 ur.013214745135.53 1 NaN NaN Catherine L Newell 0 grid.26790.3a University of Miami Coral Gables United States 889 ur.014464032227.37 1 41.0 141.0 Andrew M Carton 0 grid.25879.31 University of Pennsylvania Philadelphia United States 890 ur.010610572173.89 1 10.0 NaN Zhan Liu 0 grid.411510.0 China University of Mining and Technology Xuzhou China","title":"From the DSL API to Google BigQuery"},{"location":"tutorials/02-dsl/#from-the-dsl-api-to-google-bigquery","text":"This tutorial demonstrates how to perform a full-text search in Dimensions using the Analytics API and then export the data to Google BigQuery for further analysis. This technique allows to take advantage of the strengths of each of these data products: The Analytics API allows to run full-text searches over the tens of millions documents stored in the Dimensions database. This makes it an ideal tool for identifying a corpus of documents using collections of keywords and/or other filters (note: this is the same functionality available when you search on app.dimensions.ai) The Dimensions on Google BigQuery database allows to run SQL queries of any complexity using a cloud-based environment containing all of the metadata available in Dimensions, thus removing the need to download/analyse the data offline first. This makes is the perfect solution for advanced analytics tasks such as benchmarking, metrics calculations or impact analyses. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . You have a valid Dimensions API account . You have some basic familiarity with Python and Jupyter notebooks .","title":"From the DSL API to Google BigQuery"},{"location":"tutorials/02-dsl/#example-profiling-researchers-linked-to-a-topic","text":"The concrete usecase we'll be looking at involves running a full-text search for \"moon landing\" publications using the DSL API, then creating a corpus in GBQ based on this search (eg see this search ). Once we have the publication corpus available in GBQ, we will extract all associated researchers (=authors). At the same time, we are going to use SQL in order to enrich the results using other metrics (eg more researchers metadata including citations & altmetric).","title":"Example: profiling researchers linked to a topic"},{"location":"tutorials/02-dsl/#getting-started","text":"The following code will load the Python BigQuery library and authenticate you as a valid user. ! pip install google - cloud - bigquery - U -- quiet % load_ext google . cloud . bigquery import sys print ( \"== \\n Authenticating...\" ) if 'google.colab' in sys . modules : from google.colab import auth auth . authenticate_user () print ( '..done (method: Colab)' ) else : from google.cloud import bigquery print ( '..done (method: local credentials)' ) # # PLEASE UPDATE USING YOUR CLOUD PROJECT ID (= the 'billing' account) # MY_PROJECT_ID = \"ds-data-solutions-gbq\" print ( \"== \\n Testing connection..\" ) client = bigquery . Client ( project = MY_PROJECT_ID ) test = client . query ( \"\"\" SELECT COUNT(*) as pubs from `dimensions-ai.data_analytics.publications` \"\"\" ) rows = [ x for x in test . result ()] print ( \"...success!\" ) print ( \"Total publications in Dimensions: \" , rows [ 0 ][ 'pubs' ]) The google.cloud.bigquery extension is already loaded. To reload it, use: %reload_ext google.cloud.bigquery == Authenticating... ..done (method: local credentials) == Testing connection.. ...success! Total publications in Dimensions: 115963650","title":"Getting started"},{"location":"tutorials/02-dsl/#1-connecting-to-the-dsl-api","text":"For more background on the Analytics API and how to work with it, see this tutorial ! pip install dimcli -- quiet import dimcli from dimcli.utils import * import json import sys import pandas as pd # ENDPOINT = \"https://app.dimensions.ai\" USERNAME , PASSWORD = \"\" , \"\" dimcli . login ( USERNAME , PASSWORD , ENDPOINT ) dsl = dimcli . Dsl () Let's try running a sample query. TIP Review the full text search syntax of the Dimensions Search Language. %% dsldf search publications for \" \\\" moon landing \\\" AND Moon AND \\\" lunar surface \\\" \" return publications limit 10 Returned Publications: 10 (total = 11305) \u001b[2mTime: 0.72s\u001b[0m Row type volume pages id year author_affiliations title journal.id journal.title issue 0 article 122 100692 pub.1134954138 2021 `[[{'raw_affiliation': ['Department of Aerospac...` Review of space habitat designs for long term ... jour.1139377 Progress in Aerospace Sciences NaN 1 article 181 167-189 pub.1130384298 2021 `[[{'raw_affiliation': ['U.S. Naval War College...` Joseph G. Gavin, Jr. and MIT\u2019s contribution to... jour.1134138 Acta Astronautica NaN 2 article 180 650-678 pub.1134475636 2021 `[[{'raw_affiliation': ['Skolkovo Institute of ...` Regolith-based additive manufacturing for sust... jour.1134138 Acta Astronautica NaN 3 article NaN 1-13 pub.1135101079 2021 `[[{'raw_affiliation': ['Centre for Teaching an...` Looking at Gail Jones\u2019s \u201cThe Man in the Moon\u201d ... jour.1137860 Journal of Australian Studies NaN 4 article 21 959 pub.1135057882 2021 `[[{'raw_affiliation': ['School of Artificial I...` Three-Dimensional Model of the Moon with Seman... jour.1033312 Sensors 3","title":"1. Connecting to the DSL API"},{"location":"tutorials/02-dsl/#2-exporting-dsl-results-to-gbq","text":"First off, we want to run the full-text search so to extract all relevant publications IDs. Second, we will export the publications IDs to GBQ. NOTE: Pandas provides a handy command to move data to GBQ: DataFrame.to_gbq . %% dslloopdf search publications for \" \\\" moon landing \\\" AND Moon AND \\\" lunar surface \\\" \" return publications [ id ] Starting iteration with limit=1000 skip=0 ... 0-1000 / 11201 (0.43s) 1000-2000 / 11201 (1.01s) 2000-3000 / 11201 (0.69s) 3000-4000 / 11201 (1.03s) 4000-5000 / 11201 (1.30s) 5000-6000 / 11201 (1.58s) 6000-7000 / 11201 (0.33s) 7000-8000 / 11201 (0.25s) 8000-9000 / 11201 (0.27s) 9000-10000 / 11201 (0.64s) 10000-11000 / 11201 (1.05s) 11000-11201 / 11201 (1.80s) === Records extracted: 11201 Row id 0 pub.1128771471 1 pub.1130814402 2 pub.1131658726 3 pub.1124123379 4 pub.1131232278 ... ... 11196 pub.1061739351 11197 pub.1025947790 11198 pub.1091822752 11199 pub.1025757974 11200 pub.1023928923 11201 rows \u00d7 1 columns df = dsl_last_results The command below will add a new table moonlanding to the demo_dsl dataset in GQB. That destination table is entirely up to you of course, so you need to make sure you have write access to the database. DATASET = \"demo_dsl\" table_id = DATASET + \".moonlanding\" df . to_gbq ( table_id , project_id = PROJECTID , if_exists = \"replace\" ) 1it [00:05, 5.05s/it] That's it - you should now be able to go to the online GBQ console and see the new demo_dsl.moonlanding dataset.","title":"2. Exporting DSL results to GBQ"},{"location":"tutorials/02-dsl/#3-querying-your-new-dataset-using-a-join-on-dimensions","text":"We can now use the publications IDs we imported in order to create a JOIN query on the main Dimensions dataset. This is a bit like creating a 'view' of Dimensions corresponding to the full-text search we have done above. GOAL: Roughly, the results should be the same as the 'publication year' facet in the webapp %% bigquery --project $PROJECTID WITH mypubs AS ( SELECT dim_pubs . * FROM ` dimensions - ai . data_analytics . publications ` dim_pubs JOIN ` ds - data - solutions - gbq . demo_dsl . moonlanding ` dslexport ON dim_pubs . id = dslexport . id ) SELECT COUNT ( id ) as tot , year FROM mypubs GROUP BY year ORDER BY tot DESC Row tot year 0 10052 2003 1 133 2020 2 97 2019 3 70 2015 4 66 2017 5 65 2018 6 63 2009 7 59 2013 rows truncated for display","title":"3. Querying your new dataset using a JOIN on Dimensions"},{"location":"tutorials/02-dsl/#4-using-gbq-to-generate-researcher-statistics","text":"The goal is to generate a table just like the one in the 'researchers' analytical view in the webapp . For each researcher we want to display some extra information: the total number of publications the citations count the total Altmetric Attention Score %% bigquery --project $PROJECTID WITH mypubs AS ( SELECT dim_pubs . * FROM ` dimensions - ai . data_analytics . publications ` dim_pubs JOIN ` ds - data - solutions - gbq . demo_dsl . moonlanding ` dslexport ON dim_pubs . id = dslexport . id ), researchers_metrics AS ( SELECT researcher_id , COUNT ( id ) as publications_count , SUM ( citations_count ) as citations_count , SUM ( altmetrics . score ) as altmetric_sum FROM mypubs , UNNEST ( researcher_ids ) as researcher_id GROUP BY researcher_id ) SELECT * FROM researchers_metrics ORDER BY publications_count DESC Row researcher_id publications_count citations_count altmetric_sum 0 ur.01056354465.10 11 21.0 49.0 1 ur.014402173273.44 6 42.0 10.0 2 ur.012373502003.54 4 63.0 NaN 3 ur.010534421371.14 4 63.0 NaN 4 ur.015145367415.34 4 44.0 1.0 ... ... ... ... ... 1080 ur.0767272510.86 1 5.0 NaN 1081 ur.07637166751.28 1 3.0 NaN 1082 ur.012762707227.21 1 3.0 NaN 1083 ur.010101533313.52 1 NaN 1.0 1084 ur.016406136233.64 1 NaN 16.0 Final step : let's add researchers names and current organization details by joining up data from the GRID table . %% bigquery --project $PROJECTID WITH mypubs AS ( SELECT dim_pubs . * FROM ` dimensions - ai . data_analytics . publications ` dim_pubs JOIN ` ds - data - solutions - gbq . demo_dsl . moonlanding ` dslexport ON dim_pubs . id = dslexport . id ), researchers_metrics AS ( SELECT researcher_id , COUNT ( id ) as publications_count , SUM ( citations_count ) as citations_count , SUM ( altmetrics . score ) as altmetric_sum FROM mypubs , UNNEST ( researcher_ids ) as researcher_id GROUP BY researcher_id ), researchers_full AS ( SELECT researchers_metrics . * , r . first_name , r . last_name , r . total_grants , grid . id as grid_id , grid . name as grid_name , grid . address . city as grid_city , grid . address . country as grid_country FROM researchers_metrics JOIN ` dimensions - ai . data_analytics . researchers ` r ON researchers_metrics . researcher_id = r . id JOIN ` dimensions - ai . data_analytics . grid ` grid ON grid . id = r . current_research_org ) SELECT * FROM researchers_full ORDER BY publications_count DESC Row researcher_id publications_count citations_count altmetric_sum first_name last_name total_grants grid_id grid_name grid_city grid_country 0 ur.01056354465.10 11 21.0 49.0 Roger D Launius 4 grid.1214.6 Smithsonian Institution Washington D.C. United States 1 ur.014402173273.44 6 42.0 10.0 Joseph N Pelton 0 grid.33224.34 International Space University Illkirch-Graffenstaden France 2 ur.010243405673.63 4 14.0 NaN Sachiko Wakabayashi 1 grid.62167.34 Japan Aerospace Exploration Agency Tokyo Japan 3 ur.012503545245.69 4 12.0 1.0 Stephan Theil 2 grid.7551.6 German Aerospace Center Cologne Germany 4 ur.0720745255.73 4 8.0 28.0 Chun-Lai Li 3 grid.450302.0 National Astronomical Observatories Beijing China ... ... ... ... ... ... ... ... ... ... ... ... 886 ur.011430662526.22 1 10.0 NaN Gang Lei 0 grid.458502.e Technical Institute of Physics and Chemistry Beijing China 887 ur.015477605337.38 1 1.0 NaN Olivier Dubois-Matra 0 grid.424669.b European Space Research and Technology Centre Noordwijk-Binnen Netherlands 888 ur.013214745135.53 1 NaN NaN Catherine L Newell 0 grid.26790.3a University of Miami Coral Gables United States 889 ur.014464032227.37 1 41.0 141.0 Andrew M Carton 0 grid.25879.31 University of Pennsylvania Philadelphia United States 890 ur.010610572173.89 1 10.0 NaN Zhan Liu 0 grid.411510.0 China University of Mining and Technology Xuzhou China","title":"4. Using GBQ to generate researcher statistics"},{"location":"tutorials/03-dates/","text":"Working with dates Each publication has various dates available. date , year , date_normal , date_online , date_print refer to the publication object. See the documentation to find out more about their meaning. date_imported_gbq refers to when this record was last added to GBQ - this date can be handy if you want to synchronize an external data source to GBQ. date_inserted : this refers to when this records was originally added to Dimensions. This date does not change, even if the record is later adjusted. The following examples show how to work with publications dates. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . The online GBQ console can be used to test the queries below. Comparing date fields Description We'll get started by pulling a selection of the date fields to see their formats: SELECT doi , date , date_normal , year , date_online , date_print , date_imported_gbq , date_inserted FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2010 AND journal . id = \"jour.1115214\" ORDER BY citations_count DESC LIMIT 10 Results Row doi date date_normal year date_online date_print date_imported_gbq date_inserted 0 10.1038/nbt.1621 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 1 10.1038/nbt.1630 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 2 10.1038/nbt.1614 2010-03 2010-03-01 2010 null 2010-03 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 3 10.1038/nbt.1685 2010-10-13 2010-10-13 2010 2010-10-13 2010-10 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 4 10.1038/nbt1210-1248 2010-12-07 2010-12-07 2010 2010-12-07 2010-12 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 5 10.1038/nbt.1755 2010-12-22 2010-12-22 2010 2010-12-22 2011-02 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 6 10.1038/nbt1010-1045 2010-10-13 2010-10-13 2010 2010-10-13 2010-10 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 7 10.1038/nbt.1633 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 8 10.1038/nbt.1667 2010-07-19 2010-07-19 2010 2010-07-19 2010-08 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 9 10.1038/nbt.1641 2010-05-23 2010-05-23 2010 2010-05-23 2010-06 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 The first thing to stick out is that some of the dates are actually timestamps : date_imported_gbq and date_inserted have times attached to the dates. The other important caveat is that some dates aren't actually whole dates: Some values in the date and date_print fields have only a year and month. One of the reasons these different types are important is because can add an extra step when you compare fields to each other. For example, if we wanted to count how many publications were added to Dimensions before their \"publication\" date, it would be intuitive to write a query like this: SELECT COUNT ( id ) FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND date > date_inserted However, we get an error from BigQuery: No matching signature for operator > for argument types: STRING, TIMESTAMP. Supported signature: ANY > ANY at [12:11] . BigQuery won't do the comparison because both sides of the comparison aren't of the same type: The date field is of type STRING , since it doesn't always have a day (or month) attached. The date_normal field solves this for us: It uses the same information as the date field, but it fills in the gaps to make a full DATE entry\u2014so \"2010-03\" in the date field becomes 2010-03-01 in date_normal . But swapping that in doesn't fix our problems either: SELECT COUNT ( id ) FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND date_normal > date_inserted We run into a new variant of the issue now: No matching signature for operator > for argument types: DATE, TIMESTAMP. Supported signature: ANY > ANY at [5:7] . Now date_normal gives us a DATE , but we can't compare that to a TIMESTAMP . Generally, you can mitigate most issues with comparing date fields by converting one of them to match the other , and BigQuery supports multiple functions for manipulating dates and datetimes . This one should do the trick: SELECT COUNT ( id ) FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND date_normal > DATE ( date_inserted ) Results: Row f0_ 1 859011 Number of publications added to Dimensions by month Description Next, we'll use the date_inserted field to count the number of publications added to the Dimensions database per month. date_inserted is of type DATETIME , so we choose from the datetime manipulation functions to round down all dates to the first of the month: SELECT DATETIME_TRUNC ( date_inserted , MONTH ) as added_date , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY added_date ORDER BY added_date DESC LIMIT 5 Results Row added_date countDim 1 2021-04-01 00:00:00 UTC 534043 2 2021-03-01 00:00:00 UTC 746963 3 2021-02-01 00:00:00 UTC 661575 4 2021-01-01 00:00:00 UTC 687764 5 2020-12-01 00:00:00 UTC 828307 We can see the dates have all been collapsed into the first of the month for each paper, but those timestamps that are attached are unhelpful. We can get rid of them by converting date_inserted to a DATE first, and switch to using the DATE_TRUNC function instead: SELECT DATE_TRUNC ( DATE ( date_inserted ), MONTH ) as added_date , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY added_date ORDER BY added_date DESC LIMIT 5 Results Row added_date countDim 1 2021-04-01 534043 2 2021-03-01 746963 3 2021-02-01 661575 4 2021-01-01 687764 5 2020-12-01 828307 That looks much better. If we want to manipulate different parts of the dates separately, we can also use EXTRACT to split things up: SELECT EXTRACT ( MONTH FROM date_inserted ) as added_month , EXTRACT ( YEAR FROM date_inserted ) as added_year , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY added_month , added_year ORDER BY added_year DESC , added_month DESC LIMIT 5 Results Row added_month added_year countDim 1 4 2021 534043 2 3 2021 746963 3 2 2021 661575 4 1 2021 687764 5 12 2020 828307","title":"Working with dates"},{"location":"tutorials/03-dates/#working-with-dates","text":"Each publication has various dates available. date , year , date_normal , date_online , date_print refer to the publication object. See the documentation to find out more about their meaning. date_imported_gbq refers to when this record was last added to GBQ - this date can be handy if you want to synchronize an external data source to GBQ. date_inserted : this refers to when this records was originally added to Dimensions. This date does not change, even if the record is later adjusted. The following examples show how to work with publications dates. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . The online GBQ console can be used to test the queries below.","title":"Working with dates"},{"location":"tutorials/03-dates/#comparing-date-fields","text":"","title":"Comparing date fields"},{"location":"tutorials/03-dates/#description","text":"We'll get started by pulling a selection of the date fields to see their formats: SELECT doi , date , date_normal , year , date_online , date_print , date_imported_gbq , date_inserted FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2010 AND journal . id = \"jour.1115214\" ORDER BY citations_count DESC LIMIT 10 Results Row doi date date_normal year date_online date_print date_imported_gbq date_inserted 0 10.1038/nbt.1621 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 1 10.1038/nbt.1630 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 2 10.1038/nbt.1614 2010-03 2010-03-01 2010 null 2010-03 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 3 10.1038/nbt.1685 2010-10-13 2010-10-13 2010 2010-10-13 2010-10 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 4 10.1038/nbt1210-1248 2010-12-07 2010-12-07 2010 2010-12-07 2010-12 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 5 10.1038/nbt.1755 2010-12-22 2010-12-22 2010 2010-12-22 2011-02 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 6 10.1038/nbt1010-1045 2010-10-13 2010-10-13 2010 2010-10-13 2010-10 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 7 10.1038/nbt.1633 2010-05-02 2010-05-02 2010 2010-05-02 2010-05 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 8 10.1038/nbt.1667 2010-07-19 2010-07-19 2010 2010-07-19 2010-08 2021-02-10 01:09:29+00:00 2017-08-31 12:50:56+00:00 9 10.1038/nbt.1641 2010-05-23 2010-05-23 2010 2010-05-23 2010-06 2021-02-10 00:53:56+00:00 2017-08-31 12:50:56+00:00 The first thing to stick out is that some of the dates are actually timestamps : date_imported_gbq and date_inserted have times attached to the dates. The other important caveat is that some dates aren't actually whole dates: Some values in the date and date_print fields have only a year and month. One of the reasons these different types are important is because can add an extra step when you compare fields to each other. For example, if we wanted to count how many publications were added to Dimensions before their \"publication\" date, it would be intuitive to write a query like this: SELECT COUNT ( id ) FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND date > date_inserted However, we get an error from BigQuery: No matching signature for operator > for argument types: STRING, TIMESTAMP. Supported signature: ANY > ANY at [12:11] . BigQuery won't do the comparison because both sides of the comparison aren't of the same type: The date field is of type STRING , since it doesn't always have a day (or month) attached. The date_normal field solves this for us: It uses the same information as the date field, but it fills in the gaps to make a full DATE entry\u2014so \"2010-03\" in the date field becomes 2010-03-01 in date_normal . But swapping that in doesn't fix our problems either: SELECT COUNT ( id ) FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND date_normal > date_inserted We run into a new variant of the issue now: No matching signature for operator > for argument types: DATE, TIMESTAMP. Supported signature: ANY > ANY at [5:7] . Now date_normal gives us a DATE , but we can't compare that to a TIMESTAMP . Generally, you can mitigate most issues with comparing date fields by converting one of them to match the other , and BigQuery supports multiple functions for manipulating dates and datetimes . This one should do the trick: SELECT COUNT ( id ) FROM ` dimensions - ai . data_analytics . publications ` WHERE year = 2020 AND date_normal > DATE ( date_inserted ) Results: Row f0_ 1 859011","title":"Description"},{"location":"tutorials/03-dates/#number-of-publications-added-to-dimensions-by-month","text":"","title":"Number of publications added to Dimensions by month"},{"location":"tutorials/03-dates/#description_1","text":"Next, we'll use the date_inserted field to count the number of publications added to the Dimensions database per month. date_inserted is of type DATETIME , so we choose from the datetime manipulation functions to round down all dates to the first of the month: SELECT DATETIME_TRUNC ( date_inserted , MONTH ) as added_date , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY added_date ORDER BY added_date DESC LIMIT 5 Results Row added_date countDim 1 2021-04-01 00:00:00 UTC 534043 2 2021-03-01 00:00:00 UTC 746963 3 2021-02-01 00:00:00 UTC 661575 4 2021-01-01 00:00:00 UTC 687764 5 2020-12-01 00:00:00 UTC 828307 We can see the dates have all been collapsed into the first of the month for each paper, but those timestamps that are attached are unhelpful. We can get rid of them by converting date_inserted to a DATE first, and switch to using the DATE_TRUNC function instead: SELECT DATE_TRUNC ( DATE ( date_inserted ), MONTH ) as added_date , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY added_date ORDER BY added_date DESC LIMIT 5 Results Row added_date countDim 1 2021-04-01 534043 2 2021-03-01 746963 3 2021-02-01 661575 4 2021-01-01 687764 5 2020-12-01 828307 That looks much better. If we want to manipulate different parts of the dates separately, we can also use EXTRACT to split things up: SELECT EXTRACT ( MONTH FROM date_inserted ) as added_month , EXTRACT ( YEAR FROM date_inserted ) as added_year , COUNT ( id ) as countDim FROM ` dimensions - ai . data_analytics . publications ` GROUP BY added_month , added_year ORDER BY added_year DESC , added_month DESC LIMIT 5 Results Row added_month added_year countDim 1 4 2021 534043 2 3 2021 746963 3 2 2021 661575 4 1 2021 687764 5 12 2020 828307","title":"Description"},{"location":"tutorials/04-nested/","text":"Working with nested and repeated fields A prominent features of Google BigQuery is their addition of nested and repeated fields to what may otherwise be a familiar SQL paradigm. Both present opportunities to reorganize data within single tables in novel ways, but they can be difficult to get used to. Below, we explain the basics of nested and repeated fields, work through several examples, and provide links to external resources that we've found helpful. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . The online GBQ console can be used to test the queries below. What are they? Repeated fields Repeated fields approximate a \"one-to-many\" relationship and provide an opportunity to define a field that can hold multiple values per row . We can demonstrate this by running a query against the publications table for values in the clinical_trial_ids field: SELECT id , LEFT ( title . preferred , 25 ) AS title , clinical_trial_ids FROM ` dimensions - ai . data_analytics . publications ` WHERE ARRAY_LENGTH ( clinical_trial_ids ) > 0 LIMIT 10 The (heavily truncated) results look something like this: Row id title clinical_trial_ids 1 pub.1003360568 A Randomized, Controlled... NCT00014989 2 pub.1003935609 8568 Prophylactic swallow... NCT00332865 3 pub.1004269292 Clinical Trial Alert... NCT00953940 NCT00970073 NCT00994253 NCT00987103 NCT00974636 4 pub.1004095142 6502 A double-blinded, pl... NCT00219557 NCT00428597 5 pub.1004119511 Intrathecal morphine in a... NCT00119184 You can see that rows 3 and 4 have multiple values in the clinical_trial_ids field, despite all values getting listed within a single row number . UNNEST are implicit 'cross-join' queries, hence only records that have some value in the nested column are represented For example, the query below return less publications that then ones available, because only the ones with research_org_country_names are included (= cross join) Nested fields Nested fields, on their own, are much simpler: They are fields that are linked together as a single entity, like a struct or an object. The title field in the publications table is a good example of this: Rather than a single string indicating the title of the publication, it is a nested field that has two strings within it: \"original\" and \"preferred\" , mostly to accommodate titles expressed in multiple languages. Querying nested fields looks almost identical to querying more conventional ones. For example, with the title field: SELECT id , title FROM ` dimensions - ai . data_analytics . publications ` LIMIT 4 Results: Row id title.preferred title.original 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... \"1+N\"\u5ef6\u4f38\u62a4\u7406\u6a21\u5f0f\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7528\u836f\u4f9d\u4ece\u6027\u53ca\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u7684\u5f71\u54cd 2 pub.1123897378 Clinical observation of the prevention of pressu... \u9f99\u8840\u7aed\u9884\u9632\u6076\u6027\u80bf\u7624\u5f3a\u8feb\u4f53\u4f4d\u60a3\u8005\u538b\u75ae\u7684\u4e34\u5e8a\u89c2\u5bdf 3 pub.1039091814 IV. A new improved silk-reel. null 4 pub.1123920716 Effect of resina draconis for external applica... \u9f99\u8840\u7aed\u80f6\u56ca\u7c89\u5916\u6577\u6cbb\u7597\u538b\u75ae\u7597\u6548\u7684Meta\u5206\u6790 Repeated nested fields This is where things get a little more complicated: One of the main ways nested fields make themselves useful is when they're repeated : So while a repeated field might be an array of strings (clinical trial IDs, for example), they can also be an array of objects. The authors field of the publications table is a good example of this: SELECT id , title . preferred , authors FROM ` dimensions - ai . data_analytics . publications ` LIMIT 3 Results: Row id title.preferred authors.first_name authors.last_name authors.researcher_id 1 pub.1001350088 The T-120/130-12.8 and PT... G.D. Barinberg ur.012510636551.40 A.E. Valamin ur.012211770163.32 Yu. A. Sakhnin ur.010306240353.29 A. Yu. Kultyshev ur.014402311563.25 2 pub.1000116807 Application of Electrorhe... Ken'ichi Koyanagi ur.013307555250.87 Yasuhiro Kakinuma ur.013275435603.18 You can see here that the author information appears the same way as the clinical trial IDs above, except each repeated entry within a row has multiple fields about each author. (There are many more fields that will appear if you query authors; they've been removed here for clarity.) The useful part about using nested fields for the authors, rather than a bunch of repeated fields alone (one for first_name , another repeated field for last_name , etc) is because those nested fields will stay together : For the publication in row 1, the \"A.E.\" first name will always appear alongside the \"Valamin\" last name, rather than shuffling them around like what may happen if you queried them separately. Querying nested fields We'll start writing queries with nested fields alone first, since it's the simplest to do. We actually did it several times in the above examples: The title field in the publications table is a nested field with two fields in it: original and preferred . If you don't specify which values you want, you'll get them all , like this: SELECT id , title FROM ` dimensions - ai . data_analytics . publications ` LIMIT 4 Results: Row id title.preferred title.original 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... \"1+N\"\u5ef6\u4f38\u62a4\u7406\u6a21\u5f0f\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7528\u836f\u4f9d\u4ece\u6027\u53ca\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u7684\u5f71\u54cd 2 pub.1123897378 Clinical observation of the prevention of pressu... \u9f99\u8840\u7aed\u9884\u9632\u6076\u6027\u80bf\u7624\u5f3a\u8feb\u4f53\u4f4d\u60a3\u8005\u538b\u75ae\u7684\u4e34\u5e8a\u89c2\u5bdf 3 pub.1039091814 IV. A new improved silk-reel. null 4 pub.1123920716 Effect of resina draconis for external applica... \u9f99\u8840\u7aed\u80f6\u56ca\u7c89\u5916\u6577\u6cbb\u7597\u538b\u75ae\u7597\u6548\u7684Meta\u5206\u6790 If you wanted only the preferred field of the title , you can specify that using periods. Nested fields can have more nested fields within them, so there may be multiple entries. Luckily, we only need one period for the title: SELECT id , title . preferred FROM ` dimensions - ai . data_analytics . publications ` LIMIT 4 Results: Row id title.preferred 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... 2 pub.1123897378 Clinical observation of the prevention of pressu... 3 pub.1039091814 IV. A new improved silk-reel. 4 pub.1123920716 Effect of resina draconis for external applica... Querying repeated fields Repeated fields are where we need to start using more exotic patterns to extract information. The UNNEST function is the primary tool for the job here\u2014it converts an array of values into rows in a table, which, if necessary, can then be joined to the original table you're querying. Example 1: Checking contents of array We'll start with a simple one: the funder_orgs field in the publications table, which lists GRID IDs indicating which organizations funded the research in the publication. IF we wanted to find publications funded by the Brazilian Agricultural Research Corporation, for example, we can use its GRID ID (grid.460200.0) in a WHERE clause: SELECT type , COUNT ( id ) AS funded_pubs FROM ` dimensions - ai . data_analytics . publications ` WHERE 'grid.460200.0' IN UNNEST ( funder_orgs ) GROUP BY type Results Row type funded_pubs 1 preceeding 23 2 article 6042 3 preprint 21 4 chapter 33 Example 2: Joining tables using a repeated field Queries can also return the contents of repeated fields. Using a CROSS JOIN , the information can be distributed into separate rows, rather than arrays inside single rows. For this example, we'll look at organizations that have funded recent articles published in eLife , a life sciences journal. We'll start by selecting the information we can get from the publications table: SELECT p . id , forg FROM ` dimensions - ai . data_analytics . publications ` AS p CROSS JOIN UNNEST ( funder_orgs ) AS forg -- This is the important line WHERE type = 'article' AND journal . id = 'jour.1046517' -- eLife Row id forg 1 pub.1000035854 grid.14105.31 2 pub.1000321327 grid.48336.3a 3 pub.1000131550 grid.422384.b 4 pub.1000131550 grid.419475.a 5 pub.1000131550 grid.453152.4 6 pub.1000131550 grid.280362.d 7 pub.1000131550 grid.416870.c There are a few things to point out here: First, notice that we're querying a nested field within the journal field on the final line\u2014we want only publications in which the journal field lists an id that matches the one assigned to eLife . We're also using a CROSS JOIN with the funder_orgs field. A cross join returns the Cartesian product of the two tables being joined\u2014to wit, every value on one side of the join (in this case, the publications table) will appear with every matching value from the right side of the join (the \"table\" created by the call to UNNEST(funder_orgs) ). This is demonstrated in lines 3 through 7 of the results above\u2014publication id \"pub.1000131550\" has five different strings in its funder_orgs field, so when we unnest that field, the results contain multiple rows for \"pub.1000131550,\" one for each value unnested from funder_orgs . We're not done yet, however\u2014we have a table that associates every eLife paper with each of its funders, but that's not really useful on its own. If we use group by the forg field (the values unnested from funder_orgs ), we can get a count for each organization, like this: SELECT forg , COUNT ( p . id ) AS funded_pubs FROM ` dimensions - ai . data_analytics . publications ` AS p CROSS JOIN UNNEST ( funder_orgs ) AS forg -- This is the important line WHERE type = 'article' AND journal . id = 'jour.1046517' -- eLife GROUP BY forg ORDER BY funded_pubs DESC LIMIT 5 Results: Row forg funded_pubs 1 grid.280785.0 2329 2 grid.416870.c 1090 3 grid.413575.1 1078 4 grid.452896.4 976 5 grid.48336.3a 893 This is getting better! Now we have the GRID ID of each funder, paired with the number of eLife publications it's funded. However, GRID IDs aren't very readable. We can get organization names by pulling them in from the grid table of organizations data : SELECT forg , grid . name , COUNT ( p . id ) AS funded_pubs FROM ` dimensions - ai . data_analytics . publications ` AS p CROSS JOIN UNNEST ( funder_orgs ) AS forg INNER JOIN ` dimensions - ai . data_analytics . grid ` AS grid -- THIS IS NEW! ON forg = grid . id WHERE type = 'article' AND journal . id = 'jour.1046517' GROUP BY forg , grid . name ORDER BY funded_pubs DESC LIMIT 5 Results: Row forg name funded_pubs 1 grid.280785.0 National Institute of General Medical Sciences 2329 2 grid.416870.c National Institute of Neurological Disorders and Stroke 1090 3 grid.413575.1 Howard Hughes Medical Institute 1078 4 grid.452896.4 European Research Council 976 5 grid.48336.3a National Cancer Institute 893 Now we have the table we wanted: We unnest the values in the funder_orgs field, use those to join the grid table, and return the name of each funder and how many publications it's funded in eLife . Example 3: Querying repeated nested fields Let's pull everything together using the task outlined in example 3 from the query library : combining all author names of a paper into a single string. As described above, the authors field is complicated because it's a repeated field in which each value is a nested field: Each repeat of authors has its own first_name field, its own last_name , and so on. It's easier to see the structure if we start with a simpler query: SELECT id , authors FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' Results (truncated for simplicity): Row id authors.first_name authors.last_name authors.researcher_id 1 pub.1132070778 O Gr\u00e5n\u00e4s ur.01027021415.21 A Mocellin ur.01316620417.40 E S Cardoso null F Burmeister ur.0631574677.49 C Caleman ur.0745346134.45 O Bj\u00f6rneholm ur.0603171002.99 A Naves de Brito ur.01206174227.82 So if we want to bring all the authors together into a single string, there are a lot of discrete steps to take care of: Pull out the first_name and last_name fields for each author in the authors repeated field. Make a new string for each author that combines their first and last name together. Pull together each of these full author names into a new array we'll call author_names . So we go from an array of author objects, each with its own collection of nested fields, into an array of strings , each one representing a single author. Combine all elements in the author_names into one long string . First, we try to make things more readable by using a WITH clause to emulate a temporary table: Within this query, there's a \"table\" called author_array filled with the results of this subquery: SELECT id , ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS authors FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' This is important, because it's where most of the work happens. We start in the middle and work our way outward. This piece takes an array ( authors ) and uses the UNNEST function to create a new table in which each row is a separate author. Then, we take each row of this temporary \"authors\" table and combine each first name with each last name: SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) So we now have a table with a single field\u2014a full name\u2014and each row is one author. We then convert this back into an array : ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS authors The outermost piece of this subquery is just to tie each array of author names to the publication that they authored: WITH author_array AS ( SELECT id , ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS author_names FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' ) So now we have a table called author_array in which each publication ID is associated with an array of author names. It looks like this): Row id author_names 1 pub.1132070778 O Gr\u00e5n\u00e4s A Mocellin E S Cardoso F Burmeister C Caleman O Bj\u00f6rneholm A Naves de Brito Now that we have the author names pulled out of the author objects, we're almost done. The last step is to iterate through each publication ID, take each entry in the author_names array, and push them all together using the ARRAY_TO_STRING function : SELECT id , ARRAY_TO_STRING ( author_names , '; ' ) AS authors_list FROM author_array Results Row id authors_list 1 pub.1132070778 O Gr\u00e5n\u00e4s; A Mocellin; E S Cardoso; F Burmeister; C Caleman; O Bj\u00f6rneholm; A Naves de Brito Be careful There are a few pitfalls to be aware of when working with nested and repeated fields; we outline some of the most common below. Example 4: Repeated fields with null values The trouble with using CROSS JOIN clauses in queries is that they omit all records for which the repeated field has no values : If a paper has zero authors listed, for example, including CROSS JOIN UNNEST(authors) in your query means there won't be any rows for that paper. We can examine this further using the research_org_country_names repeated field: SELECT COUNT ( DISTINCT p . id ) AS tot_articles FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( research_org_country_names ) AS unnested_countries WHERE year = 2000 Results Row tot_articles 1 1063394 We then run the same query without the UNNEST clause: SELECT COUNT ( DISTINCT p . id ) AS tot_articles FROM ` dimensions - ai . data_analytics . publications ` p WHERE year = 2000 Results Row tot_articles 1 1760397 So without the UNNEST , the total publication count is just over 1.7 million. With the unnest, however, it's less than 1.1 million. The gap is explained by publications that have an ID (that's what we're counting), but that do not have any values in the research_org_country_names field. So how can we be sure we aren't excluding records we actually want? In this case, a LEFT JOIN is the way to go : SELECT COUNT ( DISTINCT p . id ) AS tot_articles FROM ` dimensions - ai . data_analytics . publications ` p LEFT JOIN UNNEST ( research_org_country_names ) AS unnested_countries WHERE year = 2000 Results Row tot_articles 1 1760397 Using LEFT JOIN UNNEST(x) instead of CROSS JOIN UNNEST(x) ensures that entries in which x is NULL will still be returned\u2014those will simply have null listed in the unnested_countries field. Example 5: Counting entries too many times While it's helpful that CROSS JOIN UNNEST() gives us all relevant combinations of the selected fields, it can also present hazards if you don't account for which fields may have multiple entries. For this example, we want to examine how many papers were published in PLOS ONE that include an author with the surname \"Smith.\" This query will get us most of the way there: SELECT p . year , COUNT ( p . id ) AS totcount FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( authors ) author WHERE journal . id = 'jour.1037553' -- PLOS ONE AND year >= 2018 AND year <= 2020 AND author . last_name = 'Smith' GROUP BY year ORDER BY year Results: Row year totcount 1 2018 196 2 2019 151 3 2020 155 We start with all publications published in PLOS ONE between 2018 and 2020, then unnest the authors field so we can get to the last_name field. We then include only entries in which last_name='Smith' . However, these yearly totals aren't correct: We're counting the number of entries in the table, and we only have entries in which an author's last name is \"Smith.\" But some papers may have been written by more than one Smith . We can account for this by adding a DISTINCT clause, like this: SELECT p . year , COUNT ( DISTINCT p . id ) AS totcount -- CHANGE IS HERE! FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( authors ) author WHERE journal . id = 'jour.1037553' AND year >= 2018 AND year <= 2020 AND author . last_name = 'Smith' GROUP BY year ORDER BY year Results: Row year totcount 1 2018 189 2 2019 144 3 2020 154 Comparing these results to the previous ones, we can see that there are usually more than 140 papers published with \"Smith\" authors every year, and several papers per year authored by multiple Smiths. This was a straightforward example, but DISTINCT clauses can be a valuable check in more convoluted queries in which you may have multiple cross joins, or you have a cross join in a subquery that is later joined to another table.","title":"Working with nested and repeated fields"},{"location":"tutorials/04-nested/#working-with-nested-and-repeated-fields","text":"A prominent features of Google BigQuery is their addition of nested and repeated fields to what may otherwise be a familiar SQL paradigm. Both present opportunities to reorganize data within single tables in novel ways, but they can be difficult to get used to. Below, we explain the basics of nested and repeated fields, work through several examples, and provide links to external resources that we've found helpful. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . The online GBQ console can be used to test the queries below.","title":"Working with nested and repeated fields"},{"location":"tutorials/04-nested/#what-are-they","text":"","title":"What are they?"},{"location":"tutorials/04-nested/#repeated-fields","text":"Repeated fields approximate a \"one-to-many\" relationship and provide an opportunity to define a field that can hold multiple values per row . We can demonstrate this by running a query against the publications table for values in the clinical_trial_ids field: SELECT id , LEFT ( title . preferred , 25 ) AS title , clinical_trial_ids FROM ` dimensions - ai . data_analytics . publications ` WHERE ARRAY_LENGTH ( clinical_trial_ids ) > 0 LIMIT 10 The (heavily truncated) results look something like this: Row id title clinical_trial_ids 1 pub.1003360568 A Randomized, Controlled... NCT00014989 2 pub.1003935609 8568 Prophylactic swallow... NCT00332865 3 pub.1004269292 Clinical Trial Alert... NCT00953940 NCT00970073 NCT00994253 NCT00987103 NCT00974636 4 pub.1004095142 6502 A double-blinded, pl... NCT00219557 NCT00428597 5 pub.1004119511 Intrathecal morphine in a... NCT00119184 You can see that rows 3 and 4 have multiple values in the clinical_trial_ids field, despite all values getting listed within a single row number . UNNEST are implicit 'cross-join' queries, hence only records that have some value in the nested column are represented For example, the query below return less publications that then ones available, because only the ones with research_org_country_names are included (= cross join)","title":"Repeated fields"},{"location":"tutorials/04-nested/#nested-fields","text":"Nested fields, on their own, are much simpler: They are fields that are linked together as a single entity, like a struct or an object. The title field in the publications table is a good example of this: Rather than a single string indicating the title of the publication, it is a nested field that has two strings within it: \"original\" and \"preferred\" , mostly to accommodate titles expressed in multiple languages. Querying nested fields looks almost identical to querying more conventional ones. For example, with the title field: SELECT id , title FROM ` dimensions - ai . data_analytics . publications ` LIMIT 4 Results: Row id title.preferred title.original 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... \"1+N\"\u5ef6\u4f38\u62a4\u7406\u6a21\u5f0f\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7528\u836f\u4f9d\u4ece\u6027\u53ca\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u7684\u5f71\u54cd 2 pub.1123897378 Clinical observation of the prevention of pressu... \u9f99\u8840\u7aed\u9884\u9632\u6076\u6027\u80bf\u7624\u5f3a\u8feb\u4f53\u4f4d\u60a3\u8005\u538b\u75ae\u7684\u4e34\u5e8a\u89c2\u5bdf 3 pub.1039091814 IV. A new improved silk-reel. null 4 pub.1123920716 Effect of resina draconis for external applica... \u9f99\u8840\u7aed\u80f6\u56ca\u7c89\u5916\u6577\u6cbb\u7597\u538b\u75ae\u7597\u6548\u7684Meta\u5206\u6790","title":"Nested fields"},{"location":"tutorials/04-nested/#repeated-nested-fields","text":"This is where things get a little more complicated: One of the main ways nested fields make themselves useful is when they're repeated : So while a repeated field might be an array of strings (clinical trial IDs, for example), they can also be an array of objects. The authors field of the publications table is a good example of this: SELECT id , title . preferred , authors FROM ` dimensions - ai . data_analytics . publications ` LIMIT 3 Results: Row id title.preferred authors.first_name authors.last_name authors.researcher_id 1 pub.1001350088 The T-120/130-12.8 and PT... G.D. Barinberg ur.012510636551.40 A.E. Valamin ur.012211770163.32 Yu. A. Sakhnin ur.010306240353.29 A. Yu. Kultyshev ur.014402311563.25 2 pub.1000116807 Application of Electrorhe... Ken'ichi Koyanagi ur.013307555250.87 Yasuhiro Kakinuma ur.013275435603.18 You can see here that the author information appears the same way as the clinical trial IDs above, except each repeated entry within a row has multiple fields about each author. (There are many more fields that will appear if you query authors; they've been removed here for clarity.) The useful part about using nested fields for the authors, rather than a bunch of repeated fields alone (one for first_name , another repeated field for last_name , etc) is because those nested fields will stay together : For the publication in row 1, the \"A.E.\" first name will always appear alongside the \"Valamin\" last name, rather than shuffling them around like what may happen if you queried them separately.","title":"Repeated nested fields"},{"location":"tutorials/04-nested/#querying-nested-fields","text":"We'll start writing queries with nested fields alone first, since it's the simplest to do. We actually did it several times in the above examples: The title field in the publications table is a nested field with two fields in it: original and preferred . If you don't specify which values you want, you'll get them all , like this: SELECT id , title FROM ` dimensions - ai . data_analytics . publications ` LIMIT 4 Results: Row id title.preferred title.original 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... \"1+N\"\u5ef6\u4f38\u62a4\u7406\u6a21\u5f0f\u5bf92\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7528\u836f\u4f9d\u4ece\u6027\u53ca\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u7684\u5f71\u54cd 2 pub.1123897378 Clinical observation of the prevention of pressu... \u9f99\u8840\u7aed\u9884\u9632\u6076\u6027\u80bf\u7624\u5f3a\u8feb\u4f53\u4f4d\u60a3\u8005\u538b\u75ae\u7684\u4e34\u5e8a\u89c2\u5bdf 3 pub.1039091814 IV. A new improved silk-reel. null 4 pub.1123920716 Effect of resina draconis for external applica... \u9f99\u8840\u7aed\u80f6\u56ca\u7c89\u5916\u6577\u6cbb\u7597\u538b\u75ae\u7597\u6548\u7684Meta\u5206\u6790 If you wanted only the preferred field of the title , you can specify that using periods. Nested fields can have more nested fields within them, so there may be multiple entries. Luckily, we only need one period for the title: SELECT id , title . preferred FROM ` dimensions - ai . data_analytics . publications ` LIMIT 4 Results: Row id title.preferred 1 pub.1123921006 Effects of \"1+N\" extended nursing on medication comp... 2 pub.1123897378 Clinical observation of the prevention of pressu... 3 pub.1039091814 IV. A new improved silk-reel. 4 pub.1123920716 Effect of resina draconis for external applica...","title":"Querying nested fields"},{"location":"tutorials/04-nested/#querying-repeated-fields","text":"Repeated fields are where we need to start using more exotic patterns to extract information. The UNNEST function is the primary tool for the job here\u2014it converts an array of values into rows in a table, which, if necessary, can then be joined to the original table you're querying.","title":"Querying repeated fields"},{"location":"tutorials/04-nested/#example-1-checking-contents-of-array","text":"We'll start with a simple one: the funder_orgs field in the publications table, which lists GRID IDs indicating which organizations funded the research in the publication. IF we wanted to find publications funded by the Brazilian Agricultural Research Corporation, for example, we can use its GRID ID (grid.460200.0) in a WHERE clause: SELECT type , COUNT ( id ) AS funded_pubs FROM ` dimensions - ai . data_analytics . publications ` WHERE 'grid.460200.0' IN UNNEST ( funder_orgs ) GROUP BY type Results Row type funded_pubs 1 preceeding 23 2 article 6042 3 preprint 21 4 chapter 33","title":"Example 1: Checking contents of array"},{"location":"tutorials/04-nested/#example-2-joining-tables-using-a-repeated-field","text":"Queries can also return the contents of repeated fields. Using a CROSS JOIN , the information can be distributed into separate rows, rather than arrays inside single rows. For this example, we'll look at organizations that have funded recent articles published in eLife , a life sciences journal. We'll start by selecting the information we can get from the publications table: SELECT p . id , forg FROM ` dimensions - ai . data_analytics . publications ` AS p CROSS JOIN UNNEST ( funder_orgs ) AS forg -- This is the important line WHERE type = 'article' AND journal . id = 'jour.1046517' -- eLife Row id forg 1 pub.1000035854 grid.14105.31 2 pub.1000321327 grid.48336.3a 3 pub.1000131550 grid.422384.b 4 pub.1000131550 grid.419475.a 5 pub.1000131550 grid.453152.4 6 pub.1000131550 grid.280362.d 7 pub.1000131550 grid.416870.c There are a few things to point out here: First, notice that we're querying a nested field within the journal field on the final line\u2014we want only publications in which the journal field lists an id that matches the one assigned to eLife . We're also using a CROSS JOIN with the funder_orgs field. A cross join returns the Cartesian product of the two tables being joined\u2014to wit, every value on one side of the join (in this case, the publications table) will appear with every matching value from the right side of the join (the \"table\" created by the call to UNNEST(funder_orgs) ). This is demonstrated in lines 3 through 7 of the results above\u2014publication id \"pub.1000131550\" has five different strings in its funder_orgs field, so when we unnest that field, the results contain multiple rows for \"pub.1000131550,\" one for each value unnested from funder_orgs . We're not done yet, however\u2014we have a table that associates every eLife paper with each of its funders, but that's not really useful on its own. If we use group by the forg field (the values unnested from funder_orgs ), we can get a count for each organization, like this: SELECT forg , COUNT ( p . id ) AS funded_pubs FROM ` dimensions - ai . data_analytics . publications ` AS p CROSS JOIN UNNEST ( funder_orgs ) AS forg -- This is the important line WHERE type = 'article' AND journal . id = 'jour.1046517' -- eLife GROUP BY forg ORDER BY funded_pubs DESC LIMIT 5 Results: Row forg funded_pubs 1 grid.280785.0 2329 2 grid.416870.c 1090 3 grid.413575.1 1078 4 grid.452896.4 976 5 grid.48336.3a 893 This is getting better! Now we have the GRID ID of each funder, paired with the number of eLife publications it's funded. However, GRID IDs aren't very readable. We can get organization names by pulling them in from the grid table of organizations data : SELECT forg , grid . name , COUNT ( p . id ) AS funded_pubs FROM ` dimensions - ai . data_analytics . publications ` AS p CROSS JOIN UNNEST ( funder_orgs ) AS forg INNER JOIN ` dimensions - ai . data_analytics . grid ` AS grid -- THIS IS NEW! ON forg = grid . id WHERE type = 'article' AND journal . id = 'jour.1046517' GROUP BY forg , grid . name ORDER BY funded_pubs DESC LIMIT 5 Results: Row forg name funded_pubs 1 grid.280785.0 National Institute of General Medical Sciences 2329 2 grid.416870.c National Institute of Neurological Disorders and Stroke 1090 3 grid.413575.1 Howard Hughes Medical Institute 1078 4 grid.452896.4 European Research Council 976 5 grid.48336.3a National Cancer Institute 893 Now we have the table we wanted: We unnest the values in the funder_orgs field, use those to join the grid table, and return the name of each funder and how many publications it's funded in eLife .","title":"Example 2: Joining tables using a repeated field"},{"location":"tutorials/04-nested/#example-3-querying-repeated-nested-fields","text":"Let's pull everything together using the task outlined in example 3 from the query library : combining all author names of a paper into a single string. As described above, the authors field is complicated because it's a repeated field in which each value is a nested field: Each repeat of authors has its own first_name field, its own last_name , and so on. It's easier to see the structure if we start with a simpler query: SELECT id , authors FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' Results (truncated for simplicity): Row id authors.first_name authors.last_name authors.researcher_id 1 pub.1132070778 O Gr\u00e5n\u00e4s ur.01027021415.21 A Mocellin ur.01316620417.40 E S Cardoso null F Burmeister ur.0631574677.49 C Caleman ur.0745346134.45 O Bj\u00f6rneholm ur.0603171002.99 A Naves de Brito ur.01206174227.82 So if we want to bring all the authors together into a single string, there are a lot of discrete steps to take care of: Pull out the first_name and last_name fields for each author in the authors repeated field. Make a new string for each author that combines their first and last name together. Pull together each of these full author names into a new array we'll call author_names . So we go from an array of author objects, each with its own collection of nested fields, into an array of strings , each one representing a single author. Combine all elements in the author_names into one long string . First, we try to make things more readable by using a WITH clause to emulate a temporary table: Within this query, there's a \"table\" called author_array filled with the results of this subquery: SELECT id , ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS authors FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' This is important, because it's where most of the work happens. We start in the middle and work our way outward. This piece takes an array ( authors ) and uses the UNNEST function to create a new table in which each row is a separate author. Then, we take each row of this temporary \"authors\" table and combine each first name with each last name: SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) So we now have a table with a single field\u2014a full name\u2014and each row is one author. We then convert this back into an array : ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS authors The outermost piece of this subquery is just to tie each array of author names to the publication that they authored: WITH author_array AS ( SELECT id , ARRAY ( SELECT CONCAT ( first_name , \" \" , last_name ) FROM UNNEST ( authors ) ) AS author_names FROM ` dimensions - ai . data_analytics . publications ` WHERE id = 'pub.1132070778' ) So now we have a table called author_array in which each publication ID is associated with an array of author names. It looks like this): Row id author_names 1 pub.1132070778 O Gr\u00e5n\u00e4s A Mocellin E S Cardoso F Burmeister C Caleman O Bj\u00f6rneholm A Naves de Brito Now that we have the author names pulled out of the author objects, we're almost done. The last step is to iterate through each publication ID, take each entry in the author_names array, and push them all together using the ARRAY_TO_STRING function : SELECT id , ARRAY_TO_STRING ( author_names , '; ' ) AS authors_list FROM author_array Results Row id authors_list 1 pub.1132070778 O Gr\u00e5n\u00e4s; A Mocellin; E S Cardoso; F Burmeister; C Caleman; O Bj\u00f6rneholm; A Naves de Brito","title":"Example 3: Querying repeated nested fields"},{"location":"tutorials/04-nested/#be-careful","text":"There are a few pitfalls to be aware of when working with nested and repeated fields; we outline some of the most common below.","title":"Be careful"},{"location":"tutorials/04-nested/#example-4-repeated-fields-with-null-values","text":"The trouble with using CROSS JOIN clauses in queries is that they omit all records for which the repeated field has no values : If a paper has zero authors listed, for example, including CROSS JOIN UNNEST(authors) in your query means there won't be any rows for that paper. We can examine this further using the research_org_country_names repeated field: SELECT COUNT ( DISTINCT p . id ) AS tot_articles FROM ` dimensions - ai . data_analytics . publications ` p , UNNEST ( research_org_country_names ) AS unnested_countries WHERE year = 2000 Results Row tot_articles 1 1063394 We then run the same query without the UNNEST clause: SELECT COUNT ( DISTINCT p . id ) AS tot_articles FROM ` dimensions - ai . data_analytics . publications ` p WHERE year = 2000 Results Row tot_articles 1 1760397 So without the UNNEST , the total publication count is just over 1.7 million. With the unnest, however, it's less than 1.1 million. The gap is explained by publications that have an ID (that's what we're counting), but that do not have any values in the research_org_country_names field. So how can we be sure we aren't excluding records we actually want? In this case, a LEFT JOIN is the way to go : SELECT COUNT ( DISTINCT p . id ) AS tot_articles FROM ` dimensions - ai . data_analytics . publications ` p LEFT JOIN UNNEST ( research_org_country_names ) AS unnested_countries WHERE year = 2000 Results Row tot_articles 1 1760397 Using LEFT JOIN UNNEST(x) instead of CROSS JOIN UNNEST(x) ensures that entries in which x is NULL will still be returned\u2014those will simply have null listed in the unnested_countries field.","title":"Example 4: Repeated fields with null values"},{"location":"tutorials/04-nested/#example-5-counting-entries-too-many-times","text":"While it's helpful that CROSS JOIN UNNEST() gives us all relevant combinations of the selected fields, it can also present hazards if you don't account for which fields may have multiple entries. For this example, we want to examine how many papers were published in PLOS ONE that include an author with the surname \"Smith.\" This query will get us most of the way there: SELECT p . year , COUNT ( p . id ) AS totcount FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( authors ) author WHERE journal . id = 'jour.1037553' -- PLOS ONE AND year >= 2018 AND year <= 2020 AND author . last_name = 'Smith' GROUP BY year ORDER BY year Results: Row year totcount 1 2018 196 2 2019 151 3 2020 155 We start with all publications published in PLOS ONE between 2018 and 2020, then unnest the authors field so we can get to the last_name field. We then include only entries in which last_name='Smith' . However, these yearly totals aren't correct: We're counting the number of entries in the table, and we only have entries in which an author's last name is \"Smith.\" But some papers may have been written by more than one Smith . We can account for this by adding a DISTINCT clause, like this: SELECT p . year , COUNT ( DISTINCT p . id ) AS totcount -- CHANGE IS HERE! FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( authors ) author WHERE journal . id = 'jour.1037553' AND year >= 2018 AND year <= 2020 AND author . last_name = 'Smith' GROUP BY year ORDER BY year Results: Row year totcount 1 2018 189 2 2019 144 3 2020 154 Comparing these results to the previous ones, we can see that there are usually more than 140 papers published with \"Smith\" authors every year, and several papers per year authored by multiple Smiths. This was a straightforward example, but DISTINCT clauses can be a valuable check in more convoluted queries in which you may have multiple cross joins, or you have a cross join in a subquery that is later joined to another table.","title":"Example 5: Counting entries too many times"},{"location":"tutorials/05-topic_clusters/","text":"Basic Topic Clustering using TensorFlow and BigQuery ML In this tutorial we will implement a basic topic clustering on publications, generating text embeddings using a pre-trained TensorFlow model and creating the groupings via K-means clustering provided by BigQuery ML. This tutorial utilises datasets which are only available to Dimensions on BigQuery customers. For this specific example we will be analysing the publications of New Zealand\u2019s top 8 universities from 2016 onwards. The example below is based around using a Python Notebook which utilises BigQuery iPython magic commands to execute BigQuery SQL statements. The basic steps taken are: Setup the Python environment and BigQuery access. Extract titles/abstracts for the publications of interest from Big Query. Use TensorFlow to generate word embeddings from the titles/abstracts. Export these embedding vectors back into GBQ and cleanup the format of them. Create k-means models using BigQuery ML (multiple models for different cluster counts). Compare the different k-means models and select the most appropriate. Associate publications back to each cluster. Determine the topics/concepts associated with each cluster. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . You have some basic familiarity with Python and Jupyter notebooks . 1. Setup Google BigQuery access The initial setup is to authenticate your Google Account for accessing GCP resources. This account must be the account which has access to the Dimensions BigQuery datasets. It must also must be allowed to execute and pay for GBQ queries. In the code example below project_id should be replaced with the GCP project identifier which you own or can access GBQ resources on. from google.colab import auth auth . authenticate_user () project_id = \"my-gbq-project\" # replace with GCP project to bill against % load_ext google . cloud . bigquery The project referenced in the examples below is my-gbq-project , anytime that is contained within Python code or SQL statements it should be replaced with your own projects identifier. The SQL and Python code below also assumes that two datasets exist within the GBQ project ( project_id ). temp which holds intermediate tables created during the setup phase. clustering which holds the tables we will create models from and the final models we create. 2. Gather titles/abstracts for publications The first step we will undertake is building a DataFrame which contains the titles and abstracts from publications. We will use the following BigQuery SQL statement to extract the data: WITH target_orgs AS ( SELECT org FROM UNNEST ([ \"grid.252547.3\" , -- AUT \"grid.9654.e\" , -- UoA \"grid.21006.35\" , -- Canterbury \"grid.29980.3a\" , -- Otago \"grid.267827.e\" , -- Vic \"grid.148374.d\" , -- Massey \"grid.49481.30\" , -- Waikato \"grid.16488.33\" -- Lincoln ]) org ) SELECT id , ANY_VALUE ( title ) as title , ANY_VALUE ( abstract ) as abstract , ARRAY_AGG ( org ) as orgs FROM ( SELECT id , title . preferred as title , abstract . preferred as abstract , org as org FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( research_orgs ) org RIGHT JOIN target_orgs t ON t . org = org WHERE p . year > 2015 AND abstract . preferred is not null ) GROUP BY id To execute the SQL statement above and save the results into a DataFrame we can use the BigQuery iPython magic command: %% bigquery -- params $ bq_params -- project $ project_id pubs # SQL statement from above Taking a look at the generated DataFrame we can see what is present and verify that the data matches our expectations. At this point is may also be useful to save the data off to a \u201cpickle\u201d file allowing quick reloading of the dataset and skipping this step in the future when re-running the steps below. 3. Generate word embeddings for each publication The next stage is to generate word embeddings for each publication\u2019s title and abstract. Word embedding models attempt to map words or phrases from a vocabulary into to vectors of real numbers. These word embeddings can then used in language modelling and feature learning natural language processing (NLP) techniques. Ideally, the end result is that publications with abstracts representing the same topics and concepts will be near one another within the reduced vector space generated through the word embedding process. For this example we will use TensorFlow and the Universal Sentence Encoder model to generate our word embeddings. The input into the model is variable length English text and it will generate a 512 dimensional vector. The embeddings should result in a vector representation which provides a reasonable approximation to semantic similarity between publications based on the content of the abstracts. import tensorflow as tf import tensorflow_hub as hub embed = hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) def get_embed_title ( titles ): return embed ( titles ) def get_embed_abstract ( abstracts , delimiter = \" \\n \" ): def _map_fn ( a ): t = tf . cast ( a , tf . string ) t = tf . strings . split ( t , sep = delimiter ) e = embed ( t ) e = tf . reduce_mean ( e , axis = 0 ) return tf . squeeze ( e ) return tf . map_fn ( _map_fn , abstracts , dtype = tf . float32 ) def process ( titles , abstracts ): title_embed = get_embed_title ( titles ) abstract_embed = get_embed_abstract ( abstracts ) output_features = { 'title_embed' : title_embed , 'abstract_embed' : abstract_embed , } return output_features r = process ( pubs [ \"title\" ], pubs [ \"abstract\" ]) Note The hub.load function execution may take a little while on the first run as it needs to download the USE model files. Taking a look at the generated embeddings, we can see that for each publication we have created two arrays of floating point values each 512 items long. The next step is to save these back into a temporary table in BigQuery so that we can apply k-means clustering to attempt to find groups of semantic similar publications. 4. Save embedding vectors to a BigQuery table The next stage is to save the generated title and abstract embedding vectors back into a BigQuery table. new_pubs = pubs . copy () table_name = \"temp.semantic_clustering\" # Insert into the new DataFrame columns for title and abstract embeddings. new_pubs [ \"title_embed\" ] = r [ \"title_embed\" ] . numpy () . tolist () new_pubs [ \"abstract_embed\" ] = r [ \"abstract_embed\" ] . numpy () . tolist () # Save the DataFrame to a pickle file and into a GBQ table. new_pubs . to_pickle ( \"/content/new_pubs.pkl\" ) new_pubs . to_gbq ( table_name , project_id , chunksize = 2500 ) One issue with the imported data is that the vectors for the embeddings is a string rather than a field containing repeated floating-point values. The SQL statement below however creates a new table that converts the string into a proper array of real numbers. CREATE OR REPLACE TABLE ` my - gbq - project . clustering . nz_pubs_with_embeddings ` AS SELECT id , title , abstract , REGEXP_EXTRACT_ALL ( orgs , r \"\\'([^\\s,\\']+)\\'\" ) as orgs , ( SELECT ARRAY_AGG ( CAST ( v as FLOAT64 ) ORDER BY o asc ) FROM UNNEST ( SPLIT ( REGEXP_EXTRACT ( title_embed , r \"^\\[(.*)\\]$\" ), \", \" )) v WITH OFFSET o ) AS title_embed , ( SELECT ARRAY_AGG ( CAST ( v as FLOAT64 ) ORDER BY o asc ) FROM UNNEST ( SPLIT ( REGEXP_EXTRACT ( abstract_embed , r \"^\\[(.*)\\]$\" ), \", \" )) v WITH OFFSET o ) AS abstract_embed FROM ` my - gbq - project . temp . semantic_clustering ` Taking a look at the table within Google BigQuery (web interface) we can see the schema. It contains title_embed and abstract_embed both of which are repeated fields of FLOAT type. 5. Create k-means cluster models Warning The example BigQuery SQL statement below use BigQuery ML which has a different charging model when it comes to creating models. Please keep this in mind and approximate how much it could potentially cost before executing any of the SQL statements. Pricing details are available from Google here: BigQuery ML Pricing . The most time consuming and computationally expensive part of this example is the clustering process itself. Luckily we can utilise BigQuery ML to create the models and create the clusters of publications based on the word embeddings we have created previously. In this example we will use k-means clustering to attempt to assign each publication to a grouping of semantically similar publications (based on abstracts). Essentially k-means clustering attempts to partition the individual items using Euclidean distance as the metric and minimising the within cluster sum of squares (ie. minimise squared errors). The primary input parameter that controls the k-means models is the number of partitions (ie. how many clusters we want to partition into). We can use some crude hyper-parameter tuning through creating numerous k-means models on our dataset, attempting to determine what a \u201cgood\u201d number of clusters may look like for our set of data. It is important to understand however that because of the approach k-means clustering utilises when assigning items to partitions it is important to analysis the results as a local minimums are a possibility. DECLARE NUM_CLUSTERS INT64 DEFAULT 10 ; DECLARE MODEL_NAME STRING ; WHILE NUM_CLUSTERS < 60 DO SET MODEL_NAME = CONCAT ( 'my-gbq-project.clustering.model_nz_pubs_' , CAST ( NUM_CLUSTERS AS STRING )); EXECUTE IMMEDIATE format ( \"\"\" CREATE OR REPLACE MODEL `%s` OPTIONS(model_type='kmeans', num_clusters = %d, DISTANCE_TYPE = 'cosine', kmeans_init_method = 'KMEANS++') AS WITH data AS ( SELECT abstract_embed[OFFSET(0)] as abstract0, abstract_embed[OFFSET(1)] as abstract1, abstract_embed[OFFSET(2)] as abstract2, abstract_embed[OFFSET(3)] as abstract3, abstract_embed[OFFSET(4)] as abstract4, abstract_embed[OFFSET(5)] as abstract5, abstract_embed[OFFSET(6)] as abstract6, abstract_embed[OFFSET(7)] as abstract7, ... abstract_embed[OFFSET(510)] as abstract510, abstract_embed[OFFSET(511)] as abstract511 FROM `my-gbq-project.clustering.nz_pubs_with_embeddings` ) SELECT * FROM data; \"\"\" , MODEL_NAME , NUM_CLUSTERS ); SET NUM_CLUSTERS = NUM_CLUSTERS + 5 ; END WHILE Note The SQL statement above has been truncated. The abstract_embed[OFFSET(n)] as abstract_n lines have been omitted and goes from 0 to 511 uninterrupted and has been abbreviated for the purposes of brevity in the tutorial write-up. The full SQL is available here . Creating the models may take a little while as it must apply the k-means algorithm over our dataset for each of the different cluster count parameter values. The next step is to analyse the results for all of the models created. We can evaluate each of the models, returning the Davies\u2013Bouldin index and the mean squared distance. Determining the optimal number of clusters is outside the scope of this tutorial, however a common approach is using the \u201cElbow method\u201d . # StandardSQL SELECT 60 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_60 ` ) UNION ALL SELECT 55 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_55 ` ) UNION ALL SELECT 50 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_50 ` ) UNION ALL SELECT 45 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_45 ` ) UNION ALL SELECT 40 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_40 ` ) UNION ALL SELECT 35 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_35 ` ) UNION ALL SELECT 30 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_30 ` ) UNION ALL SELECT 25 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_25 ` ) UNION ALL SELECT 20 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_20 ` ) UNION ALL SELECT 15 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_15 ` ) UNION ALL SELECT 10 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_10 ` ) 6. Publication to cluster assignment Using k=55 ( my-gbq-project.clustering.model_nz_pubs_55 ) as the cluster count from above we can now use the k-means model we created in BigQuery to determine which publications are assigned to each different centroid (cluster). Note below that the input data into the model matches the data structure that was used during the creation of the model itself (the abstracts embedding vector exploded into 512 individual string inputs). CREATE TABLE ` my - gbq - project . clustering . pubs_assigned_55 ` AS WITH data AS ( SELECT id , abstract_embed [ OFFSET ( 0 )] as abstract0 , abstract_embed [ OFFSET ( 1 )] as abstract1 , abstract_embed [ OFFSET ( 2 )] as abstract2 , ... abstract_embed [ OFFSET ( 510 )] as abstract510 , abstract_embed [ OFFSET ( 511 )] as abstract511 FROM ` my - gbq - project . clustering . nz_pubs_with_embeddings ` ) SELECT id , CENTROID_ID , NEAREST_CENTROIDS_DISTANCE FROM ML . PREDICT ( MODEL ` my - gbq - project . clustering . model_nz_pubs_55 ` , ( SELECT * FROM data )) Taking a look at the resultant table we see that each publication has been assigned to a primary cluster (centroid identifier) as well as a listing of the closest centroids as well as the Euclidean distance to the centroid. 7. Determine concepts/topics for each cluster The next stage is to try and determine the topics and concepts associated with each of the 55 clusters identified within the assigned publications above. Some publications have concepts extracted via NLP processing of the full-text. One approach to determining the concepts associated with each cluster is to aggregate these extracted concepts from for all publications within the same cluster. SELECT ANY_VALUE ( centroid ) as centroid_id , cluster_id as cluster_name , ARRAY_AGG ( concept order by ordering ) as concepts FROM ( SELECT * FROM ( SELECT * , ROW_NUMBER () OVER ( PARTITION BY cluster_id ORDER BY count desc ) as ordering FROM ( SELECT ANY_VALUE ( centroid ) as centroid , a . cluster_id as cluster_id , concept , COUNT ( * ) as count FROM ( SELECT a . id , a . CENTROID_ID as centroid , CONCAT ( \"Cluster-\" , CAST ( a . CENTROID_ID as STRING )) as cluster_id , ( SELECT ARRAY_AGG ( LOWER ( c . concept )) FROM UNNEST ( p . concepts ) c ) as concetps FROM ` my - gbq - project . clustering . pubs_assigned_55 ` a LEFT JOIN ` dimensions - ai . data_analytics . publications ` p ON p . id = a . id ) a , UNNEST ( a . concetps ) concept GROUP BY a . cluster_id , concept ) ORDER BY cluster_id desc , ordering asc ) WHERE ordering <= 20 ) GROUP BY cluster_id Taking a look at the results we can see for each centroid the 20 top concepts (by occurrence over all publications).","title":"Basic Topic Clustering using TensorFlow and BigQuery ML"},{"location":"tutorials/05-topic_clusters/#basic-topic-clustering-using-tensorflow-and-bigquery-ml","text":"In this tutorial we will implement a basic topic clustering on publications, generating text embeddings using a pre-trained TensorFlow model and creating the groupings via K-means clustering provided by BigQuery ML. This tutorial utilises datasets which are only available to Dimensions on BigQuery customers. For this specific example we will be analysing the publications of New Zealand\u2019s top 8 universities from 2016 onwards. The example below is based around using a Python Notebook which utilises BigQuery iPython magic commands to execute BigQuery SQL statements. The basic steps taken are: Setup the Python environment and BigQuery access. Extract titles/abstracts for the publications of interest from Big Query. Use TensorFlow to generate word embeddings from the titles/abstracts. Export these embedding vectors back into GBQ and cleanup the format of them. Create k-means models using BigQuery ML (multiple models for different cluster counts). Compare the different k-means models and select the most appropriate. Associate publications back to each cluster. Determine the topics/concepts associated with each cluster. Prerequisites In order to run this tutorial, please ensure that: You have a valid Dimensions on Google BigQuery account and have configured a Google Cloud project . You have some basic familiarity with Python and Jupyter notebooks .","title":"Basic Topic Clustering using TensorFlow and BigQuery ML"},{"location":"tutorials/05-topic_clusters/#1-setup-google-bigquery-access","text":"The initial setup is to authenticate your Google Account for accessing GCP resources. This account must be the account which has access to the Dimensions BigQuery datasets. It must also must be allowed to execute and pay for GBQ queries. In the code example below project_id should be replaced with the GCP project identifier which you own or can access GBQ resources on. from google.colab import auth auth . authenticate_user () project_id = \"my-gbq-project\" # replace with GCP project to bill against % load_ext google . cloud . bigquery The project referenced in the examples below is my-gbq-project , anytime that is contained within Python code or SQL statements it should be replaced with your own projects identifier. The SQL and Python code below also assumes that two datasets exist within the GBQ project ( project_id ). temp which holds intermediate tables created during the setup phase. clustering which holds the tables we will create models from and the final models we create.","title":"1. Setup Google BigQuery access"},{"location":"tutorials/05-topic_clusters/#2-gather-titlesabstracts-for-publications","text":"The first step we will undertake is building a DataFrame which contains the titles and abstracts from publications. We will use the following BigQuery SQL statement to extract the data: WITH target_orgs AS ( SELECT org FROM UNNEST ([ \"grid.252547.3\" , -- AUT \"grid.9654.e\" , -- UoA \"grid.21006.35\" , -- Canterbury \"grid.29980.3a\" , -- Otago \"grid.267827.e\" , -- Vic \"grid.148374.d\" , -- Massey \"grid.49481.30\" , -- Waikato \"grid.16488.33\" -- Lincoln ]) org ) SELECT id , ANY_VALUE ( title ) as title , ANY_VALUE ( abstract ) as abstract , ARRAY_AGG ( org ) as orgs FROM ( SELECT id , title . preferred as title , abstract . preferred as abstract , org as org FROM ` dimensions - ai . data_analytics . publications ` p CROSS JOIN UNNEST ( research_orgs ) org RIGHT JOIN target_orgs t ON t . org = org WHERE p . year > 2015 AND abstract . preferred is not null ) GROUP BY id To execute the SQL statement above and save the results into a DataFrame we can use the BigQuery iPython magic command: %% bigquery -- params $ bq_params -- project $ project_id pubs # SQL statement from above Taking a look at the generated DataFrame we can see what is present and verify that the data matches our expectations. At this point is may also be useful to save the data off to a \u201cpickle\u201d file allowing quick reloading of the dataset and skipping this step in the future when re-running the steps below.","title":"2. Gather titles/abstracts for publications"},{"location":"tutorials/05-topic_clusters/#3-generate-word-embeddings-for-each-publication","text":"The next stage is to generate word embeddings for each publication\u2019s title and abstract. Word embedding models attempt to map words or phrases from a vocabulary into to vectors of real numbers. These word embeddings can then used in language modelling and feature learning natural language processing (NLP) techniques. Ideally, the end result is that publications with abstracts representing the same topics and concepts will be near one another within the reduced vector space generated through the word embedding process. For this example we will use TensorFlow and the Universal Sentence Encoder model to generate our word embeddings. The input into the model is variable length English text and it will generate a 512 dimensional vector. The embeddings should result in a vector representation which provides a reasonable approximation to semantic similarity between publications based on the content of the abstracts. import tensorflow as tf import tensorflow_hub as hub embed = hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) def get_embed_title ( titles ): return embed ( titles ) def get_embed_abstract ( abstracts , delimiter = \" \\n \" ): def _map_fn ( a ): t = tf . cast ( a , tf . string ) t = tf . strings . split ( t , sep = delimiter ) e = embed ( t ) e = tf . reduce_mean ( e , axis = 0 ) return tf . squeeze ( e ) return tf . map_fn ( _map_fn , abstracts , dtype = tf . float32 ) def process ( titles , abstracts ): title_embed = get_embed_title ( titles ) abstract_embed = get_embed_abstract ( abstracts ) output_features = { 'title_embed' : title_embed , 'abstract_embed' : abstract_embed , } return output_features r = process ( pubs [ \"title\" ], pubs [ \"abstract\" ]) Note The hub.load function execution may take a little while on the first run as it needs to download the USE model files. Taking a look at the generated embeddings, we can see that for each publication we have created two arrays of floating point values each 512 items long. The next step is to save these back into a temporary table in BigQuery so that we can apply k-means clustering to attempt to find groups of semantic similar publications.","title":"3. Generate word embeddings for each publication"},{"location":"tutorials/05-topic_clusters/#4-save-embedding-vectors-to-a-bigquery-table","text":"The next stage is to save the generated title and abstract embedding vectors back into a BigQuery table. new_pubs = pubs . copy () table_name = \"temp.semantic_clustering\" # Insert into the new DataFrame columns for title and abstract embeddings. new_pubs [ \"title_embed\" ] = r [ \"title_embed\" ] . numpy () . tolist () new_pubs [ \"abstract_embed\" ] = r [ \"abstract_embed\" ] . numpy () . tolist () # Save the DataFrame to a pickle file and into a GBQ table. new_pubs . to_pickle ( \"/content/new_pubs.pkl\" ) new_pubs . to_gbq ( table_name , project_id , chunksize = 2500 ) One issue with the imported data is that the vectors for the embeddings is a string rather than a field containing repeated floating-point values. The SQL statement below however creates a new table that converts the string into a proper array of real numbers. CREATE OR REPLACE TABLE ` my - gbq - project . clustering . nz_pubs_with_embeddings ` AS SELECT id , title , abstract , REGEXP_EXTRACT_ALL ( orgs , r \"\\'([^\\s,\\']+)\\'\" ) as orgs , ( SELECT ARRAY_AGG ( CAST ( v as FLOAT64 ) ORDER BY o asc ) FROM UNNEST ( SPLIT ( REGEXP_EXTRACT ( title_embed , r \"^\\[(.*)\\]$\" ), \", \" )) v WITH OFFSET o ) AS title_embed , ( SELECT ARRAY_AGG ( CAST ( v as FLOAT64 ) ORDER BY o asc ) FROM UNNEST ( SPLIT ( REGEXP_EXTRACT ( abstract_embed , r \"^\\[(.*)\\]$\" ), \", \" )) v WITH OFFSET o ) AS abstract_embed FROM ` my - gbq - project . temp . semantic_clustering ` Taking a look at the table within Google BigQuery (web interface) we can see the schema. It contains title_embed and abstract_embed both of which are repeated fields of FLOAT type.","title":"4. Save embedding vectors to a BigQuery table"},{"location":"tutorials/05-topic_clusters/#5-create-k-means-cluster-models","text":"Warning The example BigQuery SQL statement below use BigQuery ML which has a different charging model when it comes to creating models. Please keep this in mind and approximate how much it could potentially cost before executing any of the SQL statements. Pricing details are available from Google here: BigQuery ML Pricing . The most time consuming and computationally expensive part of this example is the clustering process itself. Luckily we can utilise BigQuery ML to create the models and create the clusters of publications based on the word embeddings we have created previously. In this example we will use k-means clustering to attempt to assign each publication to a grouping of semantically similar publications (based on abstracts). Essentially k-means clustering attempts to partition the individual items using Euclidean distance as the metric and minimising the within cluster sum of squares (ie. minimise squared errors). The primary input parameter that controls the k-means models is the number of partitions (ie. how many clusters we want to partition into). We can use some crude hyper-parameter tuning through creating numerous k-means models on our dataset, attempting to determine what a \u201cgood\u201d number of clusters may look like for our set of data. It is important to understand however that because of the approach k-means clustering utilises when assigning items to partitions it is important to analysis the results as a local minimums are a possibility. DECLARE NUM_CLUSTERS INT64 DEFAULT 10 ; DECLARE MODEL_NAME STRING ; WHILE NUM_CLUSTERS < 60 DO SET MODEL_NAME = CONCAT ( 'my-gbq-project.clustering.model_nz_pubs_' , CAST ( NUM_CLUSTERS AS STRING )); EXECUTE IMMEDIATE format ( \"\"\" CREATE OR REPLACE MODEL `%s` OPTIONS(model_type='kmeans', num_clusters = %d, DISTANCE_TYPE = 'cosine', kmeans_init_method = 'KMEANS++') AS WITH data AS ( SELECT abstract_embed[OFFSET(0)] as abstract0, abstract_embed[OFFSET(1)] as abstract1, abstract_embed[OFFSET(2)] as abstract2, abstract_embed[OFFSET(3)] as abstract3, abstract_embed[OFFSET(4)] as abstract4, abstract_embed[OFFSET(5)] as abstract5, abstract_embed[OFFSET(6)] as abstract6, abstract_embed[OFFSET(7)] as abstract7, ... abstract_embed[OFFSET(510)] as abstract510, abstract_embed[OFFSET(511)] as abstract511 FROM `my-gbq-project.clustering.nz_pubs_with_embeddings` ) SELECT * FROM data; \"\"\" , MODEL_NAME , NUM_CLUSTERS ); SET NUM_CLUSTERS = NUM_CLUSTERS + 5 ; END WHILE Note The SQL statement above has been truncated. The abstract_embed[OFFSET(n)] as abstract_n lines have been omitted and goes from 0 to 511 uninterrupted and has been abbreviated for the purposes of brevity in the tutorial write-up. The full SQL is available here . Creating the models may take a little while as it must apply the k-means algorithm over our dataset for each of the different cluster count parameter values. The next step is to analyse the results for all of the models created. We can evaluate each of the models, returning the Davies\u2013Bouldin index and the mean squared distance. Determining the optimal number of clusters is outside the scope of this tutorial, however a common approach is using the \u201cElbow method\u201d . # StandardSQL SELECT 60 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_60 ` ) UNION ALL SELECT 55 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_55 ` ) UNION ALL SELECT 50 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_50 ` ) UNION ALL SELECT 45 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_45 ` ) UNION ALL SELECT 40 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_40 ` ) UNION ALL SELECT 35 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_35 ` ) UNION ALL SELECT 30 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_30 ` ) UNION ALL SELECT 25 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_25 ` ) UNION ALL SELECT 20 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_20 ` ) UNION ALL SELECT 15 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_15 ` ) UNION ALL SELECT 10 as cluster_size , * FROM ML . EVALUATE ( MODEL ` my - gbq - project . clustering . model_nz_pubs_10 ` )","title":"5. Create k-means cluster models"},{"location":"tutorials/05-topic_clusters/#6-publication-to-cluster-assignment","text":"Using k=55 ( my-gbq-project.clustering.model_nz_pubs_55 ) as the cluster count from above we can now use the k-means model we created in BigQuery to determine which publications are assigned to each different centroid (cluster). Note below that the input data into the model matches the data structure that was used during the creation of the model itself (the abstracts embedding vector exploded into 512 individual string inputs). CREATE TABLE ` my - gbq - project . clustering . pubs_assigned_55 ` AS WITH data AS ( SELECT id , abstract_embed [ OFFSET ( 0 )] as abstract0 , abstract_embed [ OFFSET ( 1 )] as abstract1 , abstract_embed [ OFFSET ( 2 )] as abstract2 , ... abstract_embed [ OFFSET ( 510 )] as abstract510 , abstract_embed [ OFFSET ( 511 )] as abstract511 FROM ` my - gbq - project . clustering . nz_pubs_with_embeddings ` ) SELECT id , CENTROID_ID , NEAREST_CENTROIDS_DISTANCE FROM ML . PREDICT ( MODEL ` my - gbq - project . clustering . model_nz_pubs_55 ` , ( SELECT * FROM data )) Taking a look at the resultant table we see that each publication has been assigned to a primary cluster (centroid identifier) as well as a listing of the closest centroids as well as the Euclidean distance to the centroid.","title":"6. Publication to cluster assignment"},{"location":"tutorials/05-topic_clusters/#7-determine-conceptstopics-for-each-cluster","text":"The next stage is to try and determine the topics and concepts associated with each of the 55 clusters identified within the assigned publications above. Some publications have concepts extracted via NLP processing of the full-text. One approach to determining the concepts associated with each cluster is to aggregate these extracted concepts from for all publications within the same cluster. SELECT ANY_VALUE ( centroid ) as centroid_id , cluster_id as cluster_name , ARRAY_AGG ( concept order by ordering ) as concepts FROM ( SELECT * FROM ( SELECT * , ROW_NUMBER () OVER ( PARTITION BY cluster_id ORDER BY count desc ) as ordering FROM ( SELECT ANY_VALUE ( centroid ) as centroid , a . cluster_id as cluster_id , concept , COUNT ( * ) as count FROM ( SELECT a . id , a . CENTROID_ID as centroid , CONCAT ( \"Cluster-\" , CAST ( a . CENTROID_ID as STRING )) as cluster_id , ( SELECT ARRAY_AGG ( LOWER ( c . concept )) FROM UNNEST ( p . concepts ) c ) as concetps FROM ` my - gbq - project . clustering . pubs_assigned_55 ` a LEFT JOIN ` dimensions - ai . data_analytics . publications ` p ON p . id = a . id ) a , UNNEST ( a . concetps ) concept GROUP BY a . cluster_id , concept ) ORDER BY cluster_id desc , ordering asc ) WHERE ordering <= 20 ) GROUP BY cluster_id Taking a look at the results we can see for each centroid the 20 top concepts (by occurrence over all publications).","title":"7. Determine concepts/topics for each cluster"}]}